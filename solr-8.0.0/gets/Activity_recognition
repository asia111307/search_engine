<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>Activity recognition - Wikipedia</title>
<script>document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );</script>
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Activity_recognition","wgTitle":"Activity recognition","wgCurRevisionId":883580569,"wgRevisionId":883580569,"wgArticleId":15795950,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Wikipedia articles with style issues from March 2018","All articles with style issues","Cognition","Artificial intelligence applications","Applied machine learning"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Activity_recognition","wgRelevantArticleId":15795950,"wgRequestId":"XLIpagpAICIAABX28ukAAACH","wgCSPNonce":false,"wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgFlaggedRevsParams":{"tags":{}},"wgStableRevisionId":null,"wgBetaFeaturesFeatures":[],"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsReferencePreviews":false,"wgPopupsShouldSendModuleToUser":true,"wgPopupsConflictsWithNavPopupGadget":false,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en","usePageImages":true,"usePageDescriptions":true},"wgMFDisplayWikibaseDescriptions":{"search":true,"nearby":true,"watchlist":true,"tagline":false},"wgRelatedArticles":null,"wgRelatedArticlesUseCirrusSearch":true,"wgRelatedArticlesOnlyUseCirrusSearch":false,"wgWMESchemaEditAttemptStepOversample":false,"wgPoweredByHHVM":true,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralNoticeCookiesToDelete":[],"wgCentralNoticeCategoriesUsingLegacy":["Fundraising","fundraising"],"wgWikibaseItemId":"Q4677630","wgScoreNoteLanguages":{"arabic":"العربية","catalan":"català","deutsch":"Deutsch","english":"English","espanol":"español","italiano":"italiano","nederlands":"Nederlands","norsk":"norsk","portugues":"português","suomi":"suomi","svenska":"svenska","vlaams":"West-Vlams"},"wgScoreDefaultNoteLanguage":"nederlands","wgCentralAuthMobileDomain":false,"wgVisualEditorUnsupportedEditParams":["undo","undoafter","veswitched"],"wgEditSubmitButtonLabelPublish":true});mw.loader.state({"ext.gadget.charinsert-styles":"ready","ext.globalCssJs.user.styles":"ready","ext.globalCssJs.site.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","ext.globalCssJs.site":"ready","user":"ready","user.options":"ready","user.tokens":"loading","ext.cite.styles":"ready","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","mediawiki.toc.styles":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","ext.3d.styles":"ready","mediawiki.skinning.interface":"ready","skins.vector.styles":"ready"});mw.loader.implement("user.tokens@0tffind",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});RLPAGEMODULES=["ext.cite.ux-enhancements","site","mediawiki.page.startup","mediawiki.page.ready","mediawiki.toc","mediawiki.searchSuggest","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.eventlogger","ext.uls.init","ext.uls.compactlinks","ext.uls.interface","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp","skins.vector.js"];mw.loader.load(RLPAGEMODULES);});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.3d.styles%7Cext.cite.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.skinning.interface%7Cmediawiki.toc.styles%7Cskins.vector.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.gadget.charinsert-styles&amp;only=styles&amp;skin=vector"/>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.33.0-wmf.25"/>
<meta name="referrer" content="origin"/>
<meta name="referrer" content="origin-when-crossorigin"/>
<meta name="referrer" content="origin-when-cross-origin"/>
<link rel="alternate" href="android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/Activity_recognition"/>
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Activity_recognition&amp;action=edit"/>
<link rel="edit" title="Edit this page" href="/w/index.php?title=Activity_recognition&amp;action=edit"/>
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png"/>
<link rel="shortcut icon" href="/static/favicon/wikipedia.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd"/>
<link rel="license" href="//creativecommons.org/licenses/by-sa/3.0/"/>
<link rel="canonical" href="https://en.wikipedia.org/wiki/Activity_recognition"/>
<link rel="dns-prefetch" href="//login.wikimedia.org"/>
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<!--[if lt IE 9]><script src="/w/load.php?lang=en&amp;modules=html5shiv&amp;only=scripts&amp;skin=vector&amp;sync=1"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Activity_recognition rootpage-Activity_recognition skin-vector action-view">
<div id="mw-page-base" class="noprint"></div>
<div id="mw-head-base" class="noprint"></div>
<div id="content" class="mw-body" role="main">
	<a id="top"></a>
	<div id="siteNotice" class="mw-body-content"><!-- CentralNotice --></div>
	<div class="mw-indicators mw-body-content">
</div>

	<h1 id="firstHeading" class="firstHeading" lang="en">Activity recognition</h1>
	
	<div id="bodyContent" class="mw-body-content">
		<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
		<div id="contentSub"></div>
		
		
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="#p-search">Jump to search</a>
		<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="mw-parser-output"><table class="box-Research_paper plainlinks metadata ambox ambox-style ambox-essay-like" role="presentation"><tbody><tr><td class="mbox-image"><div style="width:52px"><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/f/f2/Edit-clear.svg/40px-Edit-clear.svg.png" decoding="async" width="40" height="40" srcset="//upload.wikimedia.org/wikipedia/en/thumb/f/f2/Edit-clear.svg/60px-Edit-clear.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/f/f2/Edit-clear.svg/80px-Edit-clear.svg.png 2x" data-file-width="48" data-file-height="48" /></div></td><td class="mbox-text"><div class="mbox-text-span">This article <b>is written like a <a href="/wiki/Wikipedia:What_Wikipedia_is_not#Wikipedia_is_not_a_manual,_guidebook,_textbook,_or_scientific_journal" title="Wikipedia:What Wikipedia is not">research paper or scientific journal</a> that may use <a href="/wiki/Wikipedia:Make_technical_articles_understandable" title="Wikipedia:Make technical articles understandable">overly technical terms</a> or may not be written <a href="/wiki/Wikipedia:TONE" class="mw-redirect" title="Wikipedia:TONE">like an encyclopedic article</a></b>.<span class="hide-when-compact"> Please <a class="external text" href="//en.wikipedia.org/w/index.php?title=Activity_recognition&amp;action=edit">help improve it</a> by rewriting it in an <a href="/wiki/Wikipedia:Encyclopedic_style" class="mw-redirect" title="Wikipedia:Encyclopedic style">encyclopedic style</a>.</span>  <small class="date-container"><i>(<span class="date">March 2018</span>)</i></small><small class="hide-when-compact"><i> (<a href="/wiki/Help:Maintenance_template_removal" title="Help:Maintenance template removal">Learn how and when to remove this template message</a>)</i></small></div></td></tr></tbody></table>
<p><b>Activity recognition</b> aims to recognize the actions and goals of one or more agents from a series of observations on the agents' actions and the environmental conditions. Since the 1980s, this research field has captured the attention of several <a href="/wiki/Computer_science" title="Computer science">computer science</a> communities due to its strength in providing personalized support for many different applications and its connection to many different fields of study such as medicine, human-computer interaction, or sociology.
</p><p>Due to its many-faceted nature, different fields may refer to activity recognition as plan recognition, goal recognition, intent recognition, behavior recognition, location estimation and location-based services.
</p>
<div id="toc" class="toc"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2>Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Types"><span class="tocnumber">1</span> <span class="toctext">Types</span></a>
<ul>
<li class="toclevel-2 tocsection-2"><a href="#Sensor-based,_single-user_activity_recognition"><span class="tocnumber">1.1</span> <span class="toctext">Sensor-based, single-user activity recognition</span></a>
<ul>
<li class="toclevel-3 tocsection-3"><a href="#Levels_of_sensor-based_activity_recognition"><span class="tocnumber">1.1.1</span> <span class="toctext">Levels of sensor-based activity recognition</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-4"><a href="#Sensor-based,_multi-user_activity_recognition"><span class="tocnumber">1.2</span> <span class="toctext">Sensor-based, multi-user activity recognition</span></a></li>
<li class="toclevel-2 tocsection-5"><a href="#Sensor-based_group_activity_recognition"><span class="tocnumber">1.3</span> <span class="toctext">Sensor-based group activity recognition</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-6"><a href="#Approaches"><span class="tocnumber">2</span> <span class="toctext">Approaches</span></a>
<ul>
<li class="toclevel-2 tocsection-7"><a href="#Activity_recognition_through_logic_and_reasoning"><span class="tocnumber">2.1</span> <span class="toctext">Activity recognition through logic and reasoning</span></a></li>
<li class="toclevel-2 tocsection-8"><a href="#Activity_recognition_through_probabilistic_reasoning"><span class="tocnumber">2.2</span> <span class="toctext">Activity recognition through probabilistic reasoning</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="#Data_mining_based_approach_to_activity_recognition"><span class="tocnumber">2.3</span> <span class="toctext">Data mining based approach to activity recognition</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-10"><a href="#Sensor_usage"><span class="tocnumber">3</span> <span class="toctext">Sensor usage</span></a>
<ul>
<li class="toclevel-2 tocsection-11"><a href="#Vision-based_activity_recognition"><span class="tocnumber">3.1</span> <span class="toctext">Vision-based activity recognition</span></a>
<ul>
<li class="toclevel-3 tocsection-12"><a href="#Levels_of_vision-based_activity_recognition"><span class="tocnumber">3.1.1</span> <span class="toctext">Levels of vision-based activity recognition</span></a></li>
<li class="toclevel-3 tocsection-13"><a href="#Fine-grained_action_localization"><span class="tocnumber">3.1.2</span> <span class="toctext">Fine-grained action localization</span></a></li>
<li class="toclevel-3 tocsection-14"><a href="#Automatic_gait_recognition"><span class="tocnumber">3.1.3</span> <span class="toctext">Automatic gait recognition</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-15"><a href="#Wi-Fi-based_activity_recognition"><span class="tocnumber">3.2</span> <span class="toctext">Wi-Fi-based activity recognition</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-16"><a href="#Applications"><span class="tocnumber">4</span> <span class="toctext">Applications</span></a></li>
<li class="toclevel-1 tocsection-17"><a href="#See_also"><span class="tocnumber">5</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-18"><a href="#References"><span class="tocnumber">6</span> <span class="toctext">References</span></a></li>
</ul>
</div>

<h2><span class="mw-headline" id="Types">Types</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Activity_recognition&amp;action=edit&amp;section=1" title="Edit section: Types">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span id="Sensor-based.2C_single-user_activity_recognition"></span><span class="mw-headline" id="Sensor-based,_single-user_activity_recognition">Sensor-based, single-user activity recognition</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Activity_recognition&amp;action=edit&amp;section=2" title="Edit section: Sensor-based, single-user activity recognition">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="/wiki/Sensor" title="Sensor">Sensor</a>-based activity recognition integrates the emerging area of sensor networks with novel <a href="/wiki/Data_mining" title="Data mining">data mining</a> and <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> techniques to model a wide range of human activities.<sup id="cite_ref-1" class="reference"><a href="#cite_note-1">&#91;1&#93;</a></sup><sup id="cite_ref-2" class="reference"><a href="#cite_note-2">&#91;2&#93;</a></sup> Mobile devices (e.g. smart phones) provide sufficient sensor data and calculation power to enable physical activity recognition to provide an estimation of the energy consumption during everyday life. Sensor-based activity recognition researchers believe that by empowering <a href="/wiki/Ubiquitous_computing" title="Ubiquitous computing">ubiquitous computers</a> and sensors to monitor the behavior of agents (under consent), these computers will be better suited to act on our behalf.
</p>
<h4><span class="mw-headline" id="Levels_of_sensor-based_activity_recognition">Levels of sensor-based activity recognition</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Activity_recognition&amp;action=edit&amp;section=3" title="Edit section: Levels of sensor-based activity recognition">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Sensor-based activity recognition is a challenging task due to the inherent noisy nature of the input. Thus, <a href="/wiki/Statistical_modeling" class="mw-redirect" title="Statistical modeling">statistical modeling</a> has been the main thrust in this direction in layers, where the recognition at several intermediate levels is conducted and connected. At the lowest level where the sensor data are collected, statistical learning concerns how to find the detailed locations of agents from the received signal data. At an intermediate level, <a href="/wiki/Statistical_inference" title="Statistical inference">statistical inference</a> may be concerned about how to recognize individuals' activities from the inferred location sequences and environmental conditions at the lower levels. Furthermore, at the highest level a major concern is to find out the overall goal or subgoals of an agent from the activity sequences through a mixture of logical and statistical reasoning.
</p>
<h3><span id="Sensor-based.2C_multi-user_activity_recognition"></span><span class="mw-headline" id="Sensor-based,_multi-user_activity_recognition">Sensor-based, multi-user activity recognition</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Activity_recognition&amp;action=edit&amp;section=4" title="Edit section: Sensor-based, multi-user activity recognition">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Recognizing activities for multiple users using on-body sensors first appeared in the work by ORL using active badge systems<sup id="cite_ref-3" class="reference"><a href="#cite_note-3">&#91;3&#93;</a></sup> in the early 90's. Other sensor technology such as acceleration sensors were used for identifying group activity patterns during office scenarios.<sup id="cite_ref-4" class="reference"><a href="#cite_note-4">&#91;4&#93;</a></sup> Activities of Multiple Users in intelligent environments are addressed in Gu et al.<sup id="cite_ref-5" class="reference"><a href="#cite_note-5">&#91;5&#93;</a></sup> In this work, they investigate the fundamental problem of recognizing activities for multiple users from sensor readings in a home environment, and propose a novel pattern mining approach to recognize both single-user and multi-user activities in a unified solution.
</p>
<h3><span class="mw-headline" id="Sensor-based_group_activity_recognition">Sensor-based group activity recognition</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Activity_recognition&amp;action=edit&amp;section=5" title="Edit section: Sensor-based group activity recognition">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Recognition of group activities is fundamentally different from single, or multi-user activity recognition in that the goal is to recognize the behavior of the group as an entity, rather than the activities of the individual members within it.<sup id="cite_ref-6" class="reference"><a href="#cite_note-6">&#91;6&#93;</a></sup>  Group behavior is emergent in nature, meaning that the properties of the behavior of the group are fundamentally different then the properties of the behavior of the individuals within it, or any sum of that behavior.<sup id="cite_ref-7" class="reference"><a href="#cite_note-7">&#91;7&#93;</a></sup>  The main challenges are in modeling the behavior of the individual group members, as well as the roles of the individual within the group dynamic <sup id="cite_ref-8" class="reference"><a href="#cite_note-8">&#91;8&#93;</a></sup> and their relationship to emergent behavior of the group in parallel.<sup id="cite_ref-9" class="reference"><a href="#cite_note-9">&#91;9&#93;</a></sup>  Challenges which must still be addressed include quantification of the behavior and roles of individuals who join the group, integration of explicit models for role description into inference algorithms, and scalability evaluations for very large groups and crowds.  Group activity recognition has applications for crowd management and response in emergency situations, as well as for social networking and <a href="/wiki/Quantified_Self" class="mw-redirect" title="Quantified Self">Quantified Self</a> applications.<sup id="cite_ref-10" class="reference"><a href="#cite_note-10">&#91;10&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Approaches">Approaches</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Activity_recognition&amp;action=edit&amp;section=6" title="Edit section: Approaches">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Activity_recognition_through_logic_and_reasoning">Activity recognition through logic and reasoning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Activity_recognition&amp;action=edit&amp;section=7" title="Edit section: Activity recognition through logic and reasoning">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Logic-based approaches keep track of all <a href="/wiki/Logically_consistent" class="mw-redirect" title="Logically consistent">logically consistent</a> explanations of the observed actions. Thus, all possible and consistent plans or goals must be considered. Kautz provided a formal theory of plan recognition. He described plan recognition as a logical inference process of circumscription. All actions, plans are uniformly referred to as goals, and a recognizer's knowledge is represented by a set of first-order statements called event hierarchy encoded in first-order logic, which defines abstraction, decomposition and functional relationships between types of events.<sup id="cite_ref-11" class="reference"><a href="#cite_note-11">&#91;11&#93;</a></sup>
</p><p>Kautz's general framework for plan recognition has an exponential time complexity in worst case, measured in the size of input hierarchy. Lesh and Etzioni went one step further and presented methods in scaling up goal recognition to scale up his work computationally. In contrast to Kautz's approach where the plan library is explicitly represented, Lesh and Etzioni's approach enables automatic plan-library construction from domain primitives. Furthermore, they introduced compact representations and efficient algorithms for goal recognition on large plan libraries.<sup id="cite_ref-12" class="reference"><a href="#cite_note-12">&#91;12&#93;</a></sup>
</p><p>Inconsistent plans and goals are repeatedly pruned when new actions arrive. Besides, they also presented methods for adapting a goal recognizer to handle individual idiosyncratic behavior given a sample of an individual's recent behavior. Pollack et al. described a direct argumentation model that can know about the relative strength of several kinds of arguments for belief and intention description.
</p><p>A serious problem of logic-based approaches is their inability or inherent infeasibility to represent uncertainty. They offer no mechanism for preferring one consistent approach to another and incapable of deciding whether one particular plan is more likely than another, as long as both of them can be consistent enough to explain the actions observed. There is also a lack of learning ability associated with logic based methods.
</p><p>Another approach to logic-based activity recognition is to use stream reasoning based on Answer Set Programming,<sup id="cite_ref-13" class="reference"><a href="#cite_note-13">&#91;13&#93;</a></sup> and has been applied to recognising activities for health-related applications,<sup id="cite_ref-14" class="reference"><a href="#cite_note-14">&#91;14&#93;</a></sup>  which uses weak constraints to model a degree of ambiguity/uncertainty.
</p>
<h3><span class="mw-headline" id="Activity_recognition_through_probabilistic_reasoning">Activity recognition through probabilistic reasoning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Activity_recognition&amp;action=edit&amp;section=8" title="Edit section: Activity recognition through probabilistic reasoning">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Probability theory and statistical learning models are more recently applied in activity recognition to reason about actions, plans and goals under uncertainty.<sup id="cite_ref-15" class="reference"><a href="#cite_note-15">&#91;15&#93;</a></sup> In the literature, there have been several approaches which explicitly represent uncertainty in reasoning about an agent's plans and goals.
</p><p>Using sensor data as input, Hodges and Pollack designed machine learning-based systems for identifying individuals as they perform routine daily activities such as making coffee.<sup id="cite_ref-16" class="reference"><a href="#cite_note-16">&#91;16&#93;</a></sup> <a href="/wiki/Intel_Research_Lablets" title="Intel Research Lablets">Intel Research (Seattle) Lab</a> and <a href="/wiki/University_of_Washington" title="University of Washington">University of Washington</a> at Seattle have done some important works on using sensors to detect human plans.<sup id="cite_ref-17" class="reference"><a href="#cite_note-17">&#91;17&#93;</a></sup><sup id="cite_ref-18" class="reference"><a href="#cite_note-18">&#91;18&#93;</a></sup><sup id="cite_ref-19" class="reference"><a href="#cite_note-19">&#91;19&#93;</a></sup> Some of these works infer user transportation modes from readings of radio-frequency identifiers (RFID) and global positioning systems (GPS).
</p><p>The use of temporal probabilistic models has been shown to perform well in activity recognition and generally outperform non-temporal models.<sup id="cite_ref-20" class="reference"><a href="#cite_note-20">&#91;20&#93;</a></sup> Generative models such as the Hidden Markov Model (HMM) and the more generally formulated Dynamic Bayesian Networks (DBN) are popular choices in modelling activities from sensor data.<sup id="cite_ref-21" class="reference"><a href="#cite_note-21">&#91;21&#93;</a></sup><sup id="cite_ref-:0_22-0" class="reference"><a href="#cite_note-:0-22">&#91;22&#93;</a></sup><sup id="cite_ref-23" class="reference"><a href="#cite_note-23">&#91;23&#93;</a></sup><sup id="cite_ref-24" class="reference"><a href="#cite_note-24">&#91;24&#93;</a></sup>
Discriminative models such as Conditional Random Fields (CRF) are also commonly applied and also give good performance in activity recognition.<sup id="cite_ref-25" class="reference"><a href="#cite_note-25">&#91;25&#93;</a></sup><sup id="cite_ref-26" class="reference"><a href="#cite_note-26">&#91;26&#93;</a></sup>
</p><p>Generative and discriminative models both have their pros and cons and the ideal choice depends on their area of application. A dataset together with implementations of a number of popular models (HMM, CRF) for activity recognition can be found <a rel="nofollow" class="external text" href="https://sites.google.com/site/tim0306/datasets">here</a>.
</p><p>Conventional temporal probabilistic models such as the hidden Markov model (HMM) and conditional random fields (CRF) model directly model the correlations between the activities and the observed sensor data. In recent years, increasing evidence has supported the use of hierarchical models which take into account the rich hierarchical structure that exists in human behavioral data.<sup id="cite_ref-:0_22-1" class="reference"><a href="#cite_note-:0-22">&#91;22&#93;</a></sup><sup id="cite_ref-:1_27-0" class="reference"><a href="#cite_note-:1-27">&#91;27&#93;</a></sup><sup id="cite_ref-28" class="reference"><a href="#cite_note-28">&#91;28&#93;</a></sup> The core idea here is that the model does not directly correlate the activities with the sensor data, but instead breaks the activity into sub-activities (sometimes referred to as actions) and models the underlying correlations accordingly. An example could be the activity of preparing spaghetti, which can be broken down into the subactivities or actions of cutting vegetables, frying the vegetables in a pan and serving it on a plate. Examples of such a hierarchical model are Layered Hidden Markov Models (LHMMs)<sup id="cite_ref-:1_27-1" class="reference"><a href="#cite_note-:1-27">&#91;27&#93;</a></sup> and the hierarchical hidden Markov model (HHMM), which have been shown to significantly outperform its non-hierarchical counterpart in activity recognition.<sup id="cite_ref-:0_22-2" class="reference"><a href="#cite_note-:0-22">&#91;22&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Data_mining_based_approach_to_activity_recognition">Data mining based approach to activity recognition</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Activity_recognition&amp;action=edit&amp;section=9" title="Edit section: Data mining based approach to activity recognition">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Different from traditional machine learning approaches, an approach based on data mining has been recently proposed. In the work of Gu et al., the problem of activity recognition is formulated as a pattern-based classification problem. They proposed a data mining approach based on discriminative patterns which describe significant changes between any two activity classes of data to recognize sequential, interleaved and concurrent activities in a unified solution.<sup id="cite_ref-29" class="reference"><a href="#cite_note-29">&#91;29&#93;</a></sup> Gilbert <i>et al.</i> use 2D corners in both space and time. These are grouped spatially and temporally using a hierarchical process, with an increasing search area. At each stage of the hierarchy, the most distinctive and descriptive features are learned efficiently through data mining (Apriori rule).<sup id="cite_ref-30" class="reference"><a href="#cite_note-30">&#91;30&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Sensor_usage">Sensor usage</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Activity_recognition&amp;action=edit&amp;section=10" title="Edit section: Sensor usage">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Vision-based_activity_recognition">Vision-based activity recognition</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Activity_recognition&amp;action=edit&amp;section=11" title="Edit section: Vision-based activity recognition">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>It is a very important and challenging problem to track and understand the behavior of agents through videos taken by various cameras. The primary technique employed is computer vision. Vision-based activity recognition has found many applications such as human-computer interaction, user interface design, <a href="/wiki/Robot_learning" title="Robot learning">robot learning</a>, and surveillance, among others.
Scientific conferences where vision based activity recognition work often appears are <a href="/wiki/ICCV" class="mw-redirect" title="ICCV">ICCV</a> and <a href="/wiki/CVPR" class="mw-redirect" title="CVPR">CVPR</a>.
</p><p>In vision-based activity recognition, a great deal of work has been done. Researchers have attempted a number of methods such as <a href="/wiki/Optical_flow" title="Optical flow">optical flow</a>, <a href="/wiki/Kalman_filtering" class="mw-redirect" title="Kalman filtering">Kalman filtering</a>, <a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov models</a>, etc., under different modalities such as single camera, stereo, and infrared. In addition, researchers have considered multiple aspects on this topic, including single pedestrian tracking, group tracking, and detecting dropped objects.
</p><p>Recently some researchers have used RGBD cameras like Microsoft Kinect to detect human activities. Depth cameras add extra dimension i.e. depth which normal 2d camera fails to provide. Sensory information from these depth cameras have been used to generate real-time skeleton model of humans with different body positions. These skeleton information provides meaningful information that researchers have used to model human activities which are trained and later used to recognize unknown activities.<sup id="cite_ref-31" class="reference"><a href="#cite_note-31">&#91;31&#93;</a></sup>
</p><p>Despite remarkable progress of vision-based activity recognition, its usage for most actual visual surveillance applications remains a distant aspiration<sup id="cite_ref-32" class="reference"><a href="#cite_note-32">&#91;32&#93;</a></sup>. Conversely, the human brain seems to have perfected the ability to recognize human actions. This capability relies not only on acquired knowledge, but also on the aptitude of extracting information relevant to a given context and logical reasoning. Based on this observation, it has been proposed to enhance vision-based activity recognition systems by integrating <a href="/wiki/Commonsense_reasoning" title="Commonsense reasoning">commonsense reasoning</a> and, contextual and commonsense knowledge. Experiments performed using video and RGBD cameras demonstrate the added value of such approach <sup id="cite_ref-33" class="reference"><a href="#cite_note-33">&#91;33&#93;</a></sup><sup id="cite_ref-34" class="reference"><a href="#cite_note-34">&#91;34&#93;</a></sup>.
</p>
<h4><span class="mw-headline" id="Levels_of_vision-based_activity_recognition">Levels of vision-based activity recognition</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Activity_recognition&amp;action=edit&amp;section=12" title="Edit section: Levels of vision-based activity recognition">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>In vision-based activity recognition, the computational process is often divided into four steps, namely human detection, human tracking, human activity recognition and then a high-level activity evaluation.
</p>
<h4><span class="mw-headline" id="Fine-grained_action_localization">Fine-grained action localization</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Activity_recognition&amp;action=edit&amp;section=13" title="Edit section: Fine-grained action localization">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/Object_Co-segmentation" title="Object Co-segmentation">Object Co-segmentation</a></div>
<p>In <a href="/wiki/Computer_vision" title="Computer vision">computer vision</a>-based activity recognition, fine-grained action localization typically provides per-image segmentation masks delineating the human object and its action category (e.g., <i>Segment-Tube</i><sup id="cite_ref-Wang_Duan_Zhang_Niu_p=1657_35-0" class="reference"><a href="#cite_note-Wang_Duan_Zhang_Niu_p=1657-35">&#91;35&#93;</a></sup>). Techniques such as dynamic <a href="/wiki/Markov_random_field" title="Markov random field">Markov Networks</a>, <a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">CNN</a> and <a href="/wiki/Long_short-term_memory" title="Long short-term memory">LSTM</a> are often employed to exploit the semantic correlations between consecutive video frames.
</p>
<h4><span class="mw-headline" id="Automatic_gait_recognition">Automatic gait recognition</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Activity_recognition&amp;action=edit&amp;section=14" title="Edit section: Automatic gait recognition">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/Gait_recognition" class="mw-redirect" title="Gait recognition">Gait recognition</a></div>
<p>One way to identify specific people is by how they walk. Gait-recognition software can be used to record a person's gait or gait feature profile in a database for the purpose of recognizing that person later, even if they are wearing a disguise.
</p>
<h3><span class="mw-headline" id="Wi-Fi-based_activity_recognition">Wi-Fi-based activity recognition</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Activity_recognition&amp;action=edit&amp;section=15" title="Edit section: Wi-Fi-based activity recognition">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>When activity recognition is performed indoors and in cities using the widely available <a href="/wiki/Wi-Fi" title="Wi-Fi">Wi-Fi</a> signals and <a href="/wiki/802.11" class="mw-redirect" title="802.11">802.11</a> access points, there is much noise and uncertainty. These uncertainties can be modeled using a dynamic <a href="/wiki/Bayesian_network" title="Bayesian network">Bayesian network</a> model.<sup id="cite_ref-36" class="reference"><a href="#cite_note-36">&#91;36&#93;</a></sup> In a multiple goal model that can reason about user's interleaving goals,a <a href="/wiki/Deterministic" class="mw-redirect" title="Deterministic">deterministic</a> state transition model is applied.<sup id="cite_ref-37" class="reference"><a href="#cite_note-37">&#91;37&#93;</a></sup> Another possible method models the concurrent and interleaving activities in a probabilistic approach.<sup id="cite_ref-38" class="reference"><a href="#cite_note-38">&#91;38&#93;</a></sup> A user action discovery model could segment Wi-Fi signals to produce possible actions.<sup id="cite_ref-39" class="reference"><a href="#cite_note-39">&#91;39&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Applications">Applications</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Activity_recognition&amp;action=edit&amp;section=16" title="Edit section: Applications">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>By automatically monitoring human activities, home-based rehabilitation can be provided for people suffering from traumatic brain injuries. One can find applications ranging from security-related applications and logistics support to <a href="/wiki/Location-based_service" title="Location-based service">location-based services</a>.<sup id="cite_ref-40" class="reference"><a href="#cite_note-40">&#91;40&#93;</a></sup> Activity recognition systems have been developed for <a href="/wiki/Wildlife_observation" title="Wildlife observation">wildlife observation</a>.<sup id="cite_ref-41" class="reference"><a href="#cite_note-41">&#91;41&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Activity_recognition&amp;action=edit&amp;section=17" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a href="/wiki/AI_effect" title="AI effect">AI effect</a></li>
<li><a href="/wiki/Applications_of_artificial_intelligence" title="Applications of artificial intelligence">Applications of artificial intelligence</a></li>
<li><a href="/wiki/Conditional_random_field" title="Conditional random field">Conditional random field</a></li>
<li><a href="/wiki/Gesture_recognition" title="Gesture recognition">Gesture recognition</a></li>
<li><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov model</a></li>
<li><a href="/wiki/Motion_analysis" title="Motion analysis">Motion analysis</a></li>
<li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes classifier</a></li>
<li><a href="/wiki/Support_vector_machines" class="mw-redirect" title="Support vector machines">Support vector machines</a></li>
<li><a href="/wiki/Object_Co-segmentation" title="Object Co-segmentation">Object Co-segmentation</a></li>
<li><a href="/wiki/Outline_of_artificial_intelligence" title="Outline of artificial intelligence">Outline of artificial intelligence</a></li>
<li><a href="/wiki/Video_content_analysis" title="Video content analysis">Video content analysis</a></li></ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Activity_recognition&amp;action=edit&amp;section=18" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist" style="list-style-type: decimal;">
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text">Tanzeem Choudhury, <a href="/wiki/Gaetano_Borriello" title="Gaetano Borriello">Gaetano Borriello</a>, et al. The Mobile Sensing Platform: An Embedded System for Activity Recognition. Appears in the IEEE Pervasive Magazine - Special Issue on Activity-Based Computing, April 2008.</span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text">Nishkam Ravi, Nikhil Dandekar, Preetham Mysore, Michael Littman. <a rel="nofollow" class="external text" href="http://www.aaai.org/Papers/AAAI/2005/IAAI05-013.pdf">Activity Recognition from Accelerometer Data</a>. Proceedings of the Seventeenth Conference on Innovative Applications of Artificial Intelligence (IAAI/AAAI 2005).</span>
</li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text">Want R., Hopper A., Falcao V., Gibbons J.: The Active Badge Location System, ACM Transactions on Information, Systems, Vol. 40, No. 1, pp. 91-102, January 1992</span>
</li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text">Bieber G., Kirste T., Untersuchung des gruppendynamischen Aktivitaetsverhaltes im Office-Umfeld, 7. Berliner Werkstatt Mensch-Maschine-Systeme, Berlin, Germany, 2007</span>
</li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text">Tao Gu, Zhanqing Wu, Liang Wang, Xianping Tao, and Jian Lu. <a rel="nofollow" class="external text" href="https://ieeexplore.ieee.org/abstract/document/5326404/">Mining Emerging Patterns for Recognizing Activities of Multiple Users in Pervasive Computing</a>. In Proc. of the 6th International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services (MobiQuitous '09), Toronto, Canada, July 13–16, 2009.</span>
</li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text">Dawud Gordon, Jan-Hendrik Hanne, Martin Berchtold, Ali Asghar Nazari Shirehjini, Michael Beigl: <a rel="nofollow" class="external text" href="http://www.teco.edu/~gordon/publications/MONE_GAR.pdf">Towards Collaborative Group Activity Recognition Using Mobile Devices</a>, Mobile Networks and Applications 18(3), 2013, p. 326-340</span>
</li>
<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7">^</a></b></span> <span class="reference-text">Lewin, K. Field theory in social science: selected theoretical papers. Social science paperbacks. Harper, New York, 1951.</span>
</li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text">Hirano, T., and Maekawa, T. <a rel="nofollow" class="external text" href="http://www-nishio.ist.osaka-u.ac.jp/~maekawa/paper/maekawa-ISWC2013.pdf">A hybrid unsupervised/supervised model for group activity recognition</a>. In Proceedings of the 2013 International Symposium on Wearable Computers, ISWC ’13, ACM (New York, NY, USA, 2013), 21–24</span>
</li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text">Brdiczka, O., Maisonnasse, J., Reignier, P., and Crowley, J. L. <a rel="nofollow" class="external text" href="http://www-prima.imag.fr/Prima/Homepages/jlc/papers/BrdiczkaAppliedIntelligence.pdf">Detecting small group activities from multimodal observations</a>. Applied Intelligence 30, 1 (July 2007), 47–57.</span>
</li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text">Dawud Gordon, Group Activity Recognition Using Wearable Sensing Devices, Dissertation, Karlsruhe Institute of Technology, 2014</span>
</li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text">H. Kautz. "<a rel="nofollow" class="external text" href="https://urresearch.rochester.edu/fileDownloadForInstitutionalItem.action?itemId=5934&amp;itemFileId=9297">A formal theory of plan recognition</a>". In PhD thesis, University of Rochester, 1987.</span>
</li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text">N. Lesh and O. Etzioni. "<a rel="nofollow" class="external text" href="http://www.ijcai.org/Proceedings/95-2/Papers/088.pdf">A sound and fast goal recognizer</a>". In <i>Proceedings of the International Joint Conference on Artificial Intelligence</i>, 1995.</span>
</li>
<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text"><cite class="citation book">Do, Thang; Seng W. Loke; Fei Liu (2011). <i>Answer Set Programming for Stream Reasoning</i>. <i>Advances in Artificial Intelligence, Lecture Notes in Computer Science</i>. Lecture Notes in Computer Science. <b>6657</b>. pp.&#160;104–109. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.453.2348">10.1.1.453.2348</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1007%2F978-3-642-21043-3_13">10.1007/978-3-642-21043-3_13</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-3-642-21042-6" title="Special:BookSources/978-3-642-21042-6">978-3-642-21042-6</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Answer+Set+Programming+for+Stream+Reasoning&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft.pages=104-109&amp;rft.date=2011&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.453.2348&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-642-21043-3_13&amp;rft.isbn=978-3-642-21042-6&amp;rft.aulast=Do&amp;rft.aufirst=Thang&amp;rft.au=Seng+W.+Loke&amp;rft.au=Fei+Liu&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AActivity+recognition" class="Z3988"></span><style data-mw-deduplicate="TemplateStyles:r886058088">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}</style></span>
</li>
<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text"><cite class="citation journal">Do, Thang; Seng W. Loke; Fei Liu (2012). <a rel="nofollow" class="external text" href="http://homepage.cs.latrobe.edu.au/sloke/papers/Mobiquitous2012.pdf">"HealthyLife: an Activity Recognition System with Smartphone using Logic-Based Stream Reasoning"</a> <span class="cs1-format">(PDF)</span>. <i>Proceedings of the 9th International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services, (Mobiquitous 2012)</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+9th+International+Conference+on+Mobile+and+Ubiquitous+Systems%3A+Computing%2C+Networking+and+Services%2C+%28Mobiquitous+2012%29&amp;rft.atitle=HealthyLife%3A+an+Activity+Recognition+System+with+Smartphone+using+Logic-Based+Stream+Reasoning&amp;rft.date=2012&amp;rft.aulast=Do&amp;rft.aufirst=Thang&amp;rft.au=Seng+W.+Loke&amp;rft.au=Fei+Liu&amp;rft_id=http%3A%2F%2Fhomepage.cs.latrobe.edu.au%2Fsloke%2Fpapers%2FMobiquitous2012.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AActivity+recognition" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text">E. Charniak and R.P. Goldman. "<a rel="nofollow" class="external text" href="https://www.sciencedirect.com/science/article/pii/000437029390060O">A Bayesian model of plan recognition</a>". <i>Artificial Intelligence</i>, 64:53–79, 1993.</span>
</li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text">M.R. Hodges and M.E. Pollack. "<a rel="nofollow" class="external text" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.109.5487&amp;rep=rep1&amp;type=pdf">An 'object-use fingerprint': The use of electronic sensors for human identification</a>". In <i>Proceedings of the 9th International Conference on Ubiquitous Computing</i>, 2007.</span>
</li>
<li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17">^</a></b></span> <span class="reference-text">Mike Perkowitz, Matthai Philipose, Donald J. Patterson, and Kenneth P. Fishkin. "<a rel="nofollow" class="external text" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.103.7512&amp;rep=rep1&amp;type=pdf">Mining models of human activities from the web</a>". In <i>Proceedings of the Thirteenth International World Wide Web Conference (WWW 2004), pages 573–582, May 2004.</i></span>
</li>
<li id="cite_note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18">^</a></b></span> <span class="reference-text">Matthai Philipose, Kenneth P. Fishkin, Mike Perkowitz, Donald J. Patterson, Dieter Fox, Henry Kautz, and Dirk Hähnel. "<a rel="nofollow" class="external text" href="https://ftp.cs.rochester.edu/u/kautz/papers/pervasive_proact_final.pdf">Inferring activities from interactions with objects</a>". In <i>IEEE Pervasive Computing</i>, pages 50–57, October 2004.</span>
</li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text">Dieter Fox Lin Liao, Donald J. Patterson and Henry A. Kautz. "<a rel="nofollow" class="external text" href="http://www.aaai.org/Papers/AAAI/2004/AAAI04-056.pdf">Learning and inferring transportation routines</a>". <i>Artif. Intell.</i>, 171(5-6):311–331, 2007.</span>
</li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text">TLM van Kasteren, Gwenn Englebienne, BJA Kröse. "<a rel="nofollow" class="external text" href="https://www.researchgate.net/profile/Tim_Van_Kasteren/publication/227017257_Human_Activity_Recognition_from_Wireless_Sensor_Network_Data_Benchmark_and_Software/links/0deec52b531429802c000000/Human-Activity-Recognition-from-Wireless-Sensor-Network-Data-Benchmark-and-Software.pdf">Human activity recognition from wireless sensor network data: Benchmark and software</a>." Activity Recognition in Pervasive Intelligent Environments, 165-186, Atlantis Press</span>
</li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text">Piyathilaka, L.; Kodagoda, S., "<a rel="nofollow" class="external text" href="https://www.researchgate.net/profile/Piyathilaka_Lasitha/publication/258341803_Gaussian_Mixture_Based_HMM_for_Human_Daily_Activity_Recognition_Using_3D_Skeleton_Features/links/0deec527f42f76c8f4000000/Gaussian-Mixture-Based-HMM-for-Human-Daily-Activity-Recognition-Using-3D-Skeleton-Features.pdf">Gaussian mixture based HMM for human daily activity recognition using 3D skeleton features</a>," Industrial Electronics and Applications (ICIEA), 2013 8th IEEE Conference on, vol., no., pp.567,572, 19–21 June 2013</span>
</li>
<li id="cite_note-:0-22"><span class="mw-cite-backlink">^ <a href="#cite_ref-:0_22-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:0_22-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:0_22-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text">TLM van Kasteren, Gwenn Englebienne, Ben Kröse" <a rel="nofollow" class="external text" href="https://sites.google.com/site/tim0306/amiKasterenHierch.pdf?attredirects=0">Hierarchical Activity Recognition Using Automatically Clustered Actions</a>", 2011, Ambient Intelligence, 82-91, Springer Berlin/Heidelberg</span>
</li>
<li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text">Daniel Wilson and Chris Atkeson. <a rel="nofollow" class="external text" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.91.3498&amp;rep=rep1&amp;type=pdf">Simultaneous tracking and activityrecognition (star) using many anonymous binary sensors</a>. In Proceedings of the 3rd international conference on Pervasive Computing, Pervasive, pages 62–79, Munich, Germany, 2005.</span>
</li>
<li id="cite_note-24"><span class="mw-cite-backlink"><b><a href="#cite_ref-24">^</a></b></span> <span class="reference-text"><a href="/wiki/Nuria_Oliver" title="Nuria Oliver">Nuria Oliver</a>, Barbara Rosario and <a href="/wiki/Alex_Pentland" title="Alex Pentland">Alex Pentland</a> "A Bayesian Computer Vision System for Modeling Human Interactions"
Appears in PAMI Special Issue on Visual Surveillance and Monitoring, Aug 00</span>
</li>
<li id="cite_note-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-25">^</a></b></span> <span class="reference-text">TLM Van Kasteren, Athanasios Noulas, Gwenn Englebienne, Ben Kröse, "<a rel="nofollow" class="external text" href="https://dl.acm.org/citation.cfm?id=1409637">Accurate activity recognition in a home setting</a>", 2008/9/21, Proceedings of the 10th international conference on Ubiquitous computing, 1-9, ACM</span>
</li>
<li id="cite_note-26"><span class="mw-cite-backlink"><b><a href="#cite_ref-26">^</a></b></span> <span class="reference-text">Derek Hao Hu, Sinno Jialin Pan, Vincent Wenchen Zheng, Nathan NanLiu, and Qiang Yang. Real world activity recognition with multiple goals. In Proceedings of the 10th international conference on Ubiquitous computing, Ubicomp, pages 30–39, New York, NY, USA, 2008. ACM.</span>
</li>
<li id="cite_note-:1-27"><span class="mw-cite-backlink">^ <a href="#cite_ref-:1_27-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:1_27-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><a href="/wiki/Nuria_Oliver" title="Nuria Oliver">Nuria Oliver</a>, Ashutosh Garg, and Eric Horvitz. Layered representations for learning and inferring office activity from multiple sensory channels. Comput. Vis. Image Underst., 96(2):163–180, 2004.</span>
</li>
<li id="cite_note-28"><span class="mw-cite-backlink"><b><a href="#cite_ref-28">^</a></b></span> <span class="reference-text">Amarnag Subramanya, Alvin Raj, Jeff Bilmes, and Dieter Fox. Hierarchical models for activity recognition. In Proceedings of the international conference on Multimedia Signal Processing, MMSP, Victoria, CA, October 2006.</span>
</li>
<li id="cite_note-29"><span class="mw-cite-backlink"><b><a href="#cite_ref-29">^</a></b></span> <span class="reference-text">Tao Gu, Zhanqing Wu, Xianping Tao, Hung Keng Pung, and Jian Lu. <a rel="nofollow" class="external text" href="https://ieeexplore.ieee.org/abstract/document/4912776/">epSICAR: An Emerging Patterns based Approach to Sequential, Interleaved and Concurrent Activity Recognition</a>. In Proc. of the 7th Annual IEEE International Conference on Pervasive Computing and Communications (Percom '09), Galveston, Texas, March 9–13, 2009.</span>
</li>
<li id="cite_note-30"><span class="mw-cite-backlink"><b><a href="#cite_ref-30">^</a></b></span> <span class="reference-text">Gilbert A, Illingworth J, Bowden R. Action Recognition using Mined Hierarchical Compound Features. IEEE Trans Pattern Analysis and Machine Learning</span>
</li>
<li id="cite_note-31"><span class="mw-cite-backlink"><b><a href="#cite_ref-31">^</a></b></span> <span class="reference-text">Piyathilaka, L.; Kodagoda, S., "<a rel="nofollow" class="external text" href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6566433&amp;isnumber=6566328">Gaussian mixture based HMM for human daily activity recognition using 3D skeleton features</a>," Industrial Electronics and Applications (ICIEA), 2013 8th IEEE Conference on, vol., no., pp.567,572, 19–21 June 2013  URL: <a rel="nofollow" class="external free" href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6566433&amp;isnumber=6566328">http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6566433&amp;isnumber=6566328</a></span>
</li>
<li id="cite_note-32"><span class="mw-cite-backlink"><b><a href="#cite_ref-32">^</a></b></span> <span class="reference-text"><cite class="citation journal">Bux, Allah; Angelov, Plamen; Habib, Zulfiqar (2017). "A comprehensive review on handcrafted and learning-based action representation approaches for human activity recognition". <i>Applied Sciences</i>. <b>7</b> (1): 110. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.3390%2Fapp7010110">10.3390/app7010110</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Applied+Sciences&amp;rft.atitle=A+comprehensive+review+on+handcrafted+and+learning-based+action+representation+approaches+for+human+activity+recognition&amp;rft.volume=7&amp;rft.issue=1&amp;rft.pages=110&amp;rft.date=2017&amp;rft_id=info%3Adoi%2F10.3390%2Fapp7010110&amp;rft.aulast=Bux&amp;rft.aufirst=Allah&amp;rft.au=Angelov%2C+Plamen&amp;rft.au=Habib%2C+Zulfiqar&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AActivity+recognition" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-33"><span class="mw-cite-backlink"><b><a href="#cite_ref-33">^</a></b></span> <span class="reference-text"><cite class="citation journal">Martinez-del-Rincon, Jesus; Santofimia, Maria Jose; Nebel, Jean-Christophe (2013). <a rel="nofollow" class="external text" href="https://pure.qub.ac.uk/portal/en/publications/commonsense-reasoning-for-human-action-recognition(e99389c4-021b-4893-80c9-d11ad02e36ec).html">"Common Sense Reasoning for Human Action Recognition"</a>. <i>Pattern Recognition Letters</i>. <b>34</b> (15): 1849–1860. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1016%2Fj.patrec.2012.10.020">10.1016/j.patrec.2012.10.020</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Pattern+Recognition+Letters&amp;rft.atitle=Common+Sense+Reasoning+for+Human+Action+Recognition&amp;rft.volume=34&amp;rft.issue=15&amp;rft.pages=1849-1860&amp;rft.date=2013&amp;rft_id=info%3Adoi%2F10.1016%2Fj.patrec.2012.10.020&amp;rft.aulast=Martinez-del-Rincon&amp;rft.aufirst=Jesus&amp;rft.au=Santofimia%2C+Maria+Jose&amp;rft.au=Nebel%2C+Jean-Christophe&amp;rft_id=https%3A%2F%2Fpure.qub.ac.uk%2Fportal%2Fen%2Fpublications%2Fcommonsense-reasoning-for-human-action-recognition%28e99389c4-021b-4893-80c9-d11ad02e36ec%29.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AActivity+recognition" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-34"><span class="mw-cite-backlink"><b><a href="#cite_ref-34">^</a></b></span> <span class="reference-text"><cite class="citation book">Cantarero, R.; Santofimia, M. J.; Villa, D.; Requena, R.; Campos, M.; Florez-Revuelta, F.; Nebel, J.-C.; Martinez-del-Rincon, J.; Lopez, J. C. (2016). <a rel="nofollow" class="external text" href="https://pure.qub.ac.uk/portal/en/publications/kinect-and-episodic-reasoning-for-human-action-recognition(5c784922-1c2e-43f9-827d-6820a59fe79d).html"><i>Kinect and Episodic Reasoning for Human Action Recognition</i></a>. <i>International Conference on Distributed Computing and Artificial Intelligence (DCAI'16)</i>. Advances in Intelligent Systems and Computing. <b>474</b>. pp.&#160;147–154. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1007%2F978-3-319-40162-1_16">10.1007/978-3-319-40162-1_16</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-3-319-40161-4" title="Special:BookSources/978-3-319-40161-4">978-3-319-40161-4</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Kinect+and+Episodic+Reasoning+for+Human+Action+Recognition&amp;rft.series=Advances+in+Intelligent+Systems+and+Computing&amp;rft.pages=147-154&amp;rft.date=2016&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-319-40162-1_16&amp;rft.isbn=978-3-319-40161-4&amp;rft.aulast=Cantarero&amp;rft.aufirst=R.&amp;rft.au=Santofimia%2C+M.+J.&amp;rft.au=Villa%2C+D.&amp;rft.au=Requena%2C+R.&amp;rft.au=Campos%2C+M.&amp;rft.au=Florez-Revuelta%2C+F.&amp;rft.au=Nebel%2C+J.-C.&amp;rft.au=Martinez-del-Rincon%2C+J.&amp;rft.au=Lopez%2C+J.+C.&amp;rft_id=https%3A%2F%2Fpure.qub.ac.uk%2Fportal%2Fen%2Fpublications%2Fkinect-and-episodic-reasoning-for-human-action-recognition%285c784922-1c2e-43f9-827d-6820a59fe79d%29.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AActivity+recognition" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-Wang_Duan_Zhang_Niu_p=1657-35"><span class="mw-cite-backlink"><b><a href="#cite_ref-Wang_Duan_Zhang_Niu_p=1657_35-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Wang, Le; Duan, Xuhuan; Zhang, Qilin; Niu, Zhenxing; Hua, Gang; Zheng, Nanning (2018-05-22). <a rel="nofollow" class="external text" href="https://qilin-zhang.github.io/_pages/pdfs/Segment-Tube_Spatio-Temporal_Action_Localization_in_Untrimmed_Videos_with_Per-Frame_Segmentation.pdf">"Segment-Tube: Spatio-Temporal Action Localization in Untrimmed Videos with Per-Frame Segmentation"</a> <span class="cs1-format">(PDF)</span>. <i>Sensors</i>. <b>18</b> (5): 1657. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.3390%2Fs18051657">10.3390/s18051657</a>. <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/1424-8220">1424-8220</a>. <a href="/wiki/PubMed_Central" title="PubMed Central">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC5982167">5982167</a></span>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/29789447">29789447</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Sensors&amp;rft.atitle=Segment-Tube%3A+Spatio-Temporal+Action+Localization+in+Untrimmed+Videos+with+Per-Frame+Segmentation&amp;rft.volume=18&amp;rft.issue=5&amp;rft.pages=1657&amp;rft.date=2018-05-22&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC5982167&amp;rft.issn=1424-8220&amp;rft_id=info%3Apmid%2F29789447&amp;rft_id=info%3Adoi%2F10.3390%2Fs18051657&amp;rft.aulast=Wang&amp;rft.aufirst=Le&amp;rft.au=Duan%2C+Xuhuan&amp;rft.au=Zhang%2C+Qilin&amp;rft.au=Niu%2C+Zhenxing&amp;rft.au=Hua%2C+Gang&amp;rft.au=Zheng%2C+Nanning&amp;rft_id=https%3A%2F%2Fqilin-zhang.github.io%2F_pages%2Fpdfs%2FSegment-Tube_Spatio-Temporal_Action_Localization_in_Untrimmed_Videos_with_Per-Frame_Segmentation.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AActivity+recognition" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/>  <a href="/wiki/File:CC-BY_icon.svg" class="image"><img alt="CC-BY icon.svg" src="//upload.wikimedia.org/wikipedia/commons/thumb/1/16/CC-BY_icon.svg/50px-CC-BY_icon.svg.png" decoding="async" width="50" height="18" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/1/16/CC-BY_icon.svg/75px-CC-BY_icon.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/1/16/CC-BY_icon.svg/100px-CC-BY_icon.svg.png 2x" data-file-width="88" data-file-height="31" /></a>].</span>
</li>
<li id="cite_note-36"><span class="mw-cite-backlink"><b><a href="#cite_ref-36">^</a></b></span> <span class="reference-text">Jie Yin, Xiaoyong Chai and Qiang Yang, "<a rel="nofollow" class="external text" href="http://www.aaai.org/Papers/AAAI/2004/AAAI04-092.pdf">High-level Goal Recognition in a Wireless LAN</a>". In <i>Proceedings of the Nineteenth National Conference on Artificial Intelligence</i> (AAAI-04), San Jose, CA USA, July 2004. Pages 578-584</span>
</li>
<li id="cite_note-37"><span class="mw-cite-backlink"><b><a href="#cite_ref-37">^</a></b></span> <span class="reference-text">Xiaoyong Chai and Qiang Yang, "<a rel="nofollow" class="external text" href="http://www.aaai.org/Papers/AAAI/2005/AAAI05-001.pdf">Multiple-Goal Recognition From Low-level Signals</a>". <i>Proceedings of the Twentieth National Conference on Artificial Intelligence</i> (AAAI 2005), Pittsburgh, PA USA, July 2005. Pages 3-8.</span>
</li>
<li id="cite_note-38"><span class="mw-cite-backlink"><b><a href="#cite_ref-38">^</a></b></span> <span class="reference-text">Derek Hao Hu, Qiang Yang. "<a rel="nofollow" class="external text" href="http://www.aaai.org/Papers/AAAI/2008/AAAI08-216.pdf">CIGAR: Concurrent and Interleaving Goal and Activity Recognition</a>", to appear in AAAI 2008</span>
</li>
<li id="cite_note-39"><span class="mw-cite-backlink"><b><a href="#cite_ref-39">^</a></b></span> <span class="reference-text">Jie Yin, Dou Shen, Qiang Yang and Ze-nian Li "<a rel="nofollow" class="external text" href="http://www.aaai.org/Papers/AAAI/2005/AAAI05-005.pdf">Activity Recognition through Goal-Based Segmentation</a>". <i>Proceedings of the Twentieth National Conference on Artificial Intelligence</i> (AAAI 2005), Pittsburgh, PA USA, July 2005. Pages 28-33.</span>
</li>
<li id="cite_note-40"><span class="mw-cite-backlink"><b><a href="#cite_ref-40">^</a></b></span> <span class="reference-text"><a href="/wiki/Martha_E._Pollack" title="Martha E. Pollack">Pollack, M.E.</a>, and et al., L. E. B. 2003. "<a rel="nofollow" class="external text" href="http://www.math.ntua.gr/aarg/projects/Thalis_files/Tsamardinos_2.pdf">Autominder: an intelligent cognitive orthotic system for people with memory impairment</a>". <i>Robotics and Autonomous Systems</i> 44(3-4):273–282.</span>
</li>
<li id="cite_note-41"><span class="mw-cite-backlink"><b><a href="#cite_ref-41">^</a></b></span> <span class="reference-text">Gao, Lianli, et al. "<a rel="nofollow" class="external text" href="http://www.academia.edu/download/33116711/Gao_et_al_2012_EN2324_EcolInform.pdf">A Web-based semantic tagging and activity recognition system for species' accelerometry data</a>." Ecological Informatics 13 (2013): 47-56.</span>
</li>
</ol></div></div>
<!-- 
NewPP limit report
Parsed by mw1329
Cached time: 20190407103044
Cache expiry: 2592000
Dynamic content: false
CPU time usage: 0.260 seconds
Real time usage: 0.333 seconds
Preprocessor visited node count: 915/1000000
Preprocessor generated node count: 0/1500000
Post‐expand include size: 21146/2097152 bytes
Template argument size: 94/2097152 bytes
Highest expansion depth: 7/40
Expensive parser function count: 5/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 41078/5000000 bytes
Number of Wikibase entities loaded: 4/400
Lua time usage: 0.144/10.000 seconds
Lua memory usage: 3.47 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  289.694      1 -total
 70.71%  204.833      1 Template:Reflist
 38.18%  110.608      2 Template:Cite_book
 18.54%   53.696      1 Template:Research_paper
 16.24%   47.045      4 Template:Cite_journal
 13.07%   37.869      1 Template:Ambox
  3.73%   10.798      2 Template:Main
  1.12%    3.241      1 Template:Main_other
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:15795950-0!canonical and timestamp 20190407103044 and revision id 883580569
 -->
</div><noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>
		
		<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Activity_recognition&amp;oldid=883580569">https://en.wikipedia.org/w/index.php?title=Activity_recognition&amp;oldid=883580569</a>"</div>
		
		<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Cognition" title="Category:Cognition">Cognition</a></li><li><a href="/wiki/Category:Artificial_intelligence_applications" title="Category:Artificial intelligence applications">Artificial intelligence applications</a></li><li><a href="/wiki/Category:Applied_machine_learning" title="Category:Applied machine learning">Applied machine learning</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:Wikipedia_articles_with_style_issues_from_March_2018" title="Category:Wikipedia articles with style issues from March 2018">Wikipedia articles with style issues from March 2018</a></li><li><a href="/wiki/Category:All_articles_with_style_issues" title="Category:All articles with style issues">All articles with style issues</a></li></ul></div></div>
		
		<div class="visualClear"></div>
		
	</div>
</div>

		<div id="mw-navigation">
			<h2>Navigation menu</h2>
			<div id="mw-head">
									<div id="p-personal" role="navigation" aria-labelledby="p-personal-label">
						<h3 id="p-personal-label">Personal tools</h3>
						<ul>
							<li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Activity+recognition" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Activity+recognition" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o">Log in</a></li>						</ul>
					</div>
									<div id="left-navigation">
										<div id="p-namespaces" role="navigation" class="vectorTabs" aria-labelledby="p-namespaces-label">
						<h3 id="p-namespaces-label">Namespaces</h3>
						<ul>
							<li id="ca-nstab-main" class="selected"><span><a href="/wiki/Activity_recognition" title="View the content page [c]" accesskey="c">Article</a></span></li><li id="ca-talk"><span><a href="/wiki/Talk:Activity_recognition" rel="discussion" title="Discussion about the content page [t]" accesskey="t">Talk</a></span></li>						</ul>
					</div>
										<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
												<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-variants-label" />
						<h3 id="p-variants-label">
							<span>Variants</span>
						</h3>
						<ul class="menu">
													</ul>
					</div>
									</div>
				<div id="right-navigation">
										<div id="p-views" role="navigation" class="vectorTabs" aria-labelledby="p-views-label">
						<h3 id="p-views-label">Views</h3>
						<ul>
							<li id="ca-view" class="collapsible selected"><span><a href="/wiki/Activity_recognition">Read</a></span></li><li id="ca-edit" class="collapsible"><span><a href="/w/index.php?title=Activity_recognition&amp;action=edit" title="Edit this page [e]" accesskey="e">Edit</a></span></li><li id="ca-history" class="collapsible"><span><a href="/w/index.php?title=Activity_recognition&amp;action=history" title="Past revisions of this page [h]" accesskey="h">View history</a></span></li>						</ul>
					</div>
										<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
						<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-cactions-label" />
						<h3 id="p-cactions-label"><span>More</span></h3>
						<ul class="menu">
													</ul>
					</div>
										<div id="p-search" role="search">
						<h3>
							<label for="searchInput">Search</label>
						</h3>
						<form action="/w/index.php" id="searchform">
							<div id="simpleSearch">
								<input type="search" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" accesskey="f" id="searchInput"/><input type="hidden" value="Special:Search" name="title"/><input type="submit" name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton"/><input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton"/>							</div>
						</form>
					</div>
									</div>
			</div>
			<div id="mw-panel">
				<div id="p-logo" role="banner"><a class="mw-wiki-logo" href="/wiki/Main_Page" title="Visit the main page"></a></div>
						<div class="portal" role="navigation" id="p-navigation" aria-labelledby="p-navigation-label">
			<h3 id="p-navigation-label">Navigation</h3>
			<div class="body">
								<ul>
					<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-interaction" aria-labelledby="p-interaction-label">
			<h3 id="p-interaction-label">Interaction</h3>
			<div class="body">
								<ul>
					<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-tb" aria-labelledby="p-tb-label">
			<h3 id="p-tb-label">Tools</h3>
			<div class="body">
								<ul>
					<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Activity_recognition" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Activity_recognition" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Activity_recognition&amp;oldid=883580569" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Activity_recognition&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q4677630" title="Link to connected data repository item [g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Activity_recognition&amp;id=883580569" title="Information on how to cite this page">Cite this page</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-coll-print_export" aria-labelledby="p-coll-print_export-label">
			<h3 id="p-coll-print_export-label">Print/export</h3>
			<div class="body">
								<ul>
					<li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Activity+recognition">Create a book</a></li><li id="coll-download-as-rdf2latex"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=Activity+recognition&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a href="/w/index.php?title=Activity_recognition&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-lang" aria-labelledby="p-lang-label">
			<h3 id="p-lang-label">Languages</h3>
			<div class="body">
								<ul>
					<li class="interlanguage-link interwiki-fa"><a href="https://fa.wikipedia.org/wiki/%D8%B4%D9%86%D8%A7%D8%AE%D8%AA_%D9%81%D8%B9%D8%A7%D9%84%DB%8C%D8%AA" title="شناخت فعالیت – Persian" lang="fa" hreflang="fa" class="interlanguage-link-target">فارسی</a></li>				</ul>
				<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q4677630#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>			</div>
		</div>
				</div>
		</div>
				<div id="footer" role="contentinfo">
						<ul id="footer-info">
								<li id="footer-info-lastmod"> This page was last edited on 16 February 2019, at 07:55<span class="anonymous-show">&#160;(UTC)</span>.</li>
								<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="//creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
							</ul>
						<ul id="footer-places">
								<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
								<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
								<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
								<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
								<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
								<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
								<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=Activity_recognition&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
							</ul>
										<ul id="footer-icons" class="noprint">
										<li id="footer-copyrightico">
						<a href="https://wikimediafoundation.org/"><img src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation"/></a>					</li>
										<li id="footer-poweredbyico">
						<a href="//www.mediawiki.org/"><img src="/static/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"/></a>					</li>
									</ul>
						<div style="clear: both;"></div>
		</div>
		

<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.260","walltime":"0.333","ppvisitednodes":{"value":915,"limit":1000000},"ppgeneratednodes":{"value":0,"limit":1500000},"postexpandincludesize":{"value":21146,"limit":2097152},"templateargumentsize":{"value":94,"limit":2097152},"expansiondepth":{"value":7,"limit":40},"expensivefunctioncount":{"value":5,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":41078,"limit":5000000},"entityaccesscount":{"value":4,"limit":400},"timingprofile":["100.00%  289.694      1 -total"," 70.71%  204.833      1 Template:Reflist"," 38.18%  110.608      2 Template:Cite_book"," 18.54%   53.696      1 Template:Research_paper"," 16.24%   47.045      4 Template:Cite_journal"," 13.07%   37.869      1 Template:Ambox","  3.73%   10.798      2 Template:Main","  1.12%    3.241      1 Template:Main_other"]},"scribunto":{"limitreport-timeusage":{"value":"0.144","limit":"10.000"},"limitreport-memusage":{"value":3641656,"limit":52428800}},"cachereport":{"origin":"mw1329","timestamp":"20190407103044","ttl":2592000,"transientcontent":false}}});mw.config.set({"wgBackendResponseTime":93,"wgHostname":"mw1332"});});</script>
</body>
</html>
