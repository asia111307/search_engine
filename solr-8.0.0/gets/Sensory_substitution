<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>Sensory substitution - Wikipedia</title>
<script>document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );</script>
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Sensory_substitution","wgTitle":"Sensory substitution","wgCurRevisionId":890525548,"wgRevisionId":890525548,"wgArticleId":1903855,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["CS1 maint: Uses authors parameter","Pages with DOIs inactive since 2019","CS1 maint: Multiple names: authors list","All articles with dead external links","Articles with dead external links from July 2016","CS1 maint: Archived copy as title","All articles with specifically marked weasel-worded phrases","Articles with specifically marked weasel-worded phrases from March 2017","All articles with unsourced statements","Articles with unsourced statements from March 2016","Articles with a promotional tone from January 2018","All articles with a promotional tone","Cognitive neuroscience","Biomedical engineering","Neural engineering","Neuroprosthetics"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Sensory_substitution","wgRelevantArticleId":1903855,"wgRequestId":"XLJSRwpAMEkAAD-SEEIAAAAC","wgCSPNonce":false,"wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgFlaggedRevsParams":{"tags":{}},"wgStableRevisionId":null,"wgBetaFeaturesFeatures":[],"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsReferencePreviews":false,"wgPopupsShouldSendModuleToUser":true,"wgPopupsConflictsWithNavPopupGadget":false,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en","usePageImages":true,"usePageDescriptions":true},"wgMFDisplayWikibaseDescriptions":{"search":true,"nearby":true,"watchlist":true,"tagline":false},"wgRelatedArticles":null,"wgRelatedArticlesUseCirrusSearch":true,"wgRelatedArticlesOnlyUseCirrusSearch":false,"wgWMESchemaEditAttemptStepOversample":false,"wgPoweredByHHVM":true,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralNoticeCookiesToDelete":[],"wgCentralNoticeCategoriesUsingLegacy":["Fundraising","fundraising"],"wgWikibaseItemId":"Q356133","wgScoreNoteLanguages":{"arabic":"العربية","catalan":"català","deutsch":"Deutsch","english":"English","espanol":"español","italiano":"italiano","nederlands":"Nederlands","norsk":"norsk","portugues":"português","suomi":"suomi","svenska":"svenska","vlaams":"West-Vlams"},"wgScoreDefaultNoteLanguage":"nederlands","wgCentralAuthMobileDomain":false,"wgVisualEditorUnsupportedEditParams":["undo","undoafter","veswitched"],"wgEditSubmitButtonLabelPublish":true});mw.loader.state({"ext.gadget.charinsert-styles":"ready","ext.globalCssJs.user.styles":"ready","ext.globalCssJs.site.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","ext.globalCssJs.site":"ready","user":"ready","user.options":"ready","user.tokens":"loading","ext.cite.styles":"ready","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","mediawiki.toc.styles":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","ext.3d.styles":"ready","mediawiki.skinning.interface":"ready","skins.vector.styles":"ready"});mw.loader.implement("user.tokens@0tffind",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});RLPAGEMODULES=["ext.cite.ux-enhancements","site","mediawiki.page.startup","mediawiki.page.ready","mediawiki.toc","mediawiki.searchSuggest","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.eventlogger","ext.uls.init","ext.uls.compactlinks","ext.uls.interface","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp","skins.vector.js"];mw.loader.load(RLPAGEMODULES);});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.3d.styles%7Cext.cite.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.skinning.interface%7Cmediawiki.toc.styles%7Cskins.vector.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.gadget.charinsert-styles&amp;only=styles&amp;skin=vector"/>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.33.0-wmf.25"/>
<meta name="referrer" content="origin"/>
<meta name="referrer" content="origin-when-crossorigin"/>
<meta name="referrer" content="origin-when-cross-origin"/>
<link rel="alternate" href="android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/Sensory_substitution"/>
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Sensory_substitution&amp;action=edit"/>
<link rel="edit" title="Edit this page" href="/w/index.php?title=Sensory_substitution&amp;action=edit"/>
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png"/>
<link rel="shortcut icon" href="/static/favicon/wikipedia.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd"/>
<link rel="license" href="//creativecommons.org/licenses/by-sa/3.0/"/>
<link rel="canonical" href="https://en.wikipedia.org/wiki/Sensory_substitution"/>
<link rel="dns-prefetch" href="//login.wikimedia.org"/>
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<!--[if lt IE 9]><script src="/w/load.php?lang=en&amp;modules=html5shiv&amp;only=scripts&amp;skin=vector&amp;sync=1"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Sensory_substitution rootpage-Sensory_substitution skin-vector action-view">
<div id="mw-page-base" class="noprint"></div>
<div id="mw-head-base" class="noprint"></div>
<div id="content" class="mw-body" role="main">
	<a id="top"></a>
	<div id="siteNotice" class="mw-body-content"><!-- CentralNotice --></div>
	<div class="mw-indicators mw-body-content">
</div>

	<h1 id="firstHeading" class="firstHeading" lang="en">Sensory substitution</h1>
	
	<div id="bodyContent" class="mw-body-content">
		<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
		<div id="contentSub"></div>
		
		
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="#p-search">Jump to search</a>
		<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="mw-parser-output"><p><b>Sensory substitution</b> is a change of the characteristics of one <a href="/wiki/Sensory_modality" class="mw-redirect" title="Sensory modality">sensory modality</a> into stimuli of another sensory modality.
</p><p>A sensory substitution system consists of three parts: a sensor, a coupling system, and a stimulator. The sensor records stimuli and gives them to a coupling system which interprets these signals and transmits them to a stimulator. In case the sensor obtains signals of a kind not originally available to the bearer it is a case of <a href="#Sensory_augmentation">sensory augmentation</a>. Sensory substitution concerns human <a href="/wiki/Perception" title="Perception">perception</a> and the <a href="/wiki/Neuroplasticity" title="Neuroplasticity">plasticity</a> of the human brain; and therefore, allows us to study these aspects of neuroscience more through <a href="/wiki/Neuroimaging" title="Neuroimaging">neuroimaging</a>.
</p><p>It is hoped<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Manual_of_Style/Words_to_watch#Unsupported_attributions" title="Wikipedia:Manual of Style/Words to watch"><span title="The material near this tag may use weasel words or too-vague attribution. (March 2017)">according to whom?</span></a></i>&#93;</sup> that sensory substitution systems can help people by restoring their ability to perceive  certain defective sensory modality by using sensory information from a functioning sensory modality.
</p>
<div id="toc" class="toc"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2>Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#History"><span class="tocnumber">1</span> <span class="toctext">History</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Physiology"><span class="tocnumber">2</span> <span class="toctext">Physiology</span></a>
<ul>
<li class="toclevel-2 tocsection-3"><a href="#Technological_support"><span class="tocnumber">2.1</span> <span class="toctext">Technological support</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="#Brain_plasticity"><span class="tocnumber">2.2</span> <span class="toctext">Brain plasticity</span></a></li>
<li class="toclevel-2 tocsection-5"><a href="#Perception_versus_sensing"><span class="tocnumber">2.3</span> <span class="toctext">Perception versus sensing</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-6"><a href="#Different_applications"><span class="tocnumber">3</span> <span class="toctext">Different applications</span></a>
<ul>
<li class="toclevel-2 tocsection-7"><a href="#Tactile_systems"><span class="tocnumber">3.1</span> <span class="toctext">Tactile systems</span></a>
<ul>
<li class="toclevel-3 tocsection-8"><a href="#Tactile–visual"><span class="tocnumber">3.1.1</span> <span class="toctext">Tactile–visual</span></a></li>
<li class="toclevel-3 tocsection-9"><a href="#Tactile–auditory"><span class="tocnumber">3.1.2</span> <span class="toctext">Tactile–auditory</span></a></li>
<li class="toclevel-3 tocsection-10"><a href="#Tactile–vestibular"><span class="tocnumber">3.1.3</span> <span class="toctext">Tactile–vestibular</span></a></li>
<li class="toclevel-3 tocsection-11"><a href="#Tactile–tactile_to_restore_peripheral_sensation"><span class="tocnumber">3.1.4</span> <span class="toctext">Tactile–tactile to restore peripheral sensation</span></a></li>
<li class="toclevel-3 tocsection-12"><a href="#Tactile_feedback_system_for_prosthetic_limbs"><span class="tocnumber">3.1.5</span> <span class="toctext">Tactile feedback system for prosthetic limbs</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-13"><a href="#Auditory_systems"><span class="tocnumber">3.2</span> <span class="toctext">Auditory systems</span></a>
<ul>
<li class="toclevel-3 tocsection-14"><a href="#Auditory_vision_substitution"><span class="tocnumber">3.2.1</span> <span class="toctext">Auditory vision substitution</span></a>
<ul>
<li class="toclevel-4 tocsection-15"><a href="#The_vOICe_Auditory_Display"><span class="tocnumber">3.2.1.1</span> <span class="toctext">The vOICe Auditory Display</span></a></li>
<li class="toclevel-4 tocsection-16"><a href="#Project_Bat-eye"><span class="tocnumber">3.2.1.2</span> <span class="toctext">Project Bat-eye</span></a></li>
<li class="toclevel-4 tocsection-17"><a href="#EyeMusic"><span class="tocnumber">3.2.1.3</span> <span class="toctext">EyeMusic</span></a></li>
<li class="toclevel-4 tocsection-18"><a href="#LibreAudioView"><span class="tocnumber">3.2.1.4</span> <span class="toctext">LibreAudioView</span></a></li>
<li class="toclevel-4 tocsection-19"><a href="#PSVA"><span class="tocnumber">3.2.1.5</span> <span class="toctext">PSVA</span></a></li>
<li class="toclevel-4 tocsection-20"><a href="#The_Vibe"><span class="tocnumber">3.2.1.6</span> <span class="toctext">The Vibe</span></a></li>
<li class="toclevel-4 tocsection-21"><a href="#Other_systems"><span class="tocnumber">3.2.1.7</span> <span class="toctext">Other systems</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-2 tocsection-22"><a href="#Nervous_system_implants"><span class="tocnumber">3.3</span> <span class="toctext">Nervous system implants</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-23"><a href="#Criticism"><span class="tocnumber">4</span> <span class="toctext">Criticism</span></a></li>
<li class="toclevel-1 tocsection-24"><a href="#Sensory_augmentation"><span class="tocnumber">5</span> <span class="toctext">Sensory augmentation</span></a>
<ul>
<li class="toclevel-2 tocsection-25"><a href="#Magnetic_perception"><span class="tocnumber">5.1</span> <span class="toctext">Magnetic perception</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-26"><a href="#See_also"><span class="tocnumber">6</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-27"><a href="#References"><span class="tocnumber">7</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1 tocsection-28"><a href="#External_links"><span class="tocnumber">8</span> <span class="toctext">External links</span></a></li>
</ul>
</div>

<h2><span class="mw-headline" id="History">History</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit&amp;section=1" title="Edit section: History">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The idea of sensory substitution was introduced in the '80s by <a href="/wiki/Paul_Bach-y-Rita" title="Paul Bach-y-Rita">Paul Bach-y-Rita</a> as a means of using one sensory modality, mainly <a href="/wiki/Touch" class="mw-redirect" title="Touch">taction</a>, to gain environmental information to be used by another sensory modality, mainly <a href="/wiki/Visual_perception" title="Visual perception">vision</a>.<sup id="cite_ref-TVSS_1-0" class="reference"><a href="#cite_note-TVSS-1">&#91;1&#93;</a></sup><sup id="cite_ref-2" class="reference"><a href="#cite_note-2">&#91;2&#93;</a></sup> Thereafter, the entire field was discussed by Chaim-Meyer Scheff in "Experimental model for the study of changes in the organization of human sensory information processing through the design and testing of non-invasive prosthetic devices for sensory impaired people".<sup id="cite_ref-3" class="reference"><a href="#cite_note-3">&#91;3&#93;</a></sup> The first sensory substitution system was developed by Bach-y-Rita et al. as a means of brain plasticity in congenitally blind individuals.<sup id="cite_ref-4" class="reference"><a href="#cite_note-4">&#91;4&#93;</a></sup> After this historic invention, sensory substitution has been the basis of many studies investigating perceptive and <a href="/wiki/Cognitive_neuroscience" title="Cognitive neuroscience">cognitive neuroscience</a>. Since then, sensory substitution has contributed to the study of brain function, human <a href="/wiki/Cognition" title="Cognition">cognition</a> and rehabilitation.<sup id="cite_ref-Three_5-0" class="reference"><a href="#cite_note-Three-5">&#91;5&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Physiology">Physiology</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit&amp;section=2" title="Edit section: Physiology">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>When a person becomes blind or deaf they generally do not lose the ability to hear or see; they simply lose their ability to transmit the sensory signals from the periphery (<a href="/wiki/Retina" title="Retina">retina</a> for visions and <a href="/wiki/Cochlea" title="Cochlea">cochlea</a> for hearing) to brain.<sup id="cite_ref-bach_6-0" class="reference"><a href="#cite_note-bach-6">&#91;6&#93;</a></sup> Since the vision processing pathways are still intact, a person who has lost the ability to retrieve data from the retina can still see subjective images by using data gathered from other sensory modalities such as touch or audition.<sup id="cite_ref-Regan,_JK_2001_7-0" class="reference"><a href="#cite_note-Regan,_JK_2001-7">&#91;7&#93;</a></sup>
</p><p>In a regular visual system, the data collected by the retina is converted into an electrical stimulus in the <a href="/wiki/Optic_nerve" title="Optic nerve">optic nerve</a> and relayed to the brain, which re-creates the image and perceives it. Because it is the brain that is responsible for the final perception, sensory substitution is possible. During sensory substitution an intact sensory modality relays information to the visual perception areas of the brain so that the person can perceive sight. With sensory substitution, information gained from one sensory modality can reach brain structures physiologically related to other sensory modalities. Touch-to-visual sensory substitution transfers information from touch receptors to the visual cortex for interpretation and perception. For example, through <a href="/wiki/FMRI" class="mw-redirect" title="FMRI">fMRI</a>, one can determine which parts of the brain are activated during sensory perception. In blind persons, it is seen that while they are only receiving tactile information, their visual cortex is also activated as they perceive <i>sight</i> objects.<sup id="cite_ref-8" class="reference"><a href="#cite_note-8">&#91;8&#93;</a></sup> Touch-to-touch sensory substitution is also possible, wherein information from touch receptors of one region of the body can be used to perceive touch in another region. For example, in one experiment by Bach-y-Rita,touch perception was able to be restored in a patient who lost peripheral sensation due to leprosy.<sup id="cite_ref-six_9-0" class="reference"><a href="#cite_note-six-9">&#91;9&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Technological_support">Technological support</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit&amp;section=3" title="Edit section: Technological support">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>In order to achieve sensory substitution and stimulate the brain without intact sensory organs to relay the information, machines can be used to do the signal transduction, rather than the sensory organs. This <a href="/wiki/Brain%E2%80%93machine_interface" class="mw-redirect" title="Brain–machine interface">brain–machine interface</a> collects external signals and transforms them into electrical signals for the brain to interpret. Generally, a camera or a microphone is used to collect visual or auditory stimuli that are used to replace lost sight and hearing, respectively. The visual or auditory data collected from the sensors is transformed into tactile stimuli that are then relayed to the brain for visual and auditory perception. This and all types of sensory substitution are only possible due to neuroplasticity.<sup id="cite_ref-six_9-1" class="reference"><a href="#cite_note-six-9">&#91;9&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Brain_plasticity">Brain plasticity</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit&amp;section=4" title="Edit section: Brain plasticity">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><i><a href="/wiki/Neuroplasticity" title="Neuroplasticity">Brain plasticity</a></i> refers to the brain's ability to adapt to a changing environment, for instance to the absence or deterioration of a sense. It is conceivable that <a href="/wiki/Cortical_remapping" title="Cortical remapping">cortical remapping</a> or reorganization in response to the loss of one sense may be an evolutionary mechanism that allows people to adapt and compensate by using other senses better. Functional imaging of congenitally blind patients showed a cross-modal recruitment of the <a href="/wiki/Occipital_cortex" class="mw-redirect" title="Occipital cortex">occipital cortex</a> during perceptual tasks such as Braille reading, tactile perception, tactual object recognition, sound localization, and sound discrimination.<sup id="cite_ref-Three_5-1" class="reference"><a href="#cite_note-Three-5">&#91;5&#93;</a></sup> This may suggest that blind people can use their occipital lobe, generally used for vision, to perceive objects through the use of other sensory modalities. This <a href="/wiki/Cross_modal_plasticity" title="Cross modal plasticity">cross modal plasticity</a> may explain the often described tendency of blind people to show enhanced ability in the other senses.<sup id="cite_ref-10" class="reference"><a href="#cite_note-10">&#91;10&#93;</a></sup><sup id="cite_ref-11" class="reference"><a href="#cite_note-11">&#91;11&#93;</a></sup><sup id="cite_ref-12" class="reference"><a href="#cite_note-12">&#91;12&#93;</a></sup><sup id="cite_ref-13" class="reference"><a href="#cite_note-13">&#91;13&#93;</a></sup><sup id="cite_ref-14" class="reference"><a href="#cite_note-14">&#91;14&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Perception_versus_sensing">Perception versus sensing</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit&amp;section=5" title="Edit section: Perception versus sensing">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>While considering the physiological aspects of sensory substitution, it is essential to distinguish between sensing and perceiving. The general question posed by this differentiation is: Are blind people seeing or <i>perceiving</i> to see by putting together different sensory data? While sensation comes in one modality – visual, auditory, tactile etc. – perception due to sensory substitution is not one modality but a result of cross-modal interactions. It is therefore concluded that while sensory substitution for vision induces visual-like perception in <i>sighted</i> individuals, it induces auditory or tactile perception in <i>blind</i> individuals.<sup id="cite_ref-eight_15-0" class="reference"><a href="#cite_note-eight-15">&#91;15&#93;</a></sup> In short, blind people <i>perceive</i> to see through touch and audition with sensory substitution.
</p>
<h2><span class="mw-headline" id="Different_applications">Different applications</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit&amp;section=6" title="Edit section: Different applications">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Applications are not restricted to handicapped persons, but also include <a href="/wiki/Art" title="Art">artistic</a> presentations, <a href="/wiki/Game" title="Game">games</a>, and <a href="/wiki/Augmented_reality" title="Augmented reality">augmented reality</a>. Some examples are substitution of visual stimuli to audio or tactile, and of audio stimuli to tactile. Some of the most popular are probably Paul Bach-y-Rita's Tactile Vision Sensory Substitution (TVSS), developed with Carter Collins at <a href="/w/index.php?title=Smith-Kettlewell_Institute&amp;action=edit&amp;redlink=1" class="new" title="Smith-Kettlewell Institute (page does not exist)">Smith-Kettlewell Institute</a> and <a href="/w/index.php?title=Peter_Bartus_Leonard_Meijer&amp;action=edit&amp;redlink=1" class="new" title="Peter Bartus Leonard Meijer (page does not exist)">Peter Meijer</a>'s Seeing with Sound approach (The vOICe). Technical developments, such as <a href="/wiki/Miniaturization" title="Miniaturization">miniaturization</a> and <a href="/wiki/Electrical_stimulation" class="mw-redirect" title="Electrical stimulation">electrical stimulation</a> help the advance of sensory substitution devices.
</p><p>In sensory substitution systems, we generally have sensors that collect the data from the external environment. This data is then relayed to a coupling system that interprets and transduces the information and then replays it to a stimulator. This stimulator ultimately stimulates a functioning sensory modality.<sup id="cite_ref-eight_15-1" class="reference"><a href="#cite_note-eight-15">&#91;15&#93;</a></sup> After training, people learn to use the information gained from this stimulation to experience a perception of the sensation they lack instead of the actually stimulated sensation. For example, a leprosy patient, whose perception of peripheral touch was restored, was equipped with a glove containing artificial contact sensors coupled to skin sensory receptors on the forehead (which was stimulated). After training and acclimation, the patient was able to experience data from the glove as if it was originating in the fingertips while ignoring the sensations in the forehead.<sup id="cite_ref-six_9-2" class="reference"><a href="#cite_note-six-9">&#91;9&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Tactile_systems">Tactile systems</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit&amp;section=7" title="Edit section: Tactile systems">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>To understand <i>tactile sensory substitution</i> it is essential to understand some basic physiology of the tactile receptors of the skin. There are five basic types of tactile receptors: <a href="/wiki/Lamellar_corpuscle" title="Lamellar corpuscle">Pacinian corpuscle</a>, <a href="/wiki/Meissner%27s_corpuscle" class="mw-redirect" title="Meissner&#39;s corpuscle">Meissner's corpuscle</a>, <a href="/wiki/Bulbous_corpuscle" title="Bulbous corpuscle">Ruffini endings</a>, <a href="/wiki/Merkel_nerve_ending" title="Merkel nerve ending">Merkel nerve endings</a>, and <a href="/wiki/Free_nerve_ending" title="Free nerve ending">free nerve endings</a>. These receptors are mainly characterized by which type of stimuli best activates them, and by their rate of adaptation to sustained stimuli.<sup id="cite_ref-16" class="reference"><a href="#cite_note-16">&#91;16&#93;</a></sup> Because of the rapid adaptation of some of these receptors to sustained stimuli, those receptors require rapidly changing tactile stimulation systems in order to be optimally activated.<sup id="cite_ref-EV_17-0" class="reference"><a href="#cite_note-EV-17">&#91;17&#93;</a></sup> Among all these mechanoreceptors Pacinian corpuscle offers the highest sensitivity to high frequency vibration starting from few 10s of Hz to a few kHz with the help of its specialized <a href="/wiki/Mechanotransduction" title="Mechanotransduction">mechanotransduction</a> mechanism.<sup id="cite_ref-18" class="reference"><a href="#cite_note-18">&#91;18&#93;</a></sup><sup id="cite_ref-19" class="reference"><a href="#cite_note-19">&#91;19&#93;</a></sup>
</p><p>There have been two different types of stimulators: electrotactile or vibrotactile. Electrotactile stimulators use direct electrical stimulation of the nerve ending in the skin to initiate the action potentials; the sensation triggered, burn, itch, pain, pressure etc. depends on the stimulating voltage. Vibrotactile stimulators use pressure and the properties of the mechanoreceptors of the skin to initiate action potentials. There are advantages and disadvantages for both these stimulation systems. With the electrotactile stimulating systems a lot of factors affect the sensation triggered: stimulating voltage, current, waveform, electrode size, material, contact force, skin location, thickness and hydration.<sup id="cite_ref-EV_17-1" class="reference"><a href="#cite_note-EV-17">&#91;17&#93;</a></sup> Electrotactile stimulation may involve the direct stimulation of the nerves (<a href="/wiki/Percutaneous" title="Percutaneous">percutaneous</a>), or through the skin (<a href="/w/index.php?title=Transcutaneous&amp;action=edit&amp;redlink=1" class="new" title="Transcutaneous (page does not exist)">transcutaneous</a>). Percutaneous application causes additional distress to the patient, and is a major disadvantage of this approach. Furthermore, stimulation of the skin without insertion leads to the need for high voltage stimulation because of the high impedance of the dry skin,<sup id="cite_ref-EV_17-2" class="reference"><a href="#cite_note-EV-17">&#91;17&#93;</a></sup> unless the tongue is used as a receptor, which requires only about 3% as much voltage.<sup id="cite_ref-tong_20-0" class="reference"><a href="#cite_note-tong-20">&#91;20&#93;</a></sup> This latter technique is undergoing clinical trials for various applications, and been approved for assistance to the blind in the UK.<sup id="cite_ref-21" class="reference"><a href="#cite_note-21">&#91;21&#93;</a></sup><sup id="cite_ref-22" class="reference"><a href="#cite_note-22">&#91;22&#93;</a></sup> Alternatively, the roof of the mouth has been proposed as another area where low currents can be felt.<sup id="cite_ref-23" class="reference"><a href="#cite_note-23">&#91;23&#93;</a></sup>
</p><p><a href="/wiki/Electrostatic" class="mw-redirect" title="Electrostatic">Electrostatic</a> arrays are explored as <a href="/wiki/Human-computer_interaction" class="mw-redirect" title="Human-computer interaction">human-computer interaction</a> devices for <a href="/wiki/Touch_screen" class="mw-redirect" title="Touch screen">touch screens</a>.<sup id="cite_ref-24" class="reference"><a href="#cite_note-24">&#91;24&#93;</a></sup> These are based on a phenomenon called <a href="/wiki/Electrovibration" title="Electrovibration">electrovibration</a>, which allows microamperre-level currents to be felt as roughness on a surface.<sup id="cite_ref-25" class="reference"><a href="#cite_note-25">&#91;25&#93;</a></sup><sup id="cite_ref-26" class="reference"><a href="#cite_note-26">&#91;26&#93;</a></sup>
</p><p>Vibrotactile systems use the properties of mechanoreceptors in the skin so they have fewer parameters that need to be monitored as compared to electrotactile stimulation. However, vibrotactile stimulation systems need to account for the rapid adaptation of the tactile sense.
</p><p>Another important aspect of tactile sensory substitution systems is the location of the tactile stimulation. Tactile receptors are abundant on the fingertips, face, and tongue while sparse on the back, legs and arms. It is essential to take into account the spatial resolution of the receptor as it has a major effect on the resolution of the sensory substitution.<sup id="cite_ref-EV_17-3" class="reference"><a href="#cite_note-EV-17">&#91;17&#93;</a></sup> A high resolution pin-arrayed display is able to present spatial information via tactile symbols, such as city maps<sup id="cite_ref-Zeng,_2015_27-0" class="reference"><a href="#cite_note-Zeng,_2015-27">&#91;27&#93;</a></sup> and obstacle maps.<sup id="cite_ref-Zeng,_2012_28-0" class="reference"><a href="#cite_note-Zeng,_2012-28">&#91;28&#93;</a></sup>
</p><p>Below you can find some descriptions of current tactile substitution systems.
</p>
<h4><span id="Tactile.E2.80.93visual"></span><span class="mw-headline" id="Tactile–visual">Tactile–visual</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit&amp;section=8" title="Edit section: Tactile–visual">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>One of the earliest and most well known form of sensory substitution devices was Paul Bach-y-Rita's TVSS that converted the image from a video camera into a tactile image and coupled it to the tactile receptors on the <a href="/wiki/Back" class="mw-redirect" title="Back">back</a> of his blind subject.<sup id="cite_ref-TVSS_1-1" class="reference"><a href="#cite_note-TVSS-1">&#91;1&#93;</a></sup> Recently, several new systems have been developed that interface the tactile image to tactile receptors on different areas of the body such as the on the chest, brow, fingertip, abdomen, and forehead.<sup id="cite_ref-bach_6-1" class="reference"><a href="#cite_note-bach-6">&#91;6&#93;</a></sup> The tactile image is produced by hundreds of activators placed on the person. The activators are <a href="/wiki/Solenoid" title="Solenoid">solenoids</a> of one millimeter diameter. In experiments, <a href="/wiki/Blindness" class="mw-redirect" title="Blindness">blind</a> (or <a href="/wiki/Blindfold" title="Blindfold">blindfolded</a>) subjects equipped with the TVSS can learn to detect shapes and to orient themselves. In the case of simple geometric shapes, it took around 50 trials to achieve 100 percent correct recognition. To identify objects in different orientations requires several hours of learning.
</p><p>A system using the tongue as the human-machine interface is most practical.  The tongue-machine interface is both protected by the closed mouth and the saliva in the mouth provides a good electrolytic environment that ensures good electrode contact.<sup id="cite_ref-tong_20-1" class="reference"><a href="#cite_note-tong-20">&#91;20&#93;</a></sup> Results from a study by Bach-y-Rita et al. show that electrotactile stimulation of the tongue required 3% of the voltage required to stimulate the finger.<sup id="cite_ref-tong_20-2" class="reference"><a href="#cite_note-tong-20">&#91;20&#93;</a></sup> Also, since it is more practical to wear an orthodontic retainer holding the stimulation system than an apparatus strapped to other parts of the body, the tongue-machine interface is more popular among TVSS systems.
</p><p>This tongue TVSS system works by delivering electrotactile stimuli to the dorsum of the tongue via a flexible <a href="/wiki/Electrode_array" title="Electrode array">electrode array</a> placed in the mouth. This electrode array is connected to a Tongue Display Unit [TDU] via a ribbon cable passing out of the mouth. A video camera records a picture, transfers it to the TDU for conversion into a tactile image. The tactile image is then projected onto the tongue via the ribbon cable where the tongue's receptors pick up the signal. After training, subjects are able to associate certain types of stimuli to certain types of visual images.<sup id="cite_ref-bach_6-2" class="reference"><a href="#cite_note-bach-6">&#91;6&#93;</a></sup><sup id="cite_ref-29" class="reference"><a href="#cite_note-29">&#91;29&#93;</a></sup> In this way, tactile sensation can be used for visual perception.
</p><p>Sensory substitutions have also been successful with the emergence of wearable haptic actuators like vibrotactile motors, solenoids, peltier diodes, etc. At the <a href="/w/index.php?title=Center_for_Cognitive_Ubiquitous_Computing&amp;action=edit&amp;redlink=1" class="new" title="Center for Cognitive Ubiquitous Computing (page does not exist)">Center for Cognitive Ubiquitous Computing</a> at <a href="/wiki/Arizona_State_University" title="Arizona State University">Arizona State University</a>, researchers have developed technologies that enable people who are blind to perceive social situational information using wearable vibrotactile belts<sup id="cite_ref-Haptic_Belt_30-0" class="reference"><a href="#cite_note-Haptic_Belt-30">&#91;30&#93;</a></sup> (Haptic Belt) and gloves<sup id="cite_ref-31" class="reference"><a href="#cite_note-31">&#91;31&#93;</a></sup><sup id="cite_ref-Haptic_Glove_32-0" class="reference"><a href="#cite_note-Haptic_Glove-32">&#91;32&#93;</a></sup> (VibroGlove). Both technologies use miniature cameras that are mounted on a pair of glasses worn by the user who is blind. The Haptic Belt provides vibrations that convey the direction and distance at which a person is standing in front of a user, while the VibroGlove uses spatio-temporal mapping of vibration patterns to convey facial expressions of the interaction partner. Alternatively, it has been shown that even very simple cues indicating the presence or absence of obstacles (through small vibration modules located at strategic places in the body) can be useful for navigation, gait stabilization and reduced anxiety when evolving in an unknown space. This approach, called the "Haptic Radar"<sup id="cite_ref-33" class="reference"><a href="#cite_note-33">&#91;33&#93;</a></sup> has been studied since 2005 by researchers at the <a href="/wiki/University_of_Tokyo" title="University of Tokyo">University of Tokyo</a> in collaboration with the <a href="/wiki/University_of_Rio_de_Janeiro" class="mw-redirect" title="University of Rio de Janeiro">University of Rio de Janeiro</a>.<sup id="cite_ref-Haptic_Radar_34-0" class="reference"><a href="#cite_note-Haptic_Radar-34">&#91;34&#93;</a></sup>
</p>
<h4><span id="Tactile.E2.80.93auditory"></span><span class="mw-headline" id="Tactile–auditory">Tactile–auditory</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit&amp;section=9" title="Edit section: Tactile–auditory">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Neuroscientist <a href="/wiki/David_Eagleman" title="David Eagleman">David Eagleman</a> presented a new device for sound-to-touch hearing at TED in 2015<sup id="cite_ref-35" class="reference"><a href="#cite_note-35">&#91;35&#93;</a></sup>; his laboratory research then expanded into a company based in Palo Alto, California, called NeoSensory<sup id="cite_ref-36" class="reference"><a href="#cite_note-36">&#91;36&#93;</a></sup>.  NeoSensory devices capture sound and turn them into high-dimensional patterns of touch on the skin.<sup id="cite_ref-37" class="reference"><a href="#cite_note-37">&#91;37&#93;</a></sup><sup id="cite_ref-38" class="reference"><a href="#cite_note-38">&#91;38&#93;</a></sup>
</p><p>Experiments by Schurmann et al. show that tactile senses can activate the human auditory cortex. Currently vibrotactile stimuli can be used to facilitate hearing in normal and hearing-impaired people.<sup id="cite_ref-TASS_39-0" class="reference"><a href="#cite_note-TASS-39">&#91;39&#93;</a></sup> To test for the auditory areas activated by touch, Schurmann et al. tested subjects while stimulating their fingers and palms with vibration bursts and their fingertips with tactile pressure. They found that tactile stimulation of the fingers lead to activation of the auditory belt area, which suggests that there is a relationship between audition and tactition.<sup id="cite_ref-TASS_39-1" class="reference"><a href="#cite_note-TASS-39">&#91;39&#93;</a></sup> Therefore, future research can be done to investigate the likelihood of a tactile–auditory sensory substitution system. One promising<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs a reliable source. (March 2016)">citation needed</span></a></i>&#93;</sup> invention is the 'Sense organs synthesizer'<sup id="cite_ref-40" class="reference"><a href="#cite_note-40">&#91;40&#93;</a></sup> which aims at delivering a normal hearing range of nine octaves via 216 electrodes to sequential touch nerve zones, next to the spine.
</p>
<h4><span id="Tactile.E2.80.93vestibular"></span><span class="mw-headline" id="Tactile–vestibular">Tactile–vestibular</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit&amp;section=10" title="Edit section: Tactile–vestibular">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Some people with <a href="/wiki/Balance_disorder" title="Balance disorder">balance disorders</a> or adverse reactions to antibiotics suffer from bilateral vestibular damage (BVD). They experience difficulty maintaining posture, unstable gait, and <a href="/wiki/Oscillopsia" title="Oscillopsia">oscillopsia</a>.<sup id="cite_ref-vest_41-0" class="reference"><a href="#cite_note-vest-41">&#91;41&#93;</a></sup> Tyler et al. studied the restitution of postural control through a tactile for vestibular sensory substitution. Because BVD patients cannot integrate visual and tactile cues, they have a lot of difficulty standing. Using a head-mounted <a href="/wiki/Accelerometer" title="Accelerometer">accelerometer</a> and a brain-machine interface that employs electrotactile stimulation on the tongue, information about head-body orientation was relayed to the patient so that a new source of data is available to orient themselves and maintain good posture.<sup id="cite_ref-vest_41-1" class="reference"><a href="#cite_note-vest-41">&#91;41&#93;</a></sup>
</p>
<h4><span id="Tactile.E2.80.93tactile_to_restore_peripheral_sensation"></span><span class="mw-headline" id="Tactile–tactile_to_restore_peripheral_sensation">Tactile–tactile to restore peripheral sensation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit&amp;section=11" title="Edit section: Tactile–tactile to restore peripheral sensation">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Touch to touch sensory substitution is where information from touch receptors of one region can be used to perceive touch in another. For example, in one experiment by Bach-y-Rita, the touch perception was restored in a patient who lost peripheral sensation from leprosy.<sup id="cite_ref-six_9-3" class="reference"><a href="#cite_note-six-9">&#91;9&#93;</a></sup> For example, this leprosy patient was equipped with a glove containing artificial contact sensors coupled to skin sensory receptors on the forehead (which was stimulated). After training and acclimation, the patient was able to experience data from the glove as if it was originating in the fingertips while ignoring the sensations in the forehead.<sup id="cite_ref-six_9-4" class="reference"><a href="#cite_note-six-9">&#91;9&#93;</a></sup> After two days of training one of the leprosy subjects reported "the wonderful sensation of touching his wife, which he had been unable to experience for 20 years."<sup id="cite_ref-s_42-0" class="reference"><a href="#cite_note-s-42">&#91;42&#93;</a></sup>
</p>
<h4><span class="mw-headline" id="Tactile_feedback_system_for_prosthetic_limbs">Tactile feedback system for prosthetic limbs</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit&amp;section=12" title="Edit section: Tactile feedback system for prosthetic limbs">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>The development of new technologies has now made it plausible to provide patients with prosthetic arms with tactile and kinesthetic sensibilities.<sup id="cite_ref-pros_43-0" class="reference"><a href="#cite_note-pros-43">&#91;43&#93;</a></sup> While this is not purely a sensory substitution system, it uses the same principles to restore perception of senses. Some tactile feedback methods of restoring a perception of touch to amputees would be direct or micro stimulation of the tactile nerve afferents.<sup id="cite_ref-pros_43-1" class="reference"><a href="#cite_note-pros-43">&#91;43&#93;</a></sup>
</p><p>Other applications of sensory substitution systems can be seen in function robotic prostheses for patients with high level quadriplegia. These robotic arms have several mechanisms of slip detection, vibration and texture detection that they relay to the patient through feedback.<sup id="cite_ref-s_42-1" class="reference"><a href="#cite_note-s-42">&#91;42&#93;</a></sup>  After more research and development, the information from these arms can be used by patients to perceive that they are holding and manipulating objects while their robotic arm actually accomplishes the task.
</p>
<h3><span class="mw-headline" id="Auditory_systems">Auditory systems</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit&amp;section=13" title="Edit section: Auditory systems">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Auditory sensory substitution systems like the tactile sensory substitution systems aim to use one sensory modality to compensate for the lack of another in order to gain a perception of one that is lacking. With auditory sensory substitution, visual or tactile sensors detect and store information about the external environment. This information is then transformed by brain-machine interfaces into auditory signals that are relayed via the auditory receptors to the brain.
</p>
<h4><span class="mw-headline" id="Auditory_vision_substitution">Auditory vision substitution</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit&amp;section=14" title="Edit section: Auditory vision substitution">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Auditory vision substitution aims to use the sense of hearing to convey visual information to the blind.
</p>
<h5><span class="mw-headline" id="The_vOICe_Auditory_Display">The vOICe Auditory Display</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit&amp;section=15" title="Edit section: The vOICe Auditory Display">edit</a><span class="mw-editsection-bracket">]</span></span></h5>
<div role="note" class="hatnote navigation-not-searchable">"vOICe" redirects here. For VOICE, see <a href="/wiki/VOICE_(disambiguation)" class="mw-redirect mw-disambig" title="VOICE (disambiguation)">VOICE (disambiguation)</a>. For voice, see <a href="/wiki/Voice_(disambiguation)" class="mw-disambig" title="Voice (disambiguation)">voice (disambiguation)</a>.</div>
<div role="note" class="hatnote navigation-not-searchable">"The vOICe" redirects here. For THE VOICE, see <a href="/wiki/THE_VOICE_(disambiguation)" class="mw-redirect mw-disambig" title="THE VOICE (disambiguation)">THE VOICE (disambiguation)</a>.</div>
<table class="box-Advert plainlinks metadata ambox ambox-content ambox-Advert" role="presentation"><tbody><tr><td class="mbox-image"><div style="width:52px"><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/b/b4/Ambox_important.svg/40px-Ambox_important.svg.png" decoding="async" width="40" height="40" srcset="//upload.wikimedia.org/wikipedia/en/thumb/b/b4/Ambox_important.svg/60px-Ambox_important.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/b/b4/Ambox_important.svg/80px-Ambox_important.svg.png 2x" data-file-width="40" data-file-height="40" /></div></td><td class="mbox-text"><div class="mbox-text-span">This section <b>contains content that is written like <a href="/wiki/Wikipedia:What_Wikipedia_is_not#Wikipedia_is_not_a_soapbox_or_means_of_promotion" title="Wikipedia:What Wikipedia is not">an advertisement</a></b>.<span class="hide-when-compact"> Please help <a class="external text" href="//en.wikipedia.org/w/index.php?title=Sensory_substitution&amp;action=edit">improve it</a> by removing <a href="/wiki/Wikipedia:Spam" title="Wikipedia:Spam">promotional content</a> and inappropriate <a href="/wiki/Wikipedia:External_links" title="Wikipedia:External links">external links</a>, and by adding encyclopedic content written from a <a href="/wiki/Wikipedia:Neutral_point_of_view" title="Wikipedia:Neutral point of view">neutral point of view</a>.</span>  <small class="date-container"><i>(<span class="date">January 2018</span>)</i></small><small class="hide-when-compact"><i> (<a href="/wiki/Help:Maintenance_template_removal" title="Help:Maintenance template removal">Learn how and when to remove this template message</a>)</i></small></div></td></tr></tbody></table>
<p>The <a rel="nofollow" class="external text" href="http://www.seeingwithsound.com">vOICe Auditory Display</a> Technology is one of several approaches towards sensory substitution (vision substitution) for the blind that aims to provide synthetic <a href="/wiki/Visual_perception" title="Visual perception">vision</a> analogous to seeing their surroundings using sound waves by means of a non-invasive <a href="/wiki/Visual_prosthetic" class="mw-redirect" title="Visual prosthetic">visual prosthesis</a>.
</p><p>The vOICe converts live camera views from a video camera into soundscapes, patterns of scores of different tones at different volumes and pitches emitted simultaneously.<sup id="cite_ref-voice_44-0" class="reference"><a href="#cite_note-voice-44">&#91;44&#93;</a></sup> This system uses general video to audio mapping by associating height to pitch and brightness with loudness in a left-to-right scan of any video frame.<sup id="cite_ref-bach_6-3" class="reference"><a href="#cite_note-bach-6">&#91;6&#93;</a></sup>  Views are typically refreshed about once per second with a typical image resolution of up to 60 x 60 pixels as can be proven by spectrographic analysis.<sup id="cite_ref-voice_44-1" class="reference"><a href="#cite_note-voice-44">&#91;44&#93;</a></sup>
</p><p>Neuroscience and psychology research<sup id="cite_ref-45" class="reference"><a href="#cite_note-45">&#91;45&#93;</a></sup><sup id="cite_ref-46" class="reference"><a href="#cite_note-46">&#91;46&#93;</a></sup><sup id="cite_ref-nature.com_47-0" class="reference"><a href="#cite_note-nature.com-47">&#91;47&#93;</a></sup><sup id="cite_ref-Science_&#123;&#123;!&#125;&#125;_AAAS_48-0" class="reference"><a href="#cite_note-Science_{{!}}_AAAS-48">&#91;48&#93;</a></sup><sup id="cite_ref-Popular_Science_49-0" class="reference"><a href="#cite_note-Popular_Science-49">&#91;49&#93;</a></sup> indicate recruitment of relevant brain areas in seeing with sound, as well as functional improvement through training.<sup id="cite_ref-voicenn2007_50-0" class="reference"><a href="#cite_note-voicenn2007-50">&#91;50&#93;</a></sup><sup id="cite_ref-voicepc2007_51-0" class="reference"><a href="#cite_note-voicepc2007-51">&#91;51&#93;</a></sup><sup id="cite_ref-voiceplos2008_52-0" class="reference"><a href="#cite_note-voiceplos2008-52">&#91;52&#93;</a></sup>
</p><p>The ultimate goal is to provide synthetic vision with truly visual sensations by exploiting the <a href="/wiki/Neural_plasticity" class="mw-redirect" title="Neural plasticity">neural plasticity</a> of the human brain. The system does not require any surgery. Its interface to the mind is simply headphones.<sup id="cite_ref-nature.com_47-1" class="reference"><a href="#cite_note-nature.com-47">&#91;47&#93;</a></sup><sup id="cite_ref-Science_&#123;&#123;!&#125;&#125;_AAAS_48-1" class="reference"><a href="#cite_note-Science_{{!}}_AAAS-48">&#91;48&#93;</a></sup> Over time with practice, the processing within the brain is gradually sent down to the <a href="/wiki/Subconscious" title="Subconscious">subconscious</a> levels and becomes automatic.<sup id="cite_ref-Popular_Science_49-1" class="reference"><a href="#cite_note-Popular_Science-49">&#91;49&#93;</a></sup>
</p><p><a href="/wiki/Neuroscience" title="Neuroscience">Neuroscience</a> research<sup id="cite_ref-53" class="reference"><a href="#cite_note-53">&#91;53&#93;</a></sup><sup id="cite_ref-54" class="reference"><a href="#cite_note-54">&#91;54&#93;</a></sup><sup id="cite_ref-55" class="reference"><a href="#cite_note-55">&#91;55&#93;</a></sup><sup id="cite_ref-56" class="reference"><a href="#cite_note-56">&#91;56&#93;</a></sup> has shown that the <a href="/wiki/Visual_cortex" title="Visual cortex">visual cortex</a> of even adult blind people can become responsive to sound, and "seeing with sound" might reinforce this in a visual sense with live video from a head-mounted camera encoded in sound. The extent to which <a href="/wiki/Cortical_plasticity" class="mw-redirect" title="Cortical plasticity">cortical plasticity</a> indeed allows for functionally relevant rewiring or remapping of the human brain is still largely unknown and is being investigated in an open collaboration with research partners around the world.
</p><p>One suggestion for increasing the relative efficiency of the resulting visual stimuli is to adjust the visual field by using an accelerometer to provide a steady image even if the head is moved, which is implemented in its Android edition.<sup id="cite_ref-57" class="reference"><a href="#cite_note-57">&#91;57&#93;</a></sup> Connecting an infrared sensor to adjust the camera position to match eye movements is an option for the Windows edition (though affordable mobile eye-trackers are not yet on the market).
</p><p>The technology of the vOICe was invented in the 1990s by <a href="/w/index.php?title=Peter_Bartus_Leonard_Meijer&amp;action=edit&amp;redlink=1" class="new" title="Peter Bartus Leonard Meijer (page does not exist)">Peter Meijer</a>.<sup id="cite_ref-voice_44-2" class="reference"><a href="#cite_note-voice-44">&#91;44&#93;</a></sup> It has been positively and widely reviewed in media<sup id="cite_ref-58" class="reference"><a href="#cite_note-58">&#91;58&#93;</a></sup><sup id="cite_ref-59" class="reference"><a href="#cite_note-59">&#91;59&#93;</a></sup><sup id="cite_ref-60" class="reference"><a href="#cite_note-60">&#91;60&#93;</a></sup><sup id="cite_ref-61" class="reference"><a href="#cite_note-61">&#91;61&#93;</a></sup><sup id="cite_ref-62" class="reference"><a href="#cite_note-62">&#91;62&#93;</a></sup><sup id="cite_ref-63" class="reference"><a href="#cite_note-63">&#91;63&#93;</a></sup><sup id="cite_ref-64" class="reference"><a href="#cite_note-64">&#91;64&#93;</a></sup><sup id="cite_ref-65" class="reference"><a href="#cite_note-65">&#91;65&#93;</a></sup><sup id="cite_ref-66" class="reference"><a href="#cite_note-66">&#91;66&#93;</a></sup> and also in various high-quality <a href="/wiki/Peer-reviewed_scientific_journals" class="mw-redirect" title="Peer-reviewed scientific journals">peer-reviewed scientific journals</a>, <a rel="nofollow" class="external text" href="http://www.seeingwithsound.com/literature.htm">far extensive than those partially referenced in the current section, and maintained by the inventor's website itself</a>.
</p>
<h5><span class="mw-headline" id="Project_Bat-eye">Project Bat-eye</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit&amp;section=16" title="Edit section: Project Bat-eye">edit</a><span class="mw-editsection-bracket">]</span></span></h5>
<p>Unlike other models and prototypes aiming to do this particular task, this project is developed within a budget of 4 US dollars, and is aimed for the vast majority of partially or fully blind people living in developing and underdeveloped countries, who can't access equipment worth hundreds of dollars.<sup id="cite_ref-67" class="reference"><a href="#cite_note-67">&#91;67&#93;</a></sup>
</p><p>Project BATEYE uses an ultrasonic sensor mounted on to a wearable pair of glasses that measures the distance to the nearest object and relays it to an Arduino board. The Arduino board then processes the measurements and then plays a tone (150–15000&#160;Hz) for the respective distance(2&#160;cm to 4m) till the data from the second ultrasonic pulse (distance) comes in, and then the same process gets repeated. This cycle is repeated almost every 5 milliseconds. The person hears sound that changes according to the distance to the nearest object. The head provides a 195-degree swivel angle and the ultrasonic sensor detects anything within a 15-degree angle. Using systematic, cognitive and computational approach of neuroscience, with the hypothesis that the usage of the occipital lobe of blind people goes into processing other sensory feedback, and using the brain as a computational unit, the machine relies on the brain processing the tone produced every 14 mS to its corresponding distance and producing a soundscape corresponding to the tones and the body navigating using the same.During experimentation, the test subject could detect obstacles as far away as 2 – 3m, with horizontal or vertical movements of the head the blindfolded test subject could understand the basic shape of objects without touching them, and the basic nature of the obstacles.<sup id="cite_ref-68" class="reference"><a href="#cite_note-68">&#91;68&#93;</a></sup>
</p><p>Primarily, this research is open sourced and published (see: [Debargha Ganguly. (2016); DEVELOPING AN ECONOMIC SYSTEM THAT CAN GIVE A BLIND PERSON BASIC SPATIAL AWARENESS AND OBJECT IDENTIFICATION. Int. J. of Adv. Res. 4 (11). 2003–2008] (ISSN 2320-5407).
</p><p>This is a part of the currently undertaken research by Debargha Ganguly, known as Project Basics – an effort to improve the basic standard of living of people of developing and underdeveloped countries, which has also included Project Awaaz<sup id="cite_ref-69" class="reference"><a href="#cite_note-69">&#91;69&#93;</a></sup> – developing a low cost, non-movement restrictive plugin for hand movement to speech conversion.<sup id="cite_ref-70" class="reference"><a href="#cite_note-70">&#91;70&#93;</a></sup>
</p>
<h5><span class="mw-headline" id="EyeMusic">EyeMusic</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit&amp;section=17" title="Edit section: EyeMusic">edit</a><span class="mw-editsection-bracket">]</span></span></h5>
<p>The EyeMusic, represents high locations on the image as high-pitched musical notes on a pentatonic scale, and low vertical locations as low-pitched musical notes on a pentatonic scale. The user wears a miniature camera connected to a small computer (or smartphone) and stereo headphones. The images are converted into "soundscapes", using a predictable algorithm, allowing the user to listen to and then interpret the visual information coming from the camera.
<br />
The EyeMusic conveys color information by using different musical instruments for each of the following five colors: white, blue, red, green, yellow; Black is represented by silence. The EyeMusic currently employs an intermediate resolution of 30×50 pixels. 
An auditory cue is sounded at the beginning of each left-to-right scan of the image. (1) the higher musical notes represent pixels that are located higher on the y-axis of an image, (2) the timing of the sound after the cue indicates the x-axis location of the pixel (that is, an object located on the left of the image will be "sounded" earlier on than an object located further on the right), and (3) different colors are represented by different musical instruments.<sup id="cite_ref-71" class="reference"><a href="#cite_note-71">&#91;71&#93;</a></sup><sup id="cite_ref-72" class="reference"><a href="#cite_note-72">&#91;72&#93;</a></sup> 
In addition, subjects who used the EyeMusic dynamically were able to accurately reach for an object perceived via the SSD.<sup id="cite_ref-73" class="reference"><a href="#cite_note-73">&#91;73&#93;</a></sup>
</p>
<h5><span class="mw-headline" id="LibreAudioView">LibreAudioView</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit&amp;section=18" title="Edit section: LibreAudioView">edit</a><span class="mw-editsection-bracket">]</span></span></h5>
<p>This project, presented in 2015,<sup id="cite_ref-74" class="reference"><a href="#cite_note-74">&#91;74&#93;</a></sup> proposes a new versatile mobile device and a sonification method specifically designed to the pedestrian locomotion of the visually impaired. It sonifies in real-time spatial information from a video stream acquired at a standard frame rate. The device is composed of a miniature camera integrated into a glasses frame which is connected to a battery-powered minicomputer worn around the neck with a strap. The audio signal is transmitted to the user via running headphones. This system has two operating modes. With the first mode, when the user is static, only the edges of the moving objects are sonified. With the second mode, when the user is moving, the edges of both static and moving objects are sonified. Thus, the video stream is simplified by extracting only the edges of objects that can become dangerous obstacles. The system enables the localization of moving objects, the estimation of trajectories, and the detection of approaching objects.
</p>
<h5><span class="mw-headline" id="PSVA">PSVA</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit&amp;section=19" title="Edit section: PSVA">edit</a><span class="mw-editsection-bracket">]</span></span></h5>
<p>Another successful visual-to-auditory sensory substitution device is the Prosthesis Substituting Vision for Audition (PSVA).<sup id="cite_ref-pvsa_75-0" class="reference"><a href="#cite_note-pvsa-75">&#91;75&#93;</a></sup> This system utilizes a head-mounted TV camera that allows real-time, online translation of visual patterns into sound. While the patient moves around, the device captures visual frames at a high frequency and generates the corresponding complex sounds that allow recognition.<sup id="cite_ref-bach_6-4" class="reference"><a href="#cite_note-bach-6">&#91;6&#93;</a></sup> Visual stimuli are transduced into auditory stimuli with the use of a system that uses pixel to frequency relationship and couples a rough model of the human retina with an inverse model of the cochlea.<sup id="cite_ref-pvsa_75-1" class="reference"><a href="#cite_note-pvsa-75">&#91;75&#93;</a></sup>
</p>
<h5><span class="mw-headline" id="The_Vibe">The Vibe</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit&amp;section=20" title="Edit section: The Vibe">edit</a><span class="mw-editsection-bracket">]</span></span></h5>
<p>The sound produced by this software is a mixture of sinusoidal sounds produced by virtual "sources", corresponding each to a "receptive field" in the image. Each receptive field is a set of localized pixels. The sound's amplitude is determined by the mean luminosity of the pixels of the corresponding receptive field. The frequency and the inter-aural disparity are determined by the center of gravity of the co-ordinates of the receptive field's pixels in the image (see "There is something out there: distal attribution in sensory substitution, twenty years later"; Auvray M., Hanneton S., Lenay C., O'Regan K. <a href="/wiki/Journal_of_Integrative_Neuroscience" title="Journal of Integrative Neuroscience">Journal of Integrative Neuroscience</a> 4 (2005) 505-21). The Vibe is an Open Source project hosted by Sourceforge.
</p>
<h5><span class="mw-headline" id="Other_systems">Other systems</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit&amp;section=21" title="Edit section: Other systems">edit</a><span class="mw-editsection-bracket">]</span></span></h5>
<p>Other approaches to the substitution of hearing for vision use binaural directional cues, much as natural <a href="/wiki/Human_echolocation" title="Human echolocation">human echolocation</a> does.  An example of the latter approach is the "SeeHear" chip from Caltech.<sup id="cite_ref-76" class="reference"><a href="#cite_note-76">&#91;76&#93;</a></sup>
</p><p>Other visual-auditory substitution devices deviate from the vOICe's greyscale mapping of images. Zach Capalbo's Kromophone uses a basic color spectrum correlating to different sounds and timbres to give users perceptual information beyond the vOICe's capabilities.<sup id="cite_ref-77" class="reference"><a href="#cite_note-77">&#91;77&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Nervous_system_implants">Nervous system implants</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit&amp;section=22" title="Edit section: Nervous system implants">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>By means of stimulating electrodes implanted into the human nervous system, it is possible to apply current pulses to be learned and reliably recognized by the recipient. It has been shown successfully in experimentation, by <a href="/wiki/Kevin_Warwick" title="Kevin Warwick">Kevin Warwick</a>, that signals can be employed from force/touch indicators on a robot hand as a means of communication.<sup id="cite_ref-78" class="reference"><a href="#cite_note-78">&#91;78&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Criticism">Criticism</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit&amp;section=23" title="Edit section: Criticism">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>It has been argued that the term "substitution" is misleading, as it is merely an "addition" or "supplementation" not a substitution of a sensory modality.<sup id="cite_ref-79" class="reference"><a href="#cite_note-79">&#91;79&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Sensory_augmentation">Sensory augmentation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit&amp;section=24" title="Edit section: Sensory augmentation">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div role="note" class="hatnote navigation-not-searchable">See also: <a href="/wiki/Human_enhancement" title="Human enhancement">Human enhancement</a> and <a href="/wiki/Remote_sensing" title="Remote sensing">Remote sensing</a></div>
<p>Building upon the research conducted on sensory substitution, investigations into the possibility of <i>augmenting</i> the body's sensory apparatus are now beginning. The intention is to extend the body's ability to sense aspects of the environment that are not normally perceivable by the body in its natural state.
</p><p>Active work in this direction is being conducted by, among others, the e-sense project<sup id="cite_ref-80" class="reference"><a href="#cite_note-80">&#91;80&#93;</a></sup> of the <a href="/wiki/Open_University" title="Open University">Open University</a> and <a href="/wiki/Edinburgh_University" class="mw-redirect" title="Edinburgh University">Edinburgh University</a>, and the feelSpace project of the <a href="/wiki/University_of_Osnabr%C3%BCck" class="mw-redirect" title="University of Osnabrück">University of Osnabrück</a>.
</p><p>The findings of research into sensory augmentation (as well as sensory substitution in general) that investigate the emergence of perceptual experience (qualia) from the activity of neurons have implications for the understanding of consciousness.<sup id="cite_ref-Regan,_JK_2001_7-1" class="reference"><a href="#cite_note-Regan,_JK_2001-7">&#91;7&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Magnetic_perception">Magnetic perception</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit&amp;section=25" title="Edit section: Magnetic perception">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>In 2005, the feelSpace group conducted a study<sup id="cite_ref-81" class="reference"><a href="#cite_note-81">&#91;81&#93;</a></sup> of sensory augmentation with a vibrotactile magnetic compass belt worn around the waist. In this study, the participants were provided with the direction of magnetic north as a vibration on their waist.
</p><p>Significant performance improvements in navigational tests were observed (over and above those experienced by control subjects during the same period with the same training) and, for half of the participants, the perception of the belt's vibration underwent a profound change from simple tactile innervation to approach a genuine and direct sense of allocentric orientation: in other words, could <i>perceive</i> north as an entity distinct from the vibrating transducer on the waist, like one perceives a glass on a table as an entity distinct from the impact of reflected photons on the retina. Further, tests of the influence of the belt information on the rotational nystagmus effect suggested that, after training, the processing of the belt information became subcognitive.
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit&amp;section=26" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a href="/wiki/Biological_neural_network" class="mw-redirect" title="Biological neural network">Biological neural network</a></li>
<li><a href="/wiki/Brain_implant" title="Brain implant">Brain implant</a></li>
<li><a href="/wiki/Human_echolocation" title="Human echolocation">Human echolocation</a>, blind people navigating by listening to the echo of sounds</li></ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit&amp;section=27" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist" style="list-style-type: decimal;">
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite_note-TVSS-1"><span class="mw-cite-backlink">^ <a href="#cite_ref-TVSS_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-TVSS_1-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Bach-y-Rita P, Collins CC, Saunders F, White B, Scadden L (1969). "Vision substitution by tactile the image projection". <i>Nature</i>. <b>221</b> (5184): 963–964. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="http://adsabs.harvard.edu/abs/1969Natur.221..963B">1969Natur.221..963B</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1038%2F221963a0">10.1038/221963a0</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/5818337">5818337</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature&amp;rft.atitle=Vision+substitution+by+tactile+the+image+projection&amp;rft.volume=221&amp;rft.issue=5184&amp;rft.pages=963-964&amp;rft.date=1969&amp;rft_id=info%3Apmid%2F5818337&amp;rft_id=info%3Adoi%2F10.1038%2F221963a0&amp;rft_id=info%3Abibcode%2F1969Natur.221..963B&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><span class="cs1-maint citation-comment">CS1 maint: Uses authors parameter (<a href="/wiki/Category:CS1_maint:_Uses_authors_parameter" title="Category:CS1 maint: Uses authors parameter">link</a>)</span><style data-mw-deduplicate="TemplateStyles:r886058088">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}</style></span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><cite class="citation book"><a href="/wiki/Nicholas_Humphrey" title="Nicholas Humphrey">Nicholas Humphrey</a> (1999). <a rel="nofollow" class="external text" href="https://books.google.com/?id=W8G8Oji53XsC&amp;pg=PA79&amp;dq=%22Paul+Bach-y-Rita%22"><i>A History of the Mind: Evolution and the Birth of Consciousness</i></a>. Springer. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-387-98719-4" title="Special:BookSources/978-0-387-98719-4">978-0-387-98719-4</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=A+History+of+the+Mind%3A+Evolution+and+the+Birth+of+Consciousness&amp;rft.pub=Springer&amp;rft.date=1999&amp;rft.isbn=978-0-387-98719-4&amp;rft.au=Nicholas+Humphrey&amp;rft_id=https%3A%2F%2Fbooks.google.com%2F%3Fid%3DW8G8Oji53XsC%26pg%3DPA79%26dq%3D%2522Paul%2BBach-y-Rita%2522&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text"><cite class="citation journal">Scheff, Chaim-Meyer (1 January 1986). "Experimental Model for the Study of Changes in the Organization of Human Sensory Information Processing Through the Design and Testing of Non-invasive Prosthetic Devices for Sensory Impaired People". <i>SIGCAPH Comput. Phys. Handicap.</i> (36): 3–10. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1145%2F15711.15713">10.1145/15711.15713</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=SIGCAPH+Comput.+Phys.+Handicap.&amp;rft.atitle=Experimental+Model+for+the+Study+of+Changes+in+the+Organization+of+Human+Sensory+Information+Processing+Through+the+Design+and+Testing+of+Non-invasive+Prosthetic+Devices+for+Sensory+Impaired+People&amp;rft.issue=36&amp;rft.pages=3-10&amp;rft.date=1986-01-01&amp;rft_id=info%3Adoi%2F10.1145%2F15711.15713&amp;rft.aulast=Scheff&amp;rft.aufirst=Chaim-Meyer&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text"><cite class="citation journal">Bach-y-Rita P (2004). "Tactile sensory substitution studies". <i>Annals of the New York Academy of Sciences</i>. <b>1013</b> (1): 83–91. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="http://adsabs.harvard.edu/abs/2004NYASA1013...83B">2004NYASA1013...83B</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1196%2Fannals.1305.006">10.1196/annals.1305.006</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/15194608">15194608</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Annals+of+the+New+York+Academy+of+Sciences&amp;rft.atitle=Tactile+sensory+substitution+studies&amp;rft.volume=1013&amp;rft.issue=1&amp;rft.pages=83-91&amp;rft.date=2004&amp;rft_id=info%3Apmid%2F15194608&amp;rft_id=info%3Adoi%2F10.1196%2Fannals.1305.006&amp;rft_id=info%3Abibcode%2F2004NYASA1013...83B&amp;rft.au=Bach-y-Rita+P&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-Three-5"><span class="mw-cite-backlink">^ <a href="#cite_ref-Three_5-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Three_5-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Renier L, De Volder AG (2005). "Cognitive and brain mechanisms in sensory substitution of vision: a contribution to the study of human perception". <i>Journal of Integrative Neuroscience</i>. <b>4</b> (4): 489–503. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1142%2FS0219635205000999">10.1142/S0219635205000999</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/16385643">16385643</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Integrative+Neuroscience&amp;rft.atitle=Cognitive+and+brain+mechanisms+in+sensory+substitution+of+vision%3A+a+contribution+to+the+study+of+human+perception&amp;rft.volume=4&amp;rft.issue=4&amp;rft.pages=489-503&amp;rft.date=2005&amp;rft_id=info%3Adoi%2F10.1142%2FS0219635205000999&amp;rft_id=info%3Apmid%2F16385643&amp;rft.aulast=Renier&amp;rft.aufirst=L&amp;rft.au=De+Volder%2C+AG&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-bach-6"><span class="mw-cite-backlink">^ <a href="#cite_ref-bach_6-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-bach_6-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-bach_6-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-bach_6-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-bach_6-4"><sup><i><b>e</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Bach-y-Rita P, Kercel SW (2003). <a rel="nofollow" class="external text" href="http://hci.ucsd.edu/102a/readings/SensorySubstitution.pdf">"Sensory substitution and the human-machine interface"</a> <span class="cs1-format">(PDF)</span>. <i>Trends in Cognitive Sciences</i>. <b>7</b> (12): 541–546. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.159.9777">10.1.1.159.9777</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1016%2Fj.tics.2003.10.013">10.1016/j.tics.2003.10.013</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/14643370">14643370</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Trends+in+Cognitive+Sciences&amp;rft.atitle=Sensory+substitution+and+the+human-machine+interface&amp;rft.volume=7&amp;rft.issue=12&amp;rft.pages=541-546&amp;rft.date=2003&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.159.9777&amp;rft_id=info%3Apmid%2F14643370&amp;rft_id=info%3Adoi%2F10.1016%2Fj.tics.2003.10.013&amp;rft.aulast=Bach-y-Rita&amp;rft.aufirst=P&amp;rft.au=Kercel%2C+SW&amp;rft_id=http%3A%2F%2Fhci.ucsd.edu%2F102a%2Freadings%2FSensorySubstitution.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-Regan,_JK_2001-7"><span class="mw-cite-backlink">^ <a href="#cite_ref-Regan,_JK_2001_7-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Regan,_JK_2001_7-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">O'Regan, JK; Noe, A. (2001). "A sensorimotor account of vision and visual consciousness". <i>Behavioral and Brain Sciences</i>. <b>24</b> (5): 939–973. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1017%2Fs0140525x01000115">10.1017/s0140525x01000115</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/12239892">12239892</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Behavioral+and+Brain+Sciences&amp;rft.atitle=A+sensorimotor+account+of+vision+and+visual+consciousness&amp;rft.volume=24&amp;rft.issue=5&amp;rft.pages=939-973&amp;rft.date=2001&amp;rft_id=info%3Adoi%2F10.1017%2Fs0140525x01000115&amp;rft_id=info%3Apmid%2F12239892&amp;rft.au=O%27Regan%2C+JK&amp;rft.au=Noe%2C+A.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text">Bach-y-Rita P. <i>Brain Mechanisms in Sensory Substitution</i>, Academic Press New York:1972.</span>
</li>
<li id="cite_note-six-9"><span class="mw-cite-backlink">^ <a href="#cite_ref-six_9-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-six_9-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-six_9-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-six_9-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-six_9-4"><sup><i><b>e</b></i></sup></a></span> <span class="reference-text">Bach-y-Rita P. <i>Nonsynaptic Diffusion Neurotransmission and Late Brain Reorganization</i>, Demos-Vermande, New York :1995.</span>
</li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text"><cite class="citation journal">Van Boven, R. W.; Hamilton, R. H.; Kauffman, T.; Keenan, J. P.; Pascual-Leone, A. (2000-06-27). "Tactile spatial resolution in blind braille readers". <i>Neurology</i>. <b>54</b> (12): 2230–2236. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1212%2Fwnl.54.12.2230">10.1212/wnl.54.12.2230</a>. <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/0028-3878">0028-3878</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/10881245">10881245</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neurology&amp;rft.atitle=Tactile+spatial+resolution+in+blind+braille+readers&amp;rft.volume=54&amp;rft.issue=12&amp;rft.pages=2230-2236&amp;rft.date=2000-06-27&amp;rft.issn=0028-3878&amp;rft_id=info%3Apmid%2F10881245&amp;rft_id=info%3Adoi%2F10.1212%2Fwnl.54.12.2230&amp;rft.aulast=Van+Boven&amp;rft.aufirst=R.+W.&amp;rft.au=Hamilton%2C+R.+H.&amp;rft.au=Kauffman%2C+T.&amp;rft.au=Keenan%2C+J.+P.&amp;rft.au=Pascual-Leone%2C+A.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text"><cite class="citation journal">Goldreich, Daniel; Kanics, Ingrid M. (2003-04-15). "Tactile acuity is enhanced in blindness". <i>The Journal of Neuroscience</i>. <b>23</b> (8): 3439–3445. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1523%2FJNEUROSCI.23-08-03439.2003">10.1523/JNEUROSCI.23-08-03439.2003</a>. <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/1529-2401">1529-2401</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/12716952">12716952</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Journal+of+Neuroscience&amp;rft.atitle=Tactile+acuity+is+enhanced+in+blindness&amp;rft.volume=23&amp;rft.issue=8&amp;rft.pages=3439-3445&amp;rft.date=2003-04-15&amp;rft.issn=1529-2401&amp;rft_id=info%3Apmid%2F12716952&amp;rft_id=info%3Adoi%2F10.1523%2FJNEUROSCI.23-08-03439.2003&amp;rft.aulast=Goldreich&amp;rft.aufirst=Daniel&amp;rft.au=Kanics%2C+Ingrid+M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text"><cite class="citation journal">Goldreich, Daniel; Kanics, Ingrid M. (November 2006). "Performance of blind and sighted humans on a tactile grating detection task". <i>Perception &amp; Psychophysics</i>. <b>68</b> (8): 1363–1371. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.3758%2Fbf03193735">10.3758/bf03193735</a>. <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/0031-5117">0031-5117</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/17378422">17378422</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Perception+%26+Psychophysics&amp;rft.atitle=Performance+of+blind+and+sighted+humans+on+a+tactile+grating+detection+task&amp;rft.volume=68&amp;rft.issue=8&amp;rft.pages=1363-1371&amp;rft.date=2006-11&amp;rft.issn=0031-5117&amp;rft_id=info%3Apmid%2F17378422&amp;rft_id=info%3Adoi%2F10.3758%2Fbf03193735&amp;rft.aulast=Goldreich&amp;rft.aufirst=Daniel&amp;rft.au=Kanics%2C+Ingrid+M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text"><cite class="citation journal">Wong, Michael; Gnanakumaran, Vishi; Goldreich, Daniel (2011-05-11). "Tactile Spatial Acuity Enhancement in Blindness: Evidence for Experience-Dependent Mechanisms". <i>Journal of Neuroscience</i>. <b>31</b> (19): 7028–7037. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1523%2Fjneurosci.6461-10.2011">10.1523/jneurosci.6461-10.2011</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/21562264">21562264</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Neuroscience&amp;rft.atitle=Tactile+Spatial+Acuity+Enhancement+in+Blindness%3A+Evidence+for+Experience-Dependent+Mechanisms&amp;rft.volume=31&amp;rft.issue=19&amp;rft.pages=7028-7037&amp;rft.date=2011-05-11&amp;rft_id=info%3Adoi%2F10.1523%2Fjneurosci.6461-10.2011&amp;rft_id=info%3Apmid%2F21562264&amp;rft.aulast=Wong&amp;rft.aufirst=Michael&amp;rft.au=Gnanakumaran%2C+Vishi&amp;rft.au=Goldreich%2C+Daniel&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text"><cite class="citation journal">Bhattacharjee, Arindam; Ye, Amanda J.; Lisak, Joy A.; Vargas, Maria G.; Goldreich, Daniel (2010-10-27). <a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC3449316">"Vibrotactile Masking Experiments Reveal Accelerated Somatosensory Processing in Congenitally Blind Braille Readers"</a>. <i>Journal of Neuroscience</i>. <b>30</b> (43): 14288–14298. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1523%2Fjneurosci.1447-10.2010">10.1523/jneurosci.1447-10.2010</a>. <a href="/wiki/PubMed_Central" title="PubMed Central">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC3449316">3449316</a></span>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/20980584">20980584</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Neuroscience&amp;rft.atitle=Vibrotactile+Masking+Experiments+Reveal+Accelerated+Somatosensory+Processing+in+Congenitally+Blind+Braille+Readers&amp;rft.volume=30&amp;rft.issue=43&amp;rft.pages=14288-14298&amp;rft.date=2010-10-27&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3449316&amp;rft_id=info%3Apmid%2F20980584&amp;rft_id=info%3Adoi%2F10.1523%2Fjneurosci.1447-10.2010&amp;rft.aulast=Bhattacharjee&amp;rft.aufirst=Arindam&amp;rft.au=Ye%2C+Amanda+J.&amp;rft.au=Lisak%2C+Joy+A.&amp;rft.au=Vargas%2C+Maria+G.&amp;rft.au=Goldreich%2C+Daniel&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3449316&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-eight-15"><span class="mw-cite-backlink">^ <a href="#cite_ref-eight_15-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-eight_15-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Poirier C, De Volder AG, Scheiber C (2007). "What neuroimaging tells us about sensory substitution". <i>Neuroscience &amp; Biobehavioral Reviews</i>. <b>31</b> (7): 1064–1070. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1016%2Fj.neubiorev.2007.05.010">10.1016/j.neubiorev.2007.05.010</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/17688948">17688948</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neuroscience+%26+Biobehavioral+Reviews&amp;rft.atitle=What+neuroimaging+tells+us+about+sensory+substitution&amp;rft.volume=31&amp;rft.issue=7&amp;rft.pages=1064-1070&amp;rft.date=2007&amp;rft_id=info%3Adoi%2F10.1016%2Fj.neubiorev.2007.05.010&amp;rft_id=info%3Apmid%2F17688948&amp;rft.aulast=Poirier&amp;rft.aufirst=C&amp;rft.au=De+Volder%2C+AG&amp;rft.au=Scheiber%2C+C&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text"><cite class="citation journal">Vallbo AB, Johansson RS (1984). "Properties of cutaneous mechanoreceptors in the human hand related to touch sensation". <i>Human Neurobiology</i>. <b>3</b> (1): 3–14. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/6330008">6330008</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Human+Neurobiology&amp;rft.atitle=Properties+of+cutaneous+mechanoreceptors+in+the+human+hand+related+to+touch+sensation&amp;rft.volume=3&amp;rft.issue=1&amp;rft.pages=3-14&amp;rft.date=1984&amp;rft_id=info%3Apmid%2F6330008&amp;rft.aulast=Vallbo&amp;rft.aufirst=AB&amp;rft.au=Johansson%2C+RS&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-EV-17"><span class="mw-cite-backlink">^ <a href="#cite_ref-EV_17-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-EV_17-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-EV_17-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-EV_17-3"><sup><i><b>d</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Kaczmarek KA, Webster JG, Bach-y-Rita P, Tompkins WJ (1991). "Electrotactile and vibrotactile displays for sensory substitution systems". <i>IEEE Transactions on Biomedical Engineering</i>. <b>38</b> (1): 1–16. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1109%2F10.68204">10.1109/10.68204</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/2026426">2026426</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Biomedical+Engineering&amp;rft.atitle=Electrotactile+and+vibrotactile+displays+for+sensory+substitution+systems&amp;rft.volume=38&amp;rft.issue=1&amp;rft.pages=1-16&amp;rft.date=1991&amp;rft_id=info%3Adoi%2F10.1109%2F10.68204&amp;rft_id=info%3Apmid%2F2026426&amp;rft.aulast=Kaczmarek&amp;rft.aufirst=KA&amp;rft.au=Webster%2C+JG&amp;rft.au=Bach-y-Rita%2C+P&amp;rft.au=Tompkins%2C+WJ&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18">^</a></b></span> <span class="reference-text"><cite class="citation journal">Biswas, Abhijit; Manivannan, M.; Srinivasan, Mandyam A. (2015). "Vibrotactile Sensitivity Threshold: Nonlinear Stochastic Mechanotransduction Model of the Pacinian Corpuscle". <i>IEEE Transactions on Haptics</i>. <b>8</b> (1): 102–113. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1109%2FTOH.2014.2369422">10.1109/TOH.2014.2369422</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/25398183">25398183</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Haptics&amp;rft.atitle=Vibrotactile+Sensitivity+Threshold%3A+Nonlinear+Stochastic+Mechanotransduction+Model+of+the+Pacinian+Corpuscle&amp;rft.volume=8&amp;rft.issue=1&amp;rft.pages=102-113&amp;rft.date=2015&amp;rft_id=info%3Adoi%2F10.1109%2FTOH.2014.2369422&amp;rft_id=info%3Apmid%2F25398183&amp;rft.aulast=Biswas&amp;rft.aufirst=Abhijit&amp;rft.au=Manivannan%2C+M.&amp;rft.au=Srinivasan%2C+Mandyam+A.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text"><cite class="citation journal">Biswas, Abhijit; Manivannan, M.; Srinivasan, Mandyam A. (2015). "Multiscale Layered Biomechanical Model of the Pacinian Corpuscle". <i>IEEE Transactions on Haptics</i>. <b>8</b> (1): 31–42. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1109%2FTOH.2014.2369416">10.1109/TOH.2014.2369416</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/25398182">25398182</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Haptics&amp;rft.atitle=Multiscale+Layered+Biomechanical+Model+of+the+Pacinian+Corpuscle&amp;rft.volume=8&amp;rft.issue=1&amp;rft.pages=31-42&amp;rft.date=2015&amp;rft_id=info%3Adoi%2F10.1109%2FTOH.2014.2369416&amp;rft_id=info%3Apmid%2F25398182&amp;rft.aulast=Biswas&amp;rft.aufirst=Abhijit&amp;rft.au=Manivannan%2C+M.&amp;rft.au=Srinivasan%2C+Mandyam+A.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-tong-20"><span class="mw-cite-backlink">^ <a href="#cite_ref-tong_20-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-tong_20-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-tong_20-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Bach-y-Rita P, Kaczmarek KA, Tyler ME, Garcia-Lara J (1998). <a rel="nofollow" class="external text" href="http://web.gc.cuny.edu/cogsci/private/bach-y-rita-tongue.pdf">"Form perception with a 49-point electrotactile stimulus array on the tongue: a technical note"</a> <span class="cs1-format">(PDF)</span>. <i>J Rehabil Res Dev</i>. <b>35</b> (4): 427–30. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/10220221">10220221</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=J+Rehabil+Res+Dev.&amp;rft.atitle=Form+perception+with+a+49-point+electrotactile+stimulus+array+on+the+tongue%3A+a+technical+note&amp;rft.volume=35&amp;rft.issue=4&amp;rft.pages=427-30&amp;rft.date=1998&amp;rft_id=info%3Apmid%2F10220221&amp;rft.aulast=Bach-y-Rita&amp;rft.aufirst=P&amp;rft.au=Kaczmarek%2C+KA&amp;rft.au=Tyler%2C+ME&amp;rft.au=Garcia-Lara%2C+J&amp;rft_id=http%3A%2F%2Fweb.gc.cuny.edu%2Fcogsci%2Fprivate%2Fbach-y-rita-tongue.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/> See also <a href="/wiki/Brainport" title="Brainport">Brainport</a></span>
</li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text"><cite class="citation web">Layton, Julia. <a rel="nofollow" class="external text" href="http://science.howstuffworks.com/brainport2.htm">"How BrainPort Works"</a>. <i>HowStuffWorks</i><span class="reference-accessdate">. Retrieved <span class="nowrap">July 21,</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=HowStuffWorks&amp;rft.atitle=How+BrainPort+Works&amp;rft.au=Layton%2C+Julia&amp;rft_id=http%3A%2F%2Fscience.howstuffworks.com%2Fbrainport2.htm&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://web.archive.org/web/20130507045650/http://www.wicab.com/media/Wicab%20Press%20Release%203-19-2013.pdf">"Wicab Announces European Market Approval for its Non-Invasive Assistive Aid for the Blind"</a> <span class="cs1-format">(PDF)</span> (Press Release). Wicab, Inc. Archived from <a rel="nofollow" class="external text" href="http://www.wicab.com/media/Wicab%20Press%20Release%203-19-2013.pdf">the original</a> <span class="cs1-format">(PDF)</span> on May 7, 2013.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Wicab+Announces+European+Market+Approval+for+its+Non-Invasive+Assistive+Aid+for+the+Blind&amp;rft.pub=Wicab%2C+Inc.&amp;rft_id=http%3A%2F%2Fwww.wicab.com%2Fmedia%2FWicab%2520Press%2520Release%25203-19-2013.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text"><cite class="citation journal">Hui Tang; D. J. Beebe (2003). "Design and microfabrication of a flexible oral electrotactile display". <i>Journal of Microelectromechanical Systems</i>. <b>12</b> (1): 29–36. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1109%2FJMEMS.2002.807478">10.1109/JMEMS.2002.807478</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Microelectromechanical+Systems&amp;rft.atitle=Design+and+microfabrication+of+a+flexible+oral+electrotactile+display&amp;rft.volume=12&amp;rft.issue=1&amp;rft.pages=29-36&amp;rft.date=2003&amp;rft_id=info%3Adoi%2F10.1109%2FJMEMS.2002.807478&amp;rft.au=Hui+Tang&amp;rft.au=D.+J.+Beebe&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-24"><span class="mw-cite-backlink"><b><a href="#cite_ref-24">^</a></b></span> <span class="reference-text"><cite class="citation web">Deyle, Travis (August 11, 2010). <a rel="nofollow" class="external text" href="http://www.hizook.com/blog/2010/08/11/electrotactile-arrays-texture-and-pressure-feedback-during-robotic-teleoperation">"Electrotactile Arrays for Texture and Pressure Feedback During Robotic Teleoperation"</a>. <i>Hizook</i><span class="reference-accessdate">. Retrieved <span class="nowrap">July 21,</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Hizook&amp;rft.atitle=Electrotactile+Arrays+for+Texture+and+Pressure+Feedback+During+Robotic+Teleoperation&amp;rft.date=2010-08-11&amp;rft.au=Deyle%2C+Travis&amp;rft_id=http%3A%2F%2Fwww.hizook.com%2Fblog%2F2010%2F08%2F11%2Felectrotactile-arrays-texture-and-pressure-feedback-during-robotic-teleoperation&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-25">^</a></b></span> <span class="reference-text">S. GRIMNES, <i>Electrovibration, cutaneous sensation of microampere current</i>, <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1111%2Fj.1748-1716.1983.tb07235.x">10.1111/j.1748-1716.1983.tb07235.x</a> <a rel="nofollow" class="external text" href="http://www.fys.uio.no/elg/bioimp/pdf/vibration.pdf">pdf</a></span>
</li>
<li id="cite_note-26"><span class="mw-cite-backlink"><b><a href="#cite_ref-26">^</a></b></span> <span class="reference-text"><cite class="citation journal">Kurt A. Kaczmarek; Krishnakant Nammi; Abhishek K. Agarwal; Mitchell E. Tyler; Steven J. Haase; David J. Beebec (2006). <a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC2582732">"Polarity effect in electrovibration for tactile display"</a>. <i>IEEE Trans Biomed Eng</i>. <b>53</b> (10): 2047–2054. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1109%2FTBME.2006.881804">10.1109/TBME.2006.881804</a>. <a href="/wiki/PubMed_Central" title="PubMed Central">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC2582732">2582732</a></span>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/17019869">17019869</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Trans+Biomed+Eng&amp;rft.atitle=Polarity+effect+in+electrovibration+for+tactile+display&amp;rft.volume=53&amp;rft.issue=10&amp;rft.pages=2047-2054&amp;rft.date=2006&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC2582732&amp;rft_id=info%3Apmid%2F17019869&amp;rft_id=info%3Adoi%2F10.1109%2FTBME.2006.881804&amp;rft.au=Kurt+A.+Kaczmarek&amp;rft.au=Krishnakant+Nammi&amp;rft.au=Abhishek+K.+Agarwal&amp;rft.au=Mitchell+E.+Tyler&amp;rft.au=Steven+J.+Haase&amp;rft.au=David+J.+Beebec&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC2582732&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-Zeng,_2015-27"><span class="mw-cite-backlink"><b><a href="#cite_ref-Zeng,_2015_27-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Zeng;  et al. (2015). "Interactive Audio-haptic Map Explorer on a Tactile Display". <i>Interacting with Computers</i>. <b>27</b> (4): 413–429. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1093%2Fiwc%2Fiwu006">10.1093/iwc/iwu006</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Interacting+with+Computers&amp;rft.atitle=Interactive+Audio-haptic+Map+Explorer+on+a+Tactile+Display&amp;rft.volume=27&amp;rft.issue=4&amp;rft.pages=413-429&amp;rft.date=2015&amp;rft_id=info%3Adoi%2F10.1093%2Fiwc%2Fiwu006&amp;rft.au=Zeng&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-Zeng,_2012-28"><span class="mw-cite-backlink"><b><a href="#cite_ref-Zeng,_2012_28-0">^</a></b></span> <span class="reference-text"><cite class="citation conference">Zeng;  et al. (2012). <i>Exploration and avoidance of surrounding obstacles for the visually impaired</i>. <i>Proc. of ACM ASSETS 2012</i>. pp.&#160;111–118. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1145%2F2384916.2384936">10.1145/2384916.2384936</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=conference&amp;rft.jtitle=Proc.+of+ACM+ASSETS+2012&amp;rft.atitle=Exploration+and+avoidance+of+surrounding+obstacles+for+the+visually+impaired&amp;rft.pages=111-118&amp;rft.date=2012&amp;rft_id=info%3Adoi%2F10.1145%2F2384916.2384936&amp;rft.au=Zeng&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-29"><span class="mw-cite-backlink"><b><a href="#cite_ref-29">^</a></b></span> <span class="reference-text">Bach-y-Rita P, and Kaczmarek KA. (2002). <i>Tongue placed tactile output device</i>. US Patent 6,430,450.</span>
</li>
<li id="cite_note-Haptic_Belt-30"><span class="mw-cite-backlink"><b><a href="#cite_ref-Haptic_Belt_30-0">^</a></b></span> <span class="reference-text"><cite class="citation conference">T. McDaniel; S. Krishna; V. Balasubramanian,; D. Colbry; S. Panchanathan (2008). <i>Using a haptic belt to convey non-verbal communication cues during social interactions to individuals who are blind</i>. IEEE International Workshop on Haptic, Audio and Visual Environments and Games, 2008. HAVE 2008. pp.&#160;13–18. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1109%2FHAVE.2008.4685291">10.1109/HAVE.2008.4685291</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Using+a+haptic+belt+to+convey+non-verbal+communication+cues+during+social+interactions+to+individuals+who+are+blind&amp;rft.pages=13-18&amp;rft.pub=HAVE+2008&amp;rft.date=2008&amp;rft_id=info%3Adoi%2F10.1109%2FHAVE.2008.4685291&amp;rft.au=T.+McDaniel&amp;rft.au=S.+Krishna&amp;rft.au=V.+Balasubramanian%2C&amp;rft.au=D.+Colbry&amp;rft.au=S.+Panchanathan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-31"><span class="mw-cite-backlink"><b><a href="#cite_ref-31">^</a></b></span> <span class="reference-text"><cite class="citation conference">Paul, Sathish Kumar; Rekha, V.; Sivarasu, Sudesh (2012). "Tactile Sensing Fabrics for Detecting Impairments in Leprosy Patients". <i>Appropriate Healthcare Technologies for Developing Countries – AHT2012. The 7th International Conference – World Health and Wellbeing</i>. London, UK: IEEE. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1049%2Fcp.2012.1461">10.1049/cp.2012.1461</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Tactile+Sensing+Fabrics+for+Detecting+Impairments+in+Leprosy+Patients&amp;rft.btitle=Appropriate+Healthcare+Technologies+for+Developing+Countries+%E2%80%93+AHT2012.+The+7th+International+Conference+%E2%80%93+World+Health+and+Wellbeing&amp;rft.place=London%2C+UK&amp;rft.pub=IEEE&amp;rft.date=2012&amp;rft_id=info%3Adoi%2F10.1049%2Fcp.2012.1461&amp;rft.aulast=Paul&amp;rft.aufirst=Sathish+Kumar&amp;rft.au=Rekha%2C+V.&amp;rft.au=Sivarasu%2C+Sudesh&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-Haptic_Glove-32"><span class="mw-cite-backlink"><b><a href="#cite_ref-Haptic_Glove_32-0">^</a></b></span> <span class="reference-text"><cite class="citation conference">S. Krishna; S. Bala; T. McDaniel; S. McGuire; S. Panchanathan (2010). <a rel="nofollow" class="external text" href="http://dmrussell.net/CHI2010/docs/p3637.pdf"><i>VibroGlove: an assistive technology aid for conveying facial expressions</i></a> <span class="cs1-format">(PDF)</span>. Proceedings of the 28th of the international conference extended abstracts on Human factors in computing systems. Atlanta, Georgia, USA: ACM. pp.&#160;3637–3642. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1145%2F1753846.1754031">10.1145/1753846.1754031</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=VibroGlove%3A+an+assistive+technology+aid+for+conveying+facial+expressions&amp;rft.place=Atlanta%2C+Georgia%2C+USA&amp;rft.pages=3637-3642&amp;rft.pub=ACM&amp;rft.date=2010&amp;rft_id=info%3Adoi%2F10.1145%2F1753846.1754031&amp;rft.au=S.+Krishna&amp;rft.au=S.+Bala&amp;rft.au=T.+McDaniel&amp;rft.au=S.+McGuire&amp;rft.au=S.+Panchanathan&amp;rft_id=http%3A%2F%2Fdmrussell.net%2FCHI2010%2Fdocs%2Fp3637.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-33"><span class="mw-cite-backlink"><b><a href="#cite_ref-33">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://www.k2.t.u-tokyo.ac.jp/perception/HapticRadar/index-e.html">"The Haptic Radar / Extended Skin Project"</a>. Ishikawa Watanabe Laboratory.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=The+Haptic+Radar+%2F+Extended+Skin+Project&amp;rft.pub=Ishikawa+Watanabe+Laboratory&amp;rft_id=http%3A%2F%2Fwww.k2.t.u-tokyo.ac.jp%2Fperception%2FHapticRadar%2Findex-e.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-Haptic_Radar-34"><span class="mw-cite-backlink"><b><a href="#cite_ref-Haptic_Radar_34-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">A. Cassinelli; E. Sampaio; S.B. Joffily; H.R.S. Lima; B.P.G.R. Gusmão (2014). <a rel="nofollow" class="external text" href="https://web.archive.org/web/20170314214320/http://www.k2.t.u-tokyo.ac.jp/members/alvaro/Publications/TechnologyDisability_Alvaro.pdf">"Do blind people move more confidently with the Tactile Radar?"</a> <span class="cs1-format">(PDF)</span>. <i>IOS Press, Technology and Disability</i>. <b>26</b> (2–3): 161–170. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.3233%2FTAD-140414">10.3233/TAD-140414</a>. Archived from <a rel="nofollow" class="external text" href="http://www.k2.t.u-tokyo.ac.jp/members/alvaro/Publications/TechnologyDisability_Alvaro.pdf">the original</a> <span class="cs1-format">(PDF)</span> on 2017-03-14<span class="reference-accessdate">. Retrieved <span class="nowrap">2016-07-21</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IOS+Press%2C+Technology+and+Disability&amp;rft.atitle=Do+blind+people+move+more+confidently+with+the+Tactile+Radar%3F&amp;rft.volume=26&amp;rft.issue=2%E2%80%933&amp;rft.pages=161-170&amp;rft.date=2014&amp;rft_id=info%3Adoi%2F10.3233%2FTAD-140414&amp;rft.au=A.+Cassinelli&amp;rft.au=E.+Sampaio&amp;rft.au=S.B.+Joffily&amp;rft.au=H.R.S.+Lima&amp;rft.au=B.P.G.R.+Gusm%C3%A3o&amp;rft_id=http%3A%2F%2Fwww.k2.t.u-tokyo.ac.jp%2Fmembers%2Falvaro%2FPublications%2FTechnologyDisability_Alvaro.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-35"><span class="mw-cite-backlink"><b><a href="#cite_ref-35">^</a></b></span> <span class="reference-text">Eagleman, David (2015). <a rel="nofollow" class="external text" href="http://www.ted.com/talks/david_eagleman_can_we_create_new_senses_for_humans?language=en">Can we create new senses for humans?</a> TED talks.</span>
</li>
<li id="cite_note-36"><span class="mw-cite-backlink"><b><a href="#cite_ref-36">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://NeoSensory.com">NeoSensory, Inc</a></span>
</li>
<li id="cite_note-37"><span class="mw-cite-backlink"><b><a href="#cite_ref-37">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://www.smithsonianmag.com/innovation/could-this-futuristic-vest-give-us-sixth-sense-180968852/">Could This Futuristic Vest Give Us a Sixth Sense?</a>, Smithsonian Magazine, Apr 2018.</span>
</li>
<li id="cite_note-38"><span class="mw-cite-backlink"><b><a href="#cite_ref-38">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://www.telegraph.co.uk/technology/2019/01/16/meet-man-trying-give-humans-sixth-senseneuroscientist-dr-david/">Meet the man who wants to give humans a sixth sense</a>, The Telegraph, Jan 2019.</span>
</li>
<li id="cite_note-TASS-39"><span class="mw-cite-backlink">^ <a href="#cite_ref-TASS_39-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-TASS_39-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Schurmann M, Caetano G, Hlushchuk Y, Jousmaki V, Hari R (2006). "Touch activates human auditory cortex". <i>NeuroImage</i>. <b>30</b> (4): 1325–1331. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1016%2Fj.neuroimage.2005.11.020">10.1016/j.neuroimage.2005.11.020</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/16488157">16488157</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=NeuroImage&amp;rft.atitle=Touch+activates+human+auditory+cortex&amp;rft.volume=30&amp;rft.issue=4&amp;rft.pages=1325-1331&amp;rft.date=2006&amp;rft_id=info%3Adoi%2F10.1016%2Fj.neuroimage.2005.11.020&amp;rft_id=info%3Apmid%2F16488157&amp;rft.aulast=Schurmann&amp;rft.aufirst=M&amp;rft.au=Caetano%2C+G&amp;rft.au=Hlushchuk%2C+Y&amp;rft.au=Jousmaki%2C+V&amp;rft.au=Hari%2C+R&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-40"><span class="mw-cite-backlink"><b><a href="#cite_ref-40">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://www.freepatentsonline.com/20020173823.pdf">"Sense organs synthesizer United States Patent Application 20020173823"</a> <span class="cs1-format">(PDF)</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Sense+organs+synthesizer+United+States+Patent+Application+20020173823&amp;rft_id=http%3A%2F%2Fwww.freepatentsonline.com%2F20020173823.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-vest-41"><span class="mw-cite-backlink">^ <a href="#cite_ref-vest_41-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-vest_41-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Tyler M, Danilov Y, Bach-y-Rita P (2003). "Closing an open-loop control system: vestibular substitution through the tongue". <i>Journal of Integrative Neuroscience</i>. <b>2</b> (2): 159–164. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1142%2FS0219635203000263">10.1142/S0219635203000263</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/15011268">15011268</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Integrative+Neuroscience&amp;rft.atitle=Closing+an+open-loop+control+system%3A+vestibular+substitution+through+the+tongue&amp;rft.volume=2&amp;rft.issue=2&amp;rft.pages=159-164&amp;rft.date=2003&amp;rft_id=info%3Adoi%2F10.1142%2FS0219635203000263&amp;rft_id=info%3Apmid%2F15011268&amp;rft.aulast=Tyler&amp;rft.aufirst=M&amp;rft.au=Danilov%2C+Y&amp;rft.au=Bach-y-Rita%2C+P&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-s-42"><span class="mw-cite-backlink">^ <a href="#cite_ref-s_42-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-s_42-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Bach-y-Rita P (1999). "Theoretical aspects of sensory substitution and of neurotransmission-related reorganization in spinal cord injury". <i>Spinal Cord</i>. <b>37</b> (7): 465–474. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1038%2Fsj.sc.3100873">10.1038/sj.sc.3100873</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/10438112">10438112</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Spinal+Cord&amp;rft.atitle=Theoretical+aspects+of+sensory+substitution+and+of+neurotransmission-related+reorganization+in+spinal+cord+injury&amp;rft.volume=37&amp;rft.issue=7&amp;rft.pages=465-474&amp;rft.date=1999&amp;rft_id=info%3Adoi%2F10.1038%2Fsj.sc.3100873&amp;rft_id=info%3Apmid%2F10438112&amp;rft.au=Bach-y-Rita+P&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-pros-43"><span class="mw-cite-backlink">^ <a href="#cite_ref-pros_43-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-pros_43-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Riso RR (1999). <a rel="nofollow" class="external text" href="https://web.archive.org/web/20100627025008/http://www.smpp.northwestern.edu/savedLiterature/Riso(1999)TechAndHealthCare7p401-409.pdf">"Strategies for providing upper extremity amputees with tactile and hand position feedback – moving closer to the bionic arm"</a> <span class="cs1-format">(PDF)</span>. <i>Technology and Health Care</i>. <b>7</b> (6): 401–409. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/10665673">10665673</a>. Archived from <a rel="nofollow" class="external text" href="http://www.smpp.northwestern.edu/savedLiterature/Riso(1999)TechAndHealthCare7p401-409.pdf">the original</a> <span class="cs1-format">(PDF)</span> on 2010-06-27<span class="reference-accessdate">. Retrieved <span class="nowrap">2016-07-21</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Technology+and+Health+Care&amp;rft.atitle=Strategies+for+providing+upper+extremity+amputees+with+tactile+and+hand+position+feedback+%E2%80%93+moving+closer+to+the+bionic+arm&amp;rft.volume=7&amp;rft.issue=6&amp;rft.pages=401-409&amp;rft.date=1999&amp;rft_id=info%3Apmid%2F10665673&amp;rft.au=Riso+RR&amp;rft_id=http%3A%2F%2Fwww.smpp.northwestern.edu%2FsavedLiterature%2FRiso%281999%29TechAndHealthCare7p401-409.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-voice-44"><span class="mw-cite-backlink">^ <a href="#cite_ref-voice_44-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-voice_44-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-voice_44-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Meijer PBL (1992). "An Experimental System for Auditory Image Representations". <i>IEEE Transactions on Biomedical Engineering</i>. <b>39</b> (2): 112–121. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1109%2F10.121642">10.1109/10.121642</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/1612614">1612614</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Biomedical+Engineering&amp;rft.atitle=An+Experimental+System+for+Auditory+Image+Representations&amp;rft.volume=39&amp;rft.issue=2&amp;rft.pages=112-121&amp;rft.date=1992&amp;rft_id=info%3Adoi%2F10.1109%2F10.121642&amp;rft_id=info%3Apmid%2F1612614&amp;rft.au=Meijer+PBL&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-45"><span class="mw-cite-backlink"><b><a href="#cite_ref-45">^</a></b></span> <span class="reference-text"><i>Neural Regeneration Research</i>, a peer-reviewed journal article, <a rel="nofollow" class="external text" href="http://www.nrronline.org/article.asp?issn=1673-5374;year=2015;volume=10;issue=11;spage=1717;epage=1719;aulast=Nau">Use of sensory substitution devices as a model system for investigating cross-modal neuroplasticity in humans</a></span>
</li>
<li id="cite_note-46"><span class="mw-cite-backlink"><b><a href="#cite_ref-46">^</a></b></span> <span class="reference-text"><i>Auditory scene analysis and sonified visual images. Does consonance negatively impact on object formation when using complex sonified stimuli?</i> <a rel="nofollow" class="external text" href="http://journal.frontiersin.org/article/10.3389/fpsyg.2015.01522/full">Original Research ARTICLE, <i>Frontiers in Psychology</i>, 13 October 2015 </a></span>
</li>
<li id="cite_note-nature.com-47"><span class="mw-cite-backlink">^ <a href="#cite_ref-nature.com_47-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-nature.com_47-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">A <i>Nature</i> Scientific Report, <a rel="nofollow" class="external text" href="https://www.nature.com/articles/srep15628">Auditory Sensory Substitution is Intuitive and Automatic with Texture Stimuli</a></span>
</li>
<li id="cite_note-Science_&#123;&#123;!&#125;&#125;_AAAS-48"><span class="mw-cite-backlink">^ <a href="#cite_ref-Science_{{!}}_AAAS_48-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Science_{{!}}_AAAS_48-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation news"><a rel="nofollow" class="external text" href="http://www.sciencemag.org/news/2014/03/computer-program-allows-blind-see-sound">"Computer Program Allows the Blind to 'See' With Sound"</a>. <i>Science | AAAS</i>. 2014-03-06<span class="reference-accessdate">. Retrieved <span class="nowrap">2017-04-15</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Science+%7C+AAAS&amp;rft.atitle=Computer+Program+Allows+the+Blind+to+%27See%27+With+Sound&amp;rft.date=2014-03-06&amp;rft_id=http%3A%2F%2Fwww.sciencemag.org%2Fnews%2F2014%2F03%2Fcomputer-program-allows-blind-see-sound&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-Popular_Science-49"><span class="mw-cite-backlink">^ <a href="#cite_ref-Popular_Science_49-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Popular_Science_49-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation news"><a rel="nofollow" class="external text" href="http://www.popsci.com/science/article/2013-07/synesthesia-blind">"Device Trains Blind People To 'See' By Listening"</a>. <i>Popular Science</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-04-15</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Popular+Science&amp;rft.atitle=Device+Trains+Blind+People+To+%27See%27+By+Listening&amp;rft_id=http%3A%2F%2Fwww.popsci.com%2Fscience%2Farticle%2F2013-07%2Fsynesthesia-blind&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-voicenn2007-50"><span class="mw-cite-backlink"><b><a href="#cite_ref-voicenn2007_50-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">A. Amedi; W. Stern; J. A. Camprodon; F. Bermpohl; L. Merabet; S. Rotman; C. Hemond; P. Meijer; A. Pascual-Leone (June 2007). "Shape conveyed by visual-to-auditory sensory substitution activates the lateral occipital complex". <i>Nature Neuroscience</i>. <b>10</b> (6): 687–689. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1038%2Fnn1912">10.1038/nn1912</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/17515898">17515898</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature+Neuroscience&amp;rft.atitle=Shape+conveyed+by+visual-to-auditory+sensory+substitution+activates+the+lateral+occipital+complex&amp;rft.volume=10&amp;rft.issue=6&amp;rft.pages=687-689&amp;rft.date=2007-06&amp;rft_id=info%3Adoi%2F10.1038%2Fnn1912&amp;rft_id=info%3Apmid%2F17515898&amp;rft.au=A.+Amedi&amp;rft.au=W.+Stern&amp;rft.au=J.+A.+Camprodon&amp;rft.au=F.+Bermpohl&amp;rft.au=L.+Merabet&amp;rft.au=S.+Rotman&amp;rft.au=C.+Hemond&amp;rft.au=P.+Meijer&amp;rft.au=A.+Pascual-Leone&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-voicepc2007-51"><span class="mw-cite-backlink"><b><a href="#cite_ref-voicepc2007_51-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">M. Auvray; S. Hanneton; J. K. O'Regan (2007). <a rel="nofollow" class="external text" href="http://www.nstu.net/malika-auvray/files/malika-auvray-auvray_hanneton_et_al_2007_perception_b.pdf">"Learning to perceive with a visuo-auditory substitution system: Localisation and object recognition with 'The vOICe<span class="cs1-kern-right">'</span>"</a> <span class="cs1-format">(PDF)</span>. <i>Perception</i>. <b>36</b> (3): 416–430. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1068%2Fp5631">10.1068/p5631</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/17455756">17455756</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Perception&amp;rft.atitle=Learning+to+perceive+with+a+visuo-auditory+substitution+system%3A+Localisation+and+object+recognition+with+%27The+vOICe%27&amp;rft.volume=36&amp;rft.issue=3&amp;rft.pages=416-430&amp;rft.date=2007&amp;rft_id=info%3Adoi%2F10.1068%2Fp5631&amp;rft_id=info%3Apmid%2F17455756&amp;rft.au=M.+Auvray&amp;rft.au=S.+Hanneton&amp;rft.au=J.+K.+O%27Regan&amp;rft_id=http%3A%2F%2Fwww.nstu.net%2Fmalika-auvray%2Ffiles%2Fmalika-auvray-auvray_hanneton_et_al_2007_perception_b.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-voiceplos2008-52"><span class="mw-cite-backlink"><b><a href="#cite_ref-voiceplos2008_52-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">M. J. Proulx; P. Stoerig; E. Ludowig; I. Knoll (March 2008). <a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC2267489">"Seeing 'Where' through the Ears: Effects of Learning-by-Doing and Long-Term Sensory Deprivation on Localization Based on Image-to-Sound Substitution"</a>. <i>PLoS ONE</i>. <b>3</b> (3): e1840. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="http://adsabs.harvard.edu/abs/2008PLoSO...3.1840P">2008PLoSO...3.1840P</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1371%2Fjournal.pone.0001840">10.1371/journal.pone.0001840</a>. <a href="/wiki/PubMed_Central" title="PubMed Central">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC2267489">2267489</a></span>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/18364998">18364998</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=PLoS+ONE&amp;rft.atitle=Seeing+%27Where%27+through+the+Ears%3A+Effects+of+Learning-by-Doing+and+Long-Term+Sensory+Deprivation+on+Localization+Based+on+Image-to-Sound+Substitution&amp;rft.volume=3&amp;rft.issue=3&amp;rft.pages=e1840&amp;rft.date=2008-03&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC2267489&amp;rft_id=info%3Apmid%2F18364998&amp;rft_id=info%3Adoi%2F10.1371%2Fjournal.pone.0001840&amp;rft_id=info%3Abibcode%2F2008PLoSO...3.1840P&amp;rft.au=M.+J.+Proulx&amp;rft.au=P.+Stoerig&amp;rft.au=E.+Ludowig&amp;rft.au=I.+Knoll&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC2267489&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-53"><span class="mw-cite-backlink"><b><a href="#cite_ref-53">^</a></b></span> <span class="reference-text">Article at the <i>American Academy of Ophthalmology</i> website, <a rel="nofollow" class="external text" href="https://www.aao.org/eyenet/academy-live/detail/humayun-restoring-vision-with-sensory-substitution"><i>Humayun: Restoring ‘Vision’ With Sensory Substitution Devices</i></a></span>
</li>
<li id="cite_note-54"><span class="mw-cite-backlink"><b><a href="#cite_ref-54">^</a></b></span> <span class="reference-text"><cite class="citation journal">Murphy, Matthew C.; Nau, Amy C.; Fisher, Christopher; Kim, Seong-Gi; Schuman, Joel S.; Chan, Kevin C. (January 2016). <a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC5536833">"Top-down influence on the visual cortex of the blind during sensory substitution"</a>. <i>NeuroImage</i>. <b>125</b>: 932–940. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1016%2Fj.neuroimage.2015.11.021">10.1016/j.neuroimage.2015.11.021</a>. <a href="/wiki/PubMed_Central" title="PubMed Central">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC5536833">5536833</a></span>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/26584776">26584776</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=NeuroImage&amp;rft.atitle=Top-down+influence+on+the+visual+cortex+of+the+blind+during+sensory+substitution&amp;rft.volume=125&amp;rft.pages=932-940&amp;rft.date=2016-01&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC5536833&amp;rft_id=info%3Apmid%2F26584776&amp;rft_id=info%3Adoi%2F10.1016%2Fj.neuroimage.2015.11.021&amp;rft.aulast=Murphy&amp;rft.aufirst=Matthew+C.&amp;rft.au=Nau%2C+Amy+C.&amp;rft.au=Fisher%2C+Christopher&amp;rft.au=Kim%2C+Seong-Gi&amp;rft.au=Schuman%2C+Joel+S.&amp;rft.au=Chan%2C+Kevin+C.&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC5536833&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-55"><span class="mw-cite-backlink"><b><a href="#cite_ref-55">^</a></b></span> <span class="reference-text"><i>How the brain's neuroplasticity lets us substitute one sense for another—and invent new ones</i> <a rel="nofollow" class="external text" href="https://motherboard.vice.com/en_us/article/rewiring-the-brain-to-create-new-senses">Rewiring the Brain to Create New Senses</a></span>
</li>
<li id="cite_note-56"><span class="mw-cite-backlink"><b><a href="#cite_ref-56">^</a></b></span> <span class="reference-text"><i>Research on synesthesia has led to devices that blur the lines between the senses, and may offer new hope for the blind</i> <a rel="nofollow" class="external text" href="http://protomag.com/articles/sensory-substitution">Sensory Substitution</a></span>
</li>
<li id="cite_note-57"><span class="mw-cite-backlink"><b><a href="#cite_ref-57">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://play.google.com/store/apps/details?id=vOICe.vOICe&amp;hl=en">"The vOICe for Android - Android Apps on Google Play"</a>. <i>play.google.com</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-04-15</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=play.google.com&amp;rft.atitle=The+vOICe+for+Android+-+Android+Apps+on+Google+Play&amp;rft_id=https%3A%2F%2Fplay.google.com%2Fstore%2Fapps%2Fdetails%3Fid%3DvOICe.vOICe%26hl%3Den&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-58"><span class="mw-cite-backlink"><b><a href="#cite_ref-58">^</a></b></span> <span class="reference-text"><cite class="citation news"><a rel="nofollow" class="external text" href="http://www.dailymail.co.uk/sciencetech/article-3305508/Seeing-SOUND-Glasses-translate-images-different-noises-help-blind-people-visualise-world-them.html">"Glasses which turn video into noise are allowing blind people to 'see<span class="cs1-kern-right">'</span>"</a>. <i>Mail Online</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-04-15</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Mail+Online&amp;rft.atitle=Glasses+which+turn+video+into+noise+are+allowing+blind+people+to+%27see%27&amp;rft_id=http%3A%2F%2Fwww.dailymail.co.uk%2Fsciencetech%2Farticle-3305508%2FSeeing-SOUND-Glasses-translate-images-different-noises-help-blind-people-visualise-world-them.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-59"><span class="mw-cite-backlink"><b><a href="#cite_ref-59">^</a></b></span> <span class="reference-text"><cite class="citation news">Doward, Jamie (2014-12-06). <a rel="nofollow" class="external text" href="https://www.theguardian.com/society/2014/dec/07/voice-soundscape-headsets-allow-blind-see">"vOICe: the soundscape headsets that allow blind people to 'see' the world"</a>. <i>The Guardian</i>. <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/0261-3077">0261-3077</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-04-15</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Guardian&amp;rft.atitle=vOICe%3A+the+soundscape+headsets+that+allow+blind+people+to+%E2%80%98see%E2%80%99+the+world&amp;rft.date=2014-12-06&amp;rft.issn=0261-3077&amp;rft.aulast=Doward&amp;rft.aufirst=Jamie&amp;rft_id=https%3A%2F%2Fwww.theguardian.com%2Fsociety%2F2014%2Fdec%2F07%2Fvoice-soundscape-headsets-allow-blind-see&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-60"><span class="mw-cite-backlink"><b><a href="#cite_ref-60">^</a></b></span> <span class="reference-text"><cite class="citation news">Chaturvedi, Pooja (22 Jan 2013). <a rel="nofollow" class="external text" href="http://www.livemint.com/Leisure/T0E1A40Qu81fO56hnNwPoM/Shooting-blind.html">"Shooting blind"</a>. <i>Livemint.com</i><span class="reference-accessdate">. Retrieved <span class="nowrap">16 April</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Livemint.com&amp;rft.atitle=Shooting+blind&amp;rft.date=2013-01-22&amp;rft.aulast=Chaturvedi&amp;rft.aufirst=Pooja&amp;rft_id=http%3A%2F%2Fwww.livemint.com%2FLeisure%2FT0E1A40Qu81fO56hnNwPoM%2FShooting-blind.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/><br />
<dl><dd>Relevant portion evidencing how far the technology has travelled and empowered the differently-abled, <i>"... Delhi-based photographer Pranav Lal, who is also blind, uses vOICe, a camera-based visual sound technology ... experience of live camera views ... image-to-sound rendering ..." </i>.<br /></dd></dl>
</span></li>
<li id="cite_note-61"><span class="mw-cite-backlink"><b><a href="#cite_ref-61">^</a></b></span> <span class="reference-text"><cite class="citation news">Gurung, Regina (15 Nov 2016). <a rel="nofollow" class="external text" href="http://www.newindianexpress.com/cities/bengaluru/2016/nov/15/he-hears-shapes-clicks-it-1538959.html">"He hears shapes, clicks it!"</a>. <i><a href="/wiki/The_New_Indian_Express" title="The New Indian Express">The New Indian Express</a></i><span class="reference-accessdate">. Retrieved <span class="nowrap">16 April</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+Indian+Express&amp;rft.atitle=He+hears+shapes%2C+clicks+it%21&amp;rft.date=2016-11-15&amp;rft.aulast=Gurung&amp;rft.aufirst=Regina&amp;rft_id=http%3A%2F%2Fwww.newindianexpress.com%2Fcities%2Fbengaluru%2F2016%2Fnov%2F15%2Fhe-hears-shapes-clicks-it-1538959.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-62"><span class="mw-cite-backlink"><b><a href="#cite_ref-62">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://seeingwithsound.tumblr.com">"The vOICe - Seeing with Sound"</a>. <i>The vOICe - Seeing with Sound</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-04-15</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+vOICe+-+Seeing+with+Sound&amp;rft.atitle=The+vOICe+-+Seeing+with+Sound&amp;rft_id=http%3A%2F%2Fseeingwithsound.tumblr.com&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-63"><span class="mw-cite-backlink"><b><a href="#cite_ref-63">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://www.wired.com/2017/03/book-excerpt-body-builders">"Meet the Woman Who Can See With Her Ears"</a>. <i>Wired.com</i><span class="reference-accessdate">. Retrieved <span class="nowrap">8 January</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Wired.com&amp;rft.atitle=Meet+the+Woman+Who+Can+See+With+Her+Ears&amp;rft_id=https%3A%2F%2Fwww.wired.com%2F2017%2F03%2Fbook-excerpt-body-builders&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-64"><span class="mw-cite-backlink"><b><a href="#cite_ref-64">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://www.sciencemag.org/news/2014/03/computer-program-allows-blind-see-sound">"Computer Program Allows the Blind to 'See' With Sound"</a>. <i>Sciencemag.org</i>. 6 March 2014<span class="reference-accessdate">. Retrieved <span class="nowrap">8 January</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Sciencemag.org&amp;rft.atitle=Computer+Program+Allows+the+Blind+to+%27See%27+With+Sound&amp;rft.date=2014-03-06&amp;rft_id=http%3A%2F%2Fwww.sciencemag.org%2Fnews%2F2014%2F03%2Fcomputer-program-allows-blind-see-sound&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-65"><span class="mw-cite-backlink"><b><a href="#cite_ref-65">^</a></b></span> <span class="reference-text"><cite class="citation web">Doward, Jamie (7 December 2014). <a rel="nofollow" class="external text" href="https://www.theguardian.com/society/2014/dec/07/voice-soundscape-headsets-allow-blind-see">"vOICe: the soundscape headsets that allow blind people to 'see' the world"</a>. <i>Theguardian.com</i><span class="reference-accessdate">. Retrieved <span class="nowrap">8 January</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Theguardian.com&amp;rft.atitle=vOICe%3A+the+soundscape+headsets+that+allow+blind+people+to+%27see%27+the+world&amp;rft.date=2014-12-07&amp;rft.aulast=Doward&amp;rft.aufirst=Jamie&amp;rft_id=https%3A%2F%2Fwww.theguardian.com%2Fsociety%2F2014%2Fdec%2F07%2Fvoice-soundscape-headsets-allow-blind-see&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-66"><span class="mw-cite-backlink"><b><a href="#cite_ref-66">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://www.popsci.com/science/article/2013-07/synesthesia-blind">"Device Trains Blind People To 'See' By Listening"</a>. <i>Popsci.com</i><span class="reference-accessdate">. Retrieved <span class="nowrap">8 January</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Popsci.com&amp;rft.atitle=Device+Trains+Blind+People+To+%27See%27+By+Listening&amp;rft_id=http%3A%2F%2Fwww.popsci.com%2Fscience%2Farticle%2F2013-07%2Fsynesthesia-blind&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-67"><span class="mw-cite-backlink"><b><a href="#cite_ref-67">^</a></b></span> <span class="reference-text"><cite class="citation journal">"Article Detail - International Journal of Advanced Research". <i>International Journal of Advanced Research</i>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.21474%2Fijar01%2F2304%23sthash.usfrrruw.dpuf">10.21474/ijar01/2304#sthash.usfrrruw.dpuf</a> (inactive 2019-03-14).</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Advanced+Research&amp;rft.atitle=Article+Detail+-+International+Journal+of+Advanced+Research&amp;rft_id=info%3Adoi%2F10.21474%2Fijar01%2F2304%23sthash.usfrrruw.dpuf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-68"><span class="mw-cite-backlink"><b><a href="#cite_ref-68">^</a></b></span> <span class="reference-text"><cite class="citation journal">Debargha Ganguly (2016). <a rel="nofollow" class="external text" href="http://rgdoi.net/10.13140/RG.2.2.12296.62729">"PROJECT BAT-EYE -Developing an Economic System that can give a Blind Person Basic Spatial Awareness and Object Identification (Updated ) (PDF Download Available)"</a>. <i>ResearchGate</i>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.13140%2Frg.2.2.12296.62729">10.13140/rg.2.2.12296.62729</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=ResearchGate&amp;rft.atitle=PROJECT+BAT-EYE+-Developing+an+Economic+System+that+can+give+a+Blind+Person+Basic+Spatial+Awareness+and+Object+Identification+%28Updated+%29+%28PDF+Download+Available%29&amp;rft.date=2016&amp;rft_id=info%3Adoi%2F10.13140%2Frg.2.2.12296.62729&amp;rft.au=Debargha+Ganguly&amp;rft_id=http%3A%2F%2Frgdoi.net%2F10.13140%2FRG.2.2.12296.62729&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-69"><span class="mw-cite-backlink"><b><a href="#cite_ref-69">^</a></b></span> <span class="reference-text"><cite class="citation journal">Debargha Ganguly (2018). <a rel="nofollow" class="external text" href="http://rgdoi.net/10.13140/RG.2.2.19059.94243">"PROJECT " AWAAZ " - Developing an extremely low cost, non-movement restrictive plugin for hand movement to speech conversion (PDF Download Available)"</a>. <i>ResearchGate</i>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.13140%2Frg.2.2.19059.94243">10.13140/rg.2.2.19059.94243</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=ResearchGate&amp;rft.atitle=PROJECT+%22+AWAAZ+%22+-+Developing+an+extremely+low+cost%2C+non-movement+restrictive+plugin+for+hand+movement+to+speech+conversion+%28PDF+Download+Available%29&amp;rft.date=2018&amp;rft_id=info%3Adoi%2F10.13140%2Frg.2.2.19059.94243&amp;rft.au=Debargha+Ganguly&amp;rft_id=http%3A%2F%2Frgdoi.net%2F10.13140%2FRG.2.2.19059.94243&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-70"><span class="mw-cite-backlink"><b><a href="#cite_ref-70">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://www.researchgate.net/project/Project-Basics-a-concerned-effort-to-improve-the-basic-standard-of-living-of-people-of-developing-and-underdeveloped-countries">"Project Basics- a concerned effort to improve the basic standard of living of people of developing and underdeveloped countries. by Debargha Ganguly - Research Project on ResearchGate"</a>. <i>ResearchGate</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-02-16</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=ResearchGate&amp;rft.atitle=Project+Basics-+a+concerned+effort+to+improve+the+basic+standard+of+living+of+people+of+developing+and+underdeveloped+countries.+by+Debargha+Ganguly+-+Research+Project+on+ResearchGate&amp;rft_id=https%3A%2F%2Fwww.researchgate.net%2Fproject%2FProject-Basics-a-concerned-effort-to-improve-the-basic-standard-of-living-of-people-of-developing-and-underdeveloped-countries&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-71"><span class="mw-cite-backlink"><b><a href="#cite_ref-71">^</a></b></span> <span class="reference-text"><cite class="citation journal">Levy-Tzedek, Shelly; Hanassy S.; Abboud S.; Maidenbaum S.; Amedi A. (January 1, 2012). <a rel="nofollow" class="external text" href="http://brain.huji.ac.il/publications/Levy-Tzedek_et_al_RNN_2012.pdf">"Fast, accurate reaching movements with a visual-to-auditory sensory substitution device"</a> <span class="cs1-format">(PDF)</span>. <i>Restorative Neurology and Neuroscience</i>. <b>30</b> (4): 313–323. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.3233%2FRNN-2012-110219">10.3233/RNN-2012-110219</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/22596353">22596353</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Restorative+Neurology+and+Neuroscience&amp;rft.atitle=Fast%2C+accurate+reaching+movements+with+a+visual-to-auditory+sensory+substitution+device&amp;rft.volume=30&amp;rft.issue=4&amp;rft.pages=313-323&amp;rft.date=2012-01-01&amp;rft_id=info%3Adoi%2F10.3233%2FRNN-2012-110219&amp;rft_id=info%3Apmid%2F22596353&amp;rft.aulast=Levy-Tzedek&amp;rft.aufirst=Shelly&amp;rft.au=Hanassy+S.&amp;rft.au=Abboud+S.&amp;rft.au=Maidenbaum+S.&amp;rft.au=Amedi+A.&amp;rft_id=http%3A%2F%2Fbrain.huji.ac.il%2Fpublications%2FLevy-Tzedek_et_al_RNN_2012.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-72"><span class="mw-cite-backlink"><b><a href="#cite_ref-72">^</a></b></span> <span class="reference-text"><cite class="citation journal">Abboud, Sami; Hanassy S; Levy-Tzedek S; Maidenbaum S; Amedi A. (2014). <a rel="nofollow" class="external text" href="http://brain.huji.ac.il/publications/Abboud_RNN_2014.pdf">"EyeMusic: Introducing a "visual" colorful experience for the blind using auditory sensory substitution"</a> <span class="cs1-format">(PDF)</span>. <i>Restorative Neurology and Neuroscience</i>. <b>32</b> (2): 247–257. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.3233%2FRNN-130338">10.3233/RNN-130338</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/24398719">24398719</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Restorative+Neurology+and+Neuroscience&amp;rft.atitle=EyeMusic%3A+Introducing+a+%22visual%22+colorful+experience+for+the+blind+using+auditory+sensory+substitution&amp;rft.volume=32&amp;rft.issue=2&amp;rft.pages=247-257&amp;rft.date=2014&amp;rft_id=info%3Adoi%2F10.3233%2FRNN-130338&amp;rft_id=info%3Apmid%2F24398719&amp;rft.aulast=Abboud&amp;rft.aufirst=Sami&amp;rft.au=Hanassy+S&amp;rft.au=Levy-Tzedek+S&amp;rft.au=Maidenbaum+S&amp;rft.au=Amedi+A.&amp;rft_id=http%3A%2F%2Fbrain.huji.ac.il%2Fpublications%2FAbboud_RNN_2014.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-73"><span class="mw-cite-backlink"><b><a href="#cite_ref-73">^</a></b></span> <span class="reference-text"><cite class="citation journal">Maidenbaum, Shachar; Abboud S.; Amedi A. (April 2014). <a rel="nofollow" class="external text" href="http://brain.huji.ac.il/publications/Maidenbaum_NBR_2013.pdf">"Sensory substitution: Closing the gap between basic research and widespread practical visual rehabilitation"</a> <span class="cs1-format">(PDF)</span>. <i>Neuroscience &amp; Biobehavioral Reviews</i>. <b>41</b>: 3–15. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1016%2Fj.neubiorev.2013.11.007">10.1016/j.neubiorev.2013.11.007</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/24275274">24275274</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neuroscience+%26+Biobehavioral+Reviews&amp;rft.atitle=Sensory+substitution%3A+Closing+the+gap+between+basic+research+and+widespread+practical+visual+rehabilitation&amp;rft.volume=41&amp;rft.pages=3-15&amp;rft.date=2014-04&amp;rft_id=info%3Adoi%2F10.1016%2Fj.neubiorev.2013.11.007&amp;rft_id=info%3Apmid%2F24275274&amp;rft.aulast=Maidenbaum&amp;rft.aufirst=Shachar&amp;rft.au=Abboud+S.&amp;rft.au=Amedi+A.&amp;rft_id=http%3A%2F%2Fbrain.huji.ac.il%2Fpublications%2FMaidenbaum_NBR_2013.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-74"><span class="mw-cite-backlink"><b><a href="#cite_ref-74">^</a></b></span> <span class="reference-text"><cite class="citation journal">Ambard, Maxime; Benezeth Y.; Pfister P. (2015). "Mobile video-to-audio transducer and motion detection for sensory substitution". <i>Frontiers in ICT</i>. <b>2</b>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.3389%2Ffict.2015.00020">10.3389/fict.2015.00020</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Frontiers+in+ICT&amp;rft.atitle=Mobile+video-to-audio+transducer+and+motion+detection+for+sensory+substitution&amp;rft.volume=2&amp;rft.date=2015&amp;rft_id=info%3Adoi%2F10.3389%2Ffict.2015.00020&amp;rft.aulast=Ambard&amp;rft.aufirst=Maxime&amp;rft.au=Benezeth+Y.&amp;rft.au=Pfister+P.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-pvsa-75"><span class="mw-cite-backlink">^ <a href="#cite_ref-pvsa_75-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-pvsa_75-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Capelle C, Trullemans C, Arno P, Veraart C (1998). "A real-time experimental prototype for enhancement of Vision Rehabilitation–vision rehabilitation using auditory substitution". <i>IEEE Transactions on Biomedical Engineering</i>. <b>45</b> (10): 1279–1293. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1109%2F10.720206">10.1109/10.720206</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/9775542">9775542</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Biomedical+Engineering&amp;rft.atitle=A+real-time+experimental+prototype+for+enhancement+of+Vision+Rehabilitation%E2%80%93vision+rehabilitation+using+auditory+substitution.&amp;rft.volume=45&amp;rft.issue=10&amp;rft.pages=1279-1293&amp;rft.date=1998&amp;rft_id=info%3Adoi%2F10.1109%2F10.720206&amp;rft_id=info%3Apmid%2F9775542&amp;rft.au=Capelle+C%2C+Trullemans+C%2C+Arno+P%2C+Veraart+C&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><span class="cs1-maint citation-comment">CS1 maint: Multiple names: authors list (<a href="/wiki/Category:CS1_maint:_Multiple_names:_authors_list" title="Category:CS1 maint: Multiple names: authors list">link</a>)</span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-76"><span class="mw-cite-backlink"><b><a href="#cite_ref-76">^</a></b></span> <span class="reference-text">Nielson L, Mahowald M, Mead C (1989). "SeeHear," in <i>Analog VLSI and Neural Systems,</i> by C. Mead, Reading: Addison-Wesley, chapter 13, 207–227.</span>
</li>
<li id="cite_note-77"><span class="mw-cite-backlink"><b><a href="#cite_ref-77">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external autonumber" href="http://ppl.gordon.edu/sensory_substitution.php">[1]</a><sup class="noprint Inline-Template"><span style="white-space: nowrap;">&#91;<i><a href="/wiki/Wikipedia:Link_rot" title="Wikipedia:Link rot"><span title="&#160;Dead link since July 2016">dead link</span></a></i>&#93;</span></sup></span>
</li>
<li id="cite_note-78"><span class="mw-cite-backlink"><b><a href="#cite_ref-78">^</a></b></span> <span class="reference-text"><cite class="citation journal">Warwick K, Gasson M, Hutt B, Goodhew I, Kyberd P, Schulzrinne H, Wu X (2004). "Thought communication and control: A first step using radiotelegraphy". <i>IEEE Proceedings on Communications</i>. <b>151</b> (3): 185–189. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1049%2Fip-com%3A20040409">10.1049/ip-com:20040409</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Proceedings+on+Communications&amp;rft.atitle=Thought+communication+and+control%3A+A+first+step+using+radiotelegraphy&amp;rft.volume=151&amp;rft.issue=3&amp;rft.pages=185-189&amp;rft.date=2004&amp;rft_id=info%3Adoi%2F10.1049%2Fip-com%3A20040409&amp;rft.aulast=Warwick&amp;rft.aufirst=K&amp;rft.au=Gasson%2C+M&amp;rft.au=Hutt%2C+B&amp;rft.au=Goodhew%2C+I&amp;rft.au=Kyberd%2C+P&amp;rft.au=Schulzrinne%2C+H&amp;rft.au=Wu%2C+X&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-79"><span class="mw-cite-backlink"><b><a href="#cite_ref-79">^</a></b></span> <span class="reference-text"><cite class="citation book">Lenay C, Gapenne O, Hanneton S, Marque C, Geouelle C (2003). "Sensory Substitution: limits and perspectives". <a rel="nofollow" class="external text" href="https://www.sfu.ca/~kathleea/docs/(A%20Guide%20to%20Sensory%20Substitution)%20-%20Sensory%20Substitution%20-%20limits%20and%20perspectives%20copy.pdf"><i>Touching for Knowing, Cognitive psychology of haptic manual perception</i></a> <span class="cs1-format">(PDF)</span>. pp.&#160;275–292.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Sensory+Substitution%3A+limits+and+perspectives&amp;rft.btitle=Touching+for+Knowing%2C+Cognitive+psychology+of+haptic+manual+perception&amp;rft.pages=275-292&amp;rft.date=2003&amp;rft.aulast=Lenay&amp;rft.aufirst=C&amp;rft.au=Gapenne%2C+O&amp;rft.au=Hanneton%2C+S&amp;rft.au=Marque%2C+C&amp;rft.au=Geouelle%2C+C&amp;rft_id=https%3A%2F%2Fwww.sfu.ca%2F~kathleea%2Fdocs%2F%28A%2520Guide%2520to%2520Sensory%2520Substitution%29%2520-%2520Sensory%2520Substitution%2520-%2520limits%2520and%2520perspectives%2520copy.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-80"><span class="mw-cite-backlink"><b><a href="#cite_ref-80">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://web.archive.org/web/20140810201043/http://esenseproject.info/">"Archived copy"</a>. Archived from <a rel="nofollow" class="external text" href="http://esenseproject.info/">the original</a> on 2014-08-10<span class="reference-accessdate">. Retrieved <span class="nowrap">2014-08-06</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Archived+copy&amp;rft_id=http%3A%2F%2Fesenseproject.info%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><span class="cs1-maint citation-comment">CS1 maint: Archived copy as title (<a href="/wiki/Category:CS1_maint:_Archived_copy_as_title" title="Category:CS1 maint: Archived copy as title">link</a>)</span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-81"><span class="mw-cite-backlink"><b><a href="#cite_ref-81">^</a></b></span> <span class="reference-text"><cite class="citation journal">Nagel SK, Carl C, Kringe T, Märtin R, König P (2005). <a rel="nofollow" class="external text" href="http://www.iop.org/EJ/abstract/1741-2552/2/4/R02/">"Beyond sensory substitution – learning the sixth sense"</a>. <i>Journal of Neural Engineering</i>. <b>2</b> (4): R13–26. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1088%2F1741-2560%2F2%2F4%2Fr02">10.1088/1741-2560/2/4/r02</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/16317228">16317228</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Neural+Engineering&amp;rft.atitle=Beyond+sensory+substitution+%E2%80%93+learning+the+sixth+sense&amp;rft.volume=2&amp;rft.issue=4&amp;rft.pages=R13-26&amp;rft.date=2005&amp;rft_id=info%3Adoi%2F10.1088%2F1741-2560%2F2%2F4%2Fr02&amp;rft_id=info%3Apmid%2F16317228&amp;rft.aulast=Nagel&amp;rft.aufirst=SK&amp;rft.au=Carl%2C+C&amp;rft.au=Kringe%2C+T&amp;rft.au=M%C3%A4rtin%2C+R&amp;rft.au=K%C3%B6nig%2C+P&amp;rft_id=http%3A%2F%2Fwww.iop.org%2FEJ%2Fabstract%2F1741-2552%2F2%2F4%2FR02%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASensory+substitution" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
</ol></div></div>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit&amp;section=28" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a rel="nofollow" class="external text" href="http://www.wicab.com">Tongue display for sensory substitution</a></li>
<li><a rel="nofollow" class="external text" href="http://www.seeingwithsound.com">The vOICe auditory display for sensory substitution</a>.</li>
<li><a rel="nofollow" class="external text" href="https://web.archive.org/web/20070216024709/http://biomed.brown.edu/Courses/BI108/2006-108websites/group03retinalimplants/">Artificial Retinas</a></li>
<li><a rel="nofollow" class="external text" href="https://web.archive.org/web/20061122061515/http://www.utc.fr/gsp/publi/Lenay03-SensorySubstitution.pdf">Sensory Substitution:limits and perspectives C. Lenay et al.</a></li>
<li><a rel="nofollow" class="external text" href="http://sourceforge.net/projects/thevibe/">The Vibe</a></li>
<li><a rel="nofollow" class="external text" href="http://feelspace.cogsci.uni-osnabrueck.de">feelSpace - The Magnetic Perception Group of the University of Osnabrück</a></li>
<li><a rel="nofollow" class="external text" href="http://www.kromophone.com">The Kromophone</a></li>
<li><a rel="nofollow" class="external text" href="http://academicplatform.blogspot.com.tr/2009/05/deneme.html">Sensory Substitution For Blind (Nihat Erim İnceoğlu)</a></li></ul>
<div role="navigation" class="navbox" aria-labelledby="Brain–computer_interface" style="padding:3px"><table class="nowraplinks collapsible autocollapse navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="3"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Brain%E2%80%93computer_interface" title="Template:Brain–computer interface"><abbr title="View this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Brain%E2%80%93computer_interface" title="Template talk:Brain–computer interface"><abbr title="Discuss this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">t</abbr></a></li><li class="nv-edit"><a class="external text" href="//en.wikipedia.org/w/index.php?title=Template:Brain%E2%80%93computer_interface&amp;action=edit"><abbr title="Edit this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">e</abbr></a></li></ul></div><div id="Brain–computer_interface" style="font-size:114%;margin:0 4em"><a href="/wiki/Brain%E2%80%93computer_interface" title="Brain–computer interface">Brain–computer interface</a></div></th></tr><tr><th scope="row" class="navbox-group" style="width:1%">Technologies</th><td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Biomechatronics" title="Biomechatronics">Biomechatronics</a></li>
<li><a href="/wiki/Brain_implant" title="Brain implant">Brain implant</a></li>
<li><a href="/wiki/BrainGate" title="BrainGate">BrainGate</a></li>
<li><a href="/wiki/Brainport" title="Brainport">Brainport</a></li>
<li><a href="/wiki/Cyberware" title="Cyberware">Cyberware</a></li>
<li><a href="/wiki/Exocortex" class="mw-redirect" title="Exocortex">Exocortex</a></li>
<li><a href="/wiki/Intelligence_amplification" title="Intelligence amplification">Intelligence amplification</a></li>
<li><a href="/wiki/Isolated_brain" title="Isolated brain">Isolated brain</a></li>
<li><a href="/wiki/Neuroprosthetics" title="Neuroprosthetics">Neuroprosthetics</a></li>
<li><a href="/wiki/Neurotechnology" title="Neurotechnology">Neurotechnology</a></li>
<li><a href="/wiki/Optogenetics" title="Optogenetics">Optogenetics</a></li>
<li><a class="mw-selflink selflink">Sensory substitution</a></li>
<li><a href="/wiki/Stentrode" class="mw-redirect" title="Stentrode">Stentrode</a></li>
<li><a href="/wiki/Synthetic_telepathy" class="mw-redirect" title="Synthetic telepathy">Synthetic telepathy</a></li></ul>
</div></td><td class="navbox-image" rowspan="6" style="width:1px;padding:0px 0px 0px 2px"><div><a href="/wiki/File:BrainGate.jpg" class="image"><img alt="BrainGate.jpg" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/fd/BrainGate.jpg/100px-BrainGate.jpg" decoding="async" width="100" height="150" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fd/BrainGate.jpg/150px-BrainGate.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fd/BrainGate.jpg/200px-BrainGate.jpg 2x" data-file-width="1728" data-file-height="2592" /></a></div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Scientific phenomena</th><td class="navbox-list navbox-even hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Electrocorticography" title="Electrocorticography">Electrocorticography</a> (ECoG)</li>
<li><a href="/wiki/Neural_ensemble" title="Neural ensemble">Neural ensemble</a></li>
<li><a href="/wiki/Neuroplasticity" title="Neuroplasticity">Neuroplasticity</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Disciplines</th><td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Cognitive_science" title="Cognitive science">Cognitive science</a></li>
<li><a href="/wiki/Cognitive_neuroscience" title="Cognitive neuroscience">Cognitive neuroscience</a></li>
<li><a href="/wiki/Computational_neuroscience" title="Computational neuroscience">Computational neuroscience</a></li>
<li><a href="/wiki/Emerging_technologies#Acronyms" title="Emerging technologies">NBIC</a></li>
<li><a href="/wiki/Neural_engineering" title="Neural engineering">Neural engineering</a></li>
<li><a href="/wiki/Neuroscience" title="Neuroscience">Neuroscience</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Speculative</th><td class="navbox-list navbox-even hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Brain_transplant" title="Brain transplant">Brain transplant</a></li>
<li><a href="/wiki/Cyborg" title="Cyborg">Cyborg</a></li>
<li><a href="/wiki/Mind_uploading" title="Mind uploading">Mind uploading</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">People</th><td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Charles_Stross" title="Charles Stross">Charles Stross</a></li>
<li><a href="/wiki/Douglas_Engelbart" title="Douglas Engelbart">Douglas Engelbart</a></li>
<li><a href="/wiki/Hugh_Herr" title="Hugh Herr">Hugh Herr</a></li>
<li><a href="/wiki/J._C._R._Licklider" title="J. C. R. Licklider">J. C. R. Licklider</a></li>
<li><a href="/wiki/Kevin_Warwick" title="Kevin Warwick">Kevin Warwick</a></li>
<li><a href="/wiki/Matt_Nagle" title="Matt Nagle">Matt Nagle</a></li>
<li><a href="/wiki/Merlin_Donald" title="Merlin Donald">Merlin Donald</a></li>
<li><a href="/wiki/Miguel_Nicolelis" title="Miguel Nicolelis">Miguel Nicolelis</a></li>
<li><a href="/wiki/Peter_Kyberd" title="Peter Kyberd">Peter Kyberd</a></li>
<li><a href="/wiki/Steve_Mann" title="Steve Mann">Steve Mann</a></li>
<li><a href="/wiki/Vernor_Vinge" title="Vernor Vinge">Vernor Vinge</a></li>
<li><a href="/wiki/Yoky_Matsuoka" title="Yoky Matsuoka">Yoky Matsuoka</a></li>
<li><a href="/wiki/Edward_Boyden" title="Edward Boyden">Edward Boyden</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Other</th><td class="navbox-list navbox-even hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Human_enhancement" title="Human enhancement">Human enhancement</a></li>
<li><a href="/wiki/Neurohacking" title="Neurohacking">Neurohacking</a></li>
<li><a href="/wiki/Simulated_reality" title="Simulated reality">Simulated reality</a></li>
<li><a href="/wiki/Transhumanism" title="Transhumanism">Transhumanism</a></li></ul>
</div></td></tr><tr><td class="navbox-abovebelow hlist" colspan="3"><div>
<ul><li><img alt="Category" src="//upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/16px-Folder_Hexagonal_Icon.svg.png" decoding="async" title="Category" width="16" height="14" srcset="//upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/24px-Folder_Hexagonal_Icon.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/32px-Folder_Hexagonal_Icon.svg.png 2x" data-file-width="36" data-file-height="31" /> <b><a href="/wiki/Category:Brain%E2%80%93computer_interfacing" title="Category:Brain–computer interfacing">Category</a></b></li>
<li><img alt="Commons page" src="//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/12px-Commons-logo.svg.png" decoding="async" title="Commons page" width="12" height="16" srcset="//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/18px-Commons-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/24px-Commons-logo.svg.png 2x" data-file-width="1024" data-file-height="1376" /> <a href="https://commons.wikimedia.org/wiki/Category:Brain-computer_interfaces" class="extiw" title="commons:Category:Brain-computer interfaces"><b>Commons</b></a></li></ul>
</div></td></tr></tbody></table></div>
<!-- 
NewPP limit report
Parsed by mw1281
Cached time: 20190401212721
Cache expiry: 2592000
Dynamic content: false
CPU time usage: 0.908 seconds
Real time usage: 1.044 seconds
Preprocessor visited node count: 3770/1000000
Preprocessor generated node count: 0/1500000
Post‐expand include size: 152880/2097152 bytes
Template argument size: 2119/2097152 bytes
Highest expansion depth: 12/40
Expensive parser function count: 11/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 221556/5000000 bytes
Number of Wikibase entities loaded: 5/400
Lua time usage: 0.539/10.000 seconds
Lua memory usage: 6.03 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  897.049      1 -total
 70.96%  636.554      1 Template:Reflist
 46.18%  414.215     40 Template:Cite_journal
  7.79%   69.852      1 Template:According_to_whom
  6.40%   57.413     13 Template:Cite_web
  5.72%   51.293      2 Template:Redirect
  4.84%   43.460      1 Template:Fix-span
  4.54%   40.712      5 Template:Category_handler
  3.23%   28.976      1 Template:BCI
  3.13%   28.096      6 Template:Cite_news
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:1903855-0!canonical and timestamp 20190401212734 and revision id 890525548
 -->
</div><noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>
		
		<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Sensory_substitution&amp;oldid=890525548">https://en.wikipedia.org/w/index.php?title=Sensory_substitution&amp;oldid=890525548</a>"</div>
		
		<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Cognitive_neuroscience" title="Category:Cognitive neuroscience">Cognitive neuroscience</a></li><li><a href="/wiki/Category:Biomedical_engineering" title="Category:Biomedical engineering">Biomedical engineering</a></li><li><a href="/wiki/Category:Neural_engineering" title="Category:Neural engineering">Neural engineering</a></li><li><a href="/wiki/Category:Neuroprosthetics" title="Category:Neuroprosthetics">Neuroprosthetics</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:CS1_maint:_Uses_authors_parameter" title="Category:CS1 maint: Uses authors parameter">CS1 maint: Uses authors parameter</a></li><li><a href="/wiki/Category:Pages_with_DOIs_inactive_since_2019" title="Category:Pages with DOIs inactive since 2019">Pages with DOIs inactive since 2019</a></li><li><a href="/wiki/Category:CS1_maint:_Multiple_names:_authors_list" title="Category:CS1 maint: Multiple names: authors list">CS1 maint: Multiple names: authors list</a></li><li><a href="/wiki/Category:All_articles_with_dead_external_links" title="Category:All articles with dead external links">All articles with dead external links</a></li><li><a href="/wiki/Category:Articles_with_dead_external_links_from_July_2016" title="Category:Articles with dead external links from July 2016">Articles with dead external links from July 2016</a></li><li><a href="/wiki/Category:CS1_maint:_Archived_copy_as_title" title="Category:CS1 maint: Archived copy as title">CS1 maint: Archived copy as title</a></li><li><a href="/wiki/Category:All_articles_with_specifically_marked_weasel-worded_phrases" title="Category:All articles with specifically marked weasel-worded phrases">All articles with specifically marked weasel-worded phrases</a></li><li><a href="/wiki/Category:Articles_with_specifically_marked_weasel-worded_phrases_from_March_2017" title="Category:Articles with specifically marked weasel-worded phrases from March 2017">Articles with specifically marked weasel-worded phrases from March 2017</a></li><li><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_March_2016" title="Category:Articles with unsourced statements from March 2016">Articles with unsourced statements from March 2016</a></li><li><a href="/wiki/Category:Articles_with_a_promotional_tone_from_January_2018" title="Category:Articles with a promotional tone from January 2018">Articles with a promotional tone from January 2018</a></li><li><a href="/wiki/Category:All_articles_with_a_promotional_tone" title="Category:All articles with a promotional tone">All articles with a promotional tone</a></li></ul></div></div>
		
		<div class="visualClear"></div>
		
	</div>
</div>

		<div id="mw-navigation">
			<h2>Navigation menu</h2>
			<div id="mw-head">
									<div id="p-personal" role="navigation" aria-labelledby="p-personal-label">
						<h3 id="p-personal-label">Personal tools</h3>
						<ul>
							<li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Sensory+substitution" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Sensory+substitution" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o">Log in</a></li>						</ul>
					</div>
									<div id="left-navigation">
										<div id="p-namespaces" role="navigation" class="vectorTabs" aria-labelledby="p-namespaces-label">
						<h3 id="p-namespaces-label">Namespaces</h3>
						<ul>
							<li id="ca-nstab-main" class="selected"><span><a href="/wiki/Sensory_substitution" title="View the content page [c]" accesskey="c">Article</a></span></li><li id="ca-talk"><span><a href="/wiki/Talk:Sensory_substitution" rel="discussion" title="Discussion about the content page [t]" accesskey="t">Talk</a></span></li>						</ul>
					</div>
										<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
												<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-variants-label" />
						<h3 id="p-variants-label">
							<span>Variants</span>
						</h3>
						<ul class="menu">
													</ul>
					</div>
									</div>
				<div id="right-navigation">
										<div id="p-views" role="navigation" class="vectorTabs" aria-labelledby="p-views-label">
						<h3 id="p-views-label">Views</h3>
						<ul>
							<li id="ca-view" class="collapsible selected"><span><a href="/wiki/Sensory_substitution">Read</a></span></li><li id="ca-edit" class="collapsible"><span><a href="/w/index.php?title=Sensory_substitution&amp;action=edit" title="Edit this page [e]" accesskey="e">Edit</a></span></li><li id="ca-history" class="collapsible"><span><a href="/w/index.php?title=Sensory_substitution&amp;action=history" title="Past revisions of this page [h]" accesskey="h">View history</a></span></li>						</ul>
					</div>
										<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
						<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-cactions-label" />
						<h3 id="p-cactions-label"><span>More</span></h3>
						<ul class="menu">
													</ul>
					</div>
										<div id="p-search" role="search">
						<h3>
							<label for="searchInput">Search</label>
						</h3>
						<form action="/w/index.php" id="searchform">
							<div id="simpleSearch">
								<input type="search" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" accesskey="f" id="searchInput"/><input type="hidden" value="Special:Search" name="title"/><input type="submit" name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton"/><input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton"/>							</div>
						</form>
					</div>
									</div>
			</div>
			<div id="mw-panel">
				<div id="p-logo" role="banner"><a class="mw-wiki-logo" href="/wiki/Main_Page" title="Visit the main page"></a></div>
						<div class="portal" role="navigation" id="p-navigation" aria-labelledby="p-navigation-label">
			<h3 id="p-navigation-label">Navigation</h3>
			<div class="body">
								<ul>
					<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-interaction" aria-labelledby="p-interaction-label">
			<h3 id="p-interaction-label">Interaction</h3>
			<div class="body">
								<ul>
					<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-tb" aria-labelledby="p-tb-label">
			<h3 id="p-tb-label">Tools</h3>
			<div class="body">
								<ul>
					<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Sensory_substitution" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Sensory_substitution" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Sensory_substitution&amp;oldid=890525548" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Sensory_substitution&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q356133" title="Link to connected data repository item [g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Sensory_substitution&amp;id=890525548" title="Information on how to cite this page">Cite this page</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-coll-print_export" aria-labelledby="p-coll-print_export-label">
			<h3 id="p-coll-print_export-label">Print/export</h3>
			<div class="body">
								<ul>
					<li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Sensory+substitution">Create a book</a></li><li id="coll-download-as-rdf2latex"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=Sensory+substitution&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a href="/w/index.php?title=Sensory_substitution&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-wikibase-otherprojects" aria-labelledby="p-wikibase-otherprojects-label">
			<h3 id="p-wikibase-otherprojects-label">In other projects</h3>
			<div class="body">
								<ul>
					<li class="wb-otherproject-link wb-otherproject-commons"><a href="https://commons.wikimedia.org/wiki/Category:Sensory_substitution" hreflang="en">Wikimedia Commons</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-lang" aria-labelledby="p-lang-label">
			<h3 id="p-lang-label">Languages</h3>
			<div class="body">
								<ul>
					<li class="interlanguage-link interwiki-de"><a href="https://de.wikipedia.org/wiki/Sensorische_Substitution" title="Sensorische Substitution – German" lang="de" hreflang="de" class="interlanguage-link-target">Deutsch</a></li><li class="interlanguage-link interwiki-es"><a href="https://es.wikipedia.org/wiki/The_vOICe" title="The vOICe – Spanish" lang="es" hreflang="es" class="interlanguage-link-target">Español</a></li><li class="interlanguage-link interwiki-fr"><a href="https://fr.wikipedia.org/wiki/Substitution_sensorielle" title="Substitution sensorielle – French" lang="fr" hreflang="fr" class="interlanguage-link-target">Français</a></li>				</ul>
				<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q356133#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>			</div>
		</div>
				</div>
		</div>
				<div id="footer" role="contentinfo">
						<ul id="footer-info">
								<li id="footer-info-lastmod"> This page was last edited on 1 April 2019, at 21:27<span class="anonymous-show">&#160;(UTC)</span>.</li>
								<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="//creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
							</ul>
						<ul id="footer-places">
								<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
								<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
								<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
								<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
								<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
								<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
								<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=Sensory_substitution&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
							</ul>
										<ul id="footer-icons" class="noprint">
										<li id="footer-copyrightico">
						<a href="https://wikimediafoundation.org/"><img src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation"/></a>					</li>
										<li id="footer-poweredbyico">
						<a href="//www.mediawiki.org/"><img src="/static/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"/></a>					</li>
									</ul>
						<div style="clear: both;"></div>
		</div>
		

<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.908","walltime":"1.044","ppvisitednodes":{"value":3770,"limit":1000000},"ppgeneratednodes":{"value":0,"limit":1500000},"postexpandincludesize":{"value":152880,"limit":2097152},"templateargumentsize":{"value":2119,"limit":2097152},"expansiondepth":{"value":12,"limit":40},"expensivefunctioncount":{"value":11,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":221556,"limit":5000000},"entityaccesscount":{"value":5,"limit":400},"timingprofile":["100.00%  897.049      1 -total"," 70.96%  636.554      1 Template:Reflist"," 46.18%  414.215     40 Template:Cite_journal","  7.79%   69.852      1 Template:According_to_whom","  6.40%   57.413     13 Template:Cite_web","  5.72%   51.293      2 Template:Redirect","  4.84%   43.460      1 Template:Fix-span","  4.54%   40.712      5 Template:Category_handler","  3.23%   28.976      1 Template:BCI","  3.13%   28.096      6 Template:Cite_news"]},"scribunto":{"limitreport-timeusage":{"value":"0.539","limit":"10.000"},"limitreport-memusage":{"value":6323299,"limit":52428800}},"cachereport":{"origin":"mw1281","timestamp":"20190401212721","ttl":2592000,"transientcontent":false}}});mw.config.set({"wgBackendResponseTime":115,"wgHostname":"mw1238"});});</script>
</body>
</html>
