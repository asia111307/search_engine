<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>Eye tracking - Wikipedia</title>
<script>document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );</script>
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Eye_tracking","wgTitle":"Eye tracking","wgCurRevisionId":890163428,"wgRevisionId":890163428,"wgArticleId":1543423,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["All articles with unsourced statements","Articles with unsourced statements from March 2009","Articles with unsourced statements from November 2018","Commons category link is on Wikidata","Articles needing more detailed references","Articles containing video clips","Attention","Cognitive science","Eye","History of human–computer interaction","Market research","Multimodal interaction","Promotion and marketing communications","Usability","Vision","Web design"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Eye_tracking","wgRelevantArticleId":1543423,"wgRequestId":"XLL8wQpAICAAACLOLd0AAAAT","wgCSPNonce":false,"wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgFlaggedRevsParams":{"tags":{}},"wgStableRevisionId":null,"wgBetaFeaturesFeatures":[],"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsReferencePreviews":false,"wgPopupsShouldSendModuleToUser":true,"wgPopupsConflictsWithNavPopupGadget":false,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en","usePageImages":true,"usePageDescriptions":true},"wgMFDisplayWikibaseDescriptions":{"search":true,"nearby":true,"watchlist":true,"tagline":false},"wgRelatedArticles":null,"wgRelatedArticlesUseCirrusSearch":true,"wgRelatedArticlesOnlyUseCirrusSearch":false,"wgWMESchemaEditAttemptStepOversample":false,"wgPoweredByHHVM":true,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralNoticeCookiesToDelete":[],"wgCentralNoticeCategoriesUsingLegacy":["Fundraising","fundraising"],"wgWikibaseItemId":"Q970687","wgScoreNoteLanguages":{"arabic":"العربية","catalan":"català","deutsch":"Deutsch","english":"English","espanol":"español","italiano":"italiano","nederlands":"Nederlands","norsk":"norsk","portugues":"português","suomi":"suomi","svenska":"svenska","vlaams":"West-Vlams"},"wgScoreDefaultNoteLanguage":"nederlands","wgCentralAuthMobileDomain":false,"wgVisualEditorUnsupportedEditParams":["undo","undoafter","veswitched"],"wgEditSubmitButtonLabelPublish":true});mw.loader.state({"ext.gadget.charinsert-styles":"ready","ext.globalCssJs.user.styles":"ready","ext.globalCssJs.site.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","ext.globalCssJs.site":"ready","user":"ready","user.options":"ready","user.tokens":"loading","ext.cite.styles":"ready","mediawiki.page.gallery.styles":"ready","ext.tmh.thumbnail.styles":"ready","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","mediawiki.toc.styles":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","ext.3d.styles":"ready","mediawiki.skinning.interface":"ready","skins.vector.styles":"ready"});mw.loader.implement("user.tokens@0tffind",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});RLPAGEMODULES=["ext.cite.ux-enhancements","mediawiki.page.gallery","mw.MediaWikiPlayer.loader","mw.PopUpMediaTransform","mw.TMHGalleryHook.js","site","mediawiki.page.startup","mediawiki.page.ready","mediawiki.toc","mediawiki.searchSuggest","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.eventlogger","ext.uls.init","ext.uls.compactlinks","ext.uls.interface","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp","skins.vector.js"];mw.loader.load(RLPAGEMODULES);});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.3d.styles%7Cext.cite.styles%7Cext.tmh.thumbnail.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.page.gallery.styles%7Cmediawiki.skinning.interface%7Cmediawiki.toc.styles%7Cskins.vector.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.gadget.charinsert-styles&amp;only=styles&amp;skin=vector"/>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.33.0-wmf.25"/>
<meta name="referrer" content="origin"/>
<meta name="referrer" content="origin-when-crossorigin"/>
<meta name="referrer" content="origin-when-cross-origin"/>
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv/640px--Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv.jpg"/>
<link rel="alternate" href="android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/Eye_tracking"/>
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Eye_tracking&amp;action=edit"/>
<link rel="edit" title="Edit this page" href="/w/index.php?title=Eye_tracking&amp;action=edit"/>
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png"/>
<link rel="shortcut icon" href="/static/favicon/wikipedia.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd"/>
<link rel="license" href="//creativecommons.org/licenses/by-sa/3.0/"/>
<link rel="canonical" href="https://en.wikipedia.org/wiki/Eye_tracking"/>
<link rel="dns-prefetch" href="//login.wikimedia.org"/>
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<!--[if lt IE 9]><script src="/w/load.php?lang=en&amp;modules=html5shiv&amp;only=scripts&amp;skin=vector&amp;sync=1"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Eye_tracking rootpage-Eye_tracking skin-vector action-view">
<div id="mw-page-base" class="noprint"></div>
<div id="mw-head-base" class="noprint"></div>
<div id="content" class="mw-body" role="main">
	<a id="top"></a>
	<div id="siteNotice" class="mw-body-content"><!-- CentralNotice --></div>
	<div class="mw-indicators mw-body-content">
</div>

	<h1 id="firstHeading" class="firstHeading" lang="en">Eye tracking</h1>
	
	<div id="bodyContent" class="mw-body-content">
		<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
		<div id="contentSub"></div>
		
		
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="#p-search">Jump to search</a>
		<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="mw-parser-output"><div role="note" class="hatnote navigation-not-searchable">This article is about the study of eye movement. For the tendency to visually track potential prey, see <a href="/wiki/Eye-stalking" class="mw-redirect" title="Eye-stalking">eye-stalking</a>.</div>
<div class="thumb tright"><div class="thumbinner" style="width:222px;"><div id="mwe_player_0" class="PopUpMediaTransform" style="width:220px;" videopayload="&lt;div class=&quot;mediaContainer&quot; style=&quot;width:640px&quot;&gt;&lt;video id=&quot;mwe_player_1&quot; poster=&quot;//upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv/640px--Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv.jpg&quot; controls=&quot;&quot; preload=&quot;none&quot; autoplay=&quot;&quot; style=&quot;width:640px;height:480px&quot; class=&quot;kskin&quot; data-durationhint=&quot;49.72&quot; data-startoffset=&quot;0&quot; data-mwtitle=&quot;Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv&quot; data-mwprovider=&quot;wikimediacommons&quot;&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/transcoded/b/bc/Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv/Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv.480p.webm&quot; type=&quot;video/webm; codecs=&amp;quot;vp8, vorbis&amp;quot;&quot; data-title=&quot;SD WebM (480P)&quot; data-shorttitle=&quot;WebM 480P&quot; data-transcodekey=&quot;480p.webm&quot; data-width=&quot;640&quot; data-height=&quot;480&quot; data-bandwidth=&quot;1017192&quot; data-framerate=&quot;25&quot;/&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/transcoded/b/bc/Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv/Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv.480p.vp9.webm&quot; type=&quot;video/webm; codecs=&amp;quot;vp9, opus&amp;quot;&quot; data-title=&quot;SD VP9 (480P)&quot; data-shorttitle=&quot;VP9 480P&quot; data-transcodekey=&quot;480p.vp9.webm&quot; data-width=&quot;640&quot; data-height=&quot;480&quot; data-bandwidth=&quot;1277528&quot; data-framerate=&quot;25&quot;/&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/b/bc/Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv&quot; type=&quot;video/ogg; codecs=&amp;quot;theora, vorbis&amp;quot;&quot; data-title=&quot;Original Ogg file, 640 × 480 (2.7 Mbps)&quot; data-shorttitle=&quot;Ogg source&quot; data-width=&quot;640&quot; data-height=&quot;480&quot; data-bandwidth=&quot;2698491&quot; data-framerate=&quot;25&quot;/&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/transcoded/b/bc/Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv/Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv.120p.vp9.webm&quot; type=&quot;video/webm; codecs=&amp;quot;vp9, opus&amp;quot;&quot; data-title=&quot;Lowest bandwidth VP9 (120P)&quot; data-shorttitle=&quot;VP9 120P&quot; data-transcodekey=&quot;120p.vp9.webm&quot; data-width=&quot;160&quot; data-height=&quot;120&quot; data-bandwidth=&quot;124728&quot; data-framerate=&quot;25&quot;/&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/transcoded/b/bc/Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv/Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv.160p.webm&quot; type=&quot;video/webm; codecs=&amp;quot;vp8, vorbis&amp;quot;&quot; data-title=&quot;Low bandwidth WebM (160P)&quot; data-shorttitle=&quot;WebM 160P&quot; data-transcodekey=&quot;160p.webm&quot; data-width=&quot;214&quot; data-height=&quot;160&quot; data-bandwidth=&quot;130200&quot; data-framerate=&quot;25&quot;/&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/transcoded/b/bc/Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv/Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv.180p.vp9.webm&quot; type=&quot;video/webm; codecs=&amp;quot;vp9, opus&amp;quot;&quot; data-title=&quot;Low bandwidth VP9 (180P)&quot; data-shorttitle=&quot;VP9 180P&quot; data-transcodekey=&quot;180p.vp9.webm&quot; data-width=&quot;240&quot; data-height=&quot;180&quot; data-bandwidth=&quot;204336&quot; data-framerate=&quot;25&quot;/&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/transcoded/b/bc/Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv/Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv.240p.webm&quot; type=&quot;video/webm; codecs=&amp;quot;vp8, vorbis&amp;quot;&quot; data-title=&quot;Small WebM (240P)&quot; data-shorttitle=&quot;WebM 240P&quot; data-transcodekey=&quot;240p.webm&quot; data-width=&quot;320&quot; data-height=&quot;240&quot; data-bandwidth=&quot;256192&quot; data-framerate=&quot;25&quot;/&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/transcoded/b/bc/Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv/Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv.240p.vp9.webm&quot; type=&quot;video/webm; codecs=&amp;quot;vp9, opus&amp;quot;&quot; data-title=&quot;Small VP9 (240P)&quot; data-shorttitle=&quot;VP9 240P&quot; data-transcodekey=&quot;240p.vp9.webm&quot; data-width=&quot;320&quot; data-height=&quot;240&quot; data-bandwidth=&quot;323720&quot; data-framerate=&quot;25&quot;/&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/transcoded/b/bc/Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv/Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv.360p.webm&quot; type=&quot;video/webm; codecs=&amp;quot;vp8, vorbis&amp;quot;&quot; data-title=&quot;WebM (360P)&quot; data-shorttitle=&quot;WebM 360P&quot; data-transcodekey=&quot;360p.webm&quot; data-width=&quot;480&quot; data-height=&quot;360&quot; data-bandwidth=&quot;511672&quot; data-framerate=&quot;25&quot;/&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/transcoded/b/bc/Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv/Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv.360p.vp9.webm&quot; type=&quot;video/webm; codecs=&amp;quot;vp9, opus&amp;quot;&quot; data-title=&quot;VP9 (360P)&quot; data-shorttitle=&quot;VP9 360P&quot; data-transcodekey=&quot;360p.vp9.webm&quot; data-width=&quot;480&quot; data-height=&quot;360&quot; data-bandwidth=&quot;642192&quot; data-framerate=&quot;25&quot;/&gt;&lt;/video&gt;&lt;/div&gt;"><img alt="File:Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv" style="width:220px;height:165px" src="//upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv/220px--Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv.jpg" /><a href="//upload.wikimedia.org/wikipedia/commons/b/bc/Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv" title="Play media" target="new"><span class="play-btn-large"><span class="mw-tmh-playtext">Play media</span></span></a></div>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Exploring-Eye-Movements-in-Patients-with-Glaucoma-When-Viewing-a-Driving-Scene-pone.0009710.s001.ogv" class="internal" title="Enlarge"></a></div>Scientists track eye movements in glaucoma patients to check vision impairment while driving.</div></div></div>
<p><b>Eye tracking</b> is the process of measuring either the point of <a href="/wiki/Gaze_(physiology)" title="Gaze (physiology)">gaze</a> (where one is looking) or the motion of an eye relative to the head.  An <b>eye tracker</b> is a device for measuring eye positions and <a href="/wiki/Eye_movement_(sensory)" class="mw-redirect" title="Eye movement (sensory)">eye movement</a>.  Eye trackers are used in research on the <a href="/wiki/Visual_system" title="Visual system">visual system</a>, in psychology, in <a href="/wiki/Psycholinguistics" title="Psycholinguistics">psycholinguistics</a>, marketing, as an input device for <a href="/wiki/Human-computer_interaction" class="mw-redirect" title="Human-computer interaction">human-computer interaction</a>, and in product design.  There are a number of methods for measuring eye movement.  The most popular variant uses video images from which the eye position is extracted.  Other methods use <a href="/wiki/Search_coil" class="mw-redirect" title="Search coil">search coils</a> or are based on the <a href="/wiki/Electrooculography" title="Electrooculography">electrooculogram</a>.
</p>
<div class="thumb tright"><div class="thumbinner" style="width:222px;"><a href="/wiki/File:Yarbus_eye_tracker.jpg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/a/a8/Yarbus_eye_tracker.jpg/220px-Yarbus_eye_tracker.jpg" decoding="async" width="220" height="211" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/a/a8/Yarbus_eye_tracker.jpg/330px-Yarbus_eye_tracker.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/a/a8/Yarbus_eye_tracker.jpg/440px-Yarbus_eye_tracker.jpg 2x" data-file-width="756" data-file-height="724" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Yarbus_eye_tracker.jpg" class="internal" title="Enlarge"></a></div>Yarbus eye tracker from the 1960s.</div></div></div>
<div id="toc" class="toc"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2>Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#History"><span class="tocnumber">1</span> <span class="toctext">History</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Tracker_types"><span class="tocnumber">2</span> <span class="toctext">Tracker types</span></a>
<ul>
<li class="toclevel-2 tocsection-3"><a href="#Eye-attached_tracking"><span class="tocnumber">2.1</span> <span class="toctext">Eye-attached tracking</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="#Optical_tracking"><span class="tocnumber">2.2</span> <span class="toctext">Optical tracking</span></a></li>
<li class="toclevel-2 tocsection-5"><a href="#Electric_potential_measurement"><span class="tocnumber">2.3</span> <span class="toctext">Electric potential measurement</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-6"><a href="#Technologies_and_techniques"><span class="tocnumber">3</span> <span class="toctext">Technologies and techniques</span></a></li>
<li class="toclevel-1 tocsection-7"><a href="#Data_presentation"><span class="tocnumber">4</span> <span class="toctext">Data presentation</span></a></li>
<li class="toclevel-1 tocsection-8"><a href="#Eye-tracking_vs._gaze-tracking"><span class="tocnumber">5</span> <span class="toctext">Eye-tracking vs. gaze-tracking</span></a></li>
<li class="toclevel-1 tocsection-9"><a href="#Practice"><span class="tocnumber">6</span> <span class="toctext">Practice</span></a>
<ul>
<li class="toclevel-2 tocsection-10"><a href="#Eye-tracking_while_driving_a_car_in_a_difficult_situation"><span class="tocnumber">6.1</span> <span class="toctext">Eye-tracking while driving a car in a difficult situation</span></a></li>
<li class="toclevel-2 tocsection-11"><a href="#Eye-tracking_of_younger_and_elderly_people_while_walking"><span class="tocnumber">6.2</span> <span class="toctext">Eye-tracking of younger and elderly people while walking</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-12"><a href="#Applications"><span class="tocnumber">7</span> <span class="toctext">Applications</span></a>
<ul>
<li class="toclevel-2 tocsection-13"><a href="#Commercial_applications"><span class="tocnumber">7.1</span> <span class="toctext">Commercial applications</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-14"><a href="#See_also"><span class="tocnumber">8</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-15"><a href="#Notes"><span class="tocnumber">9</span> <span class="toctext">Notes</span></a></li>
<li class="toclevel-1 tocsection-16"><a href="#References"><span class="tocnumber">10</span> <span class="toctext">References</span></a>
<ul>
<li class="toclevel-2 tocsection-17"><a href="#Commercial_eye_tracking"><span class="tocnumber">10.1</span> <span class="toctext">Commercial eye tracking</span></a></li>
</ul>
</li>
</ul>
</div>

<h2><span class="mw-headline" id="History">History</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Eye_tracking&amp;action=edit&amp;section=1" title="Edit section: History">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>In the 1800s, studies of eye movement were made using direct observations.
</p><p>In 1879 in Paris, <a href="/wiki/Louis_%C3%89mile_Javal" title="Louis Émile Javal">Louis Émile Javal</a> observed that reading does not involve a smooth sweeping of the eyes along the text, as previously assumed, but a series of short stops (called fixations) and quick <a href="/wiki/Saccade" title="Saccade">saccades</a>.<sup id="cite_ref-1" class="reference"><a href="#cite_note-1">&#91;1&#93;</a></sup>  This observation raised important questions about reading, questions which were explored during the 1900s: On which words do the eyes stop? For how long? When do they regress to already seen words?
</p>
<div class="thumb tleft"><div class="thumbinner" style="width:322px;"><a href="/wiki/File:Reading_Fixations_Saccades.jpg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/e/ef/Reading_Fixations_Saccades.jpg/320px-Reading_Fixations_Saccades.jpg" decoding="async" width="320" height="240" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/e/ef/Reading_Fixations_Saccades.jpg/480px-Reading_Fixations_Saccades.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/e/ef/Reading_Fixations_Saccades.jpg/640px-Reading_Fixations_Saccades.jpg 2x" data-file-width="5760" data-file-height="4320" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Reading_Fixations_Saccades.jpg" class="internal" title="Enlarge"></a></div>An example of fixations and saccades over text. This is the typical pattern of eye movement during reading. The eyes never move smoothly over still text.</div></div></div>
<p>Edmund Huey<sup id="cite_ref-2" class="reference"><a href="#cite_note-2">&#91;2&#93;</a></sup> built an early eye tracker, using a sort of contact lens with a hole for the <a href="/wiki/Pupil" title="Pupil">pupil</a>. The lens was connected to an aluminum pointer that moved in response to the movement of the eye. Huey studied and quantified regressions (only a small proportion of saccades are regressions), and he showed that some words in a sentence are not fixated.
</p><p>The first non-intrusive eye-trackers were built by Guy Thomas Buswell in Chicago, using beams of light that were reflected on the eye and then recording them on film. Buswell made systematic studies into reading<sup id="cite_ref-3" class="reference"><a href="#cite_note-3">&#91;3&#93;</a></sup> and picture viewing.<sup id="cite_ref-4" class="reference"><a href="#cite_note-4">&#91;4&#93;</a></sup>
</p><p>In the 1950s, <a href="/wiki/Alfred_L._Yarbus" title="Alfred L. Yarbus">Alfred L. Yarbus</a><sup id="cite_ref-5" class="reference"><a href="#cite_note-5">&#91;5&#93;</a></sup> did important eye tracking research and his 1967 book is often quoted. He showed that the task given to a subject has a very large influence on the subject's eye movement. He also wrote about the relation between fixations and interest:
</p>
<dl><dd>"All the records ... show conclusively that the character of the eye movement is either completely independent of or only very slightly dependent on the material of the picture and how it was made, provided that it is flat or nearly flat."<sup id="cite_ref-Yarbus_1967_190_6-0" class="reference"><a href="#cite_note-Yarbus_1967_190-6">&#91;6&#93;</a></sup> The cyclical pattern in the examination of pictures "is dependent on not only what is shown on the picture, but also the problem facing the observer and the information that he hopes to gain from the picture."<sup id="cite_ref-7" class="reference"><a href="#cite_note-7">&#91;7&#93;</a></sup></dd></dl>
<div class="thumb tright"><div class="thumbinner" style="width:322px;"><a href="/wiki/File:Yarbus_The_Visitor.jpg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/d/d2/Yarbus_The_Visitor.jpg/320px-Yarbus_The_Visitor.jpg" decoding="async" width="320" height="273" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/d/d2/Yarbus_The_Visitor.jpg/480px-Yarbus_The_Visitor.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/d/d2/Yarbus_The_Visitor.jpg/640px-Yarbus_The_Visitor.jpg 2x" data-file-width="808" data-file-height="690" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Yarbus_The_Visitor.jpg" class="internal" title="Enlarge"></a></div>This study by <a href="#CITEREFYarbus1967">Yarbus (1967)</a> is often referred to as evidence on how the task given to a person influences his or her eye movement.</div></div></div>
<dl><dd>"Records of eye movements show that the observer's attention is usually held only by certain elements of the picture.... Eye movement reflects the human thought processes; so the observer's thought may be followed to some extent from records of eye movement (the thought accompanying the examination of the particular object). It is easy to determine from these records which elements attract the observer's eye (and, consequently, his thought), in what order, and how often."<sup id="cite_ref-Yarbus_1967_190_6-1" class="reference"><a href="#cite_note-Yarbus_1967_190-6">&#91;6&#93;</a></sup></dd></dl>
<dl><dd>"The observer's attention is frequently drawn to elements which do not give important information but which, in his opinion, may do so. Often an observer will focus his attention on elements that are unusual in the particular circumstances, unfamiliar, incomprehensible, and so on."<sup id="cite_ref-8" class="reference"><a href="#cite_note-8">&#91;8&#93;</a></sup></dd></dl>
<dl><dd>"... when changing its points of fixation, the observer's eye repeatedly returns to the same elements of the picture. Additional time spent on perception is not used to examine the secondary elements, but to reexamine the most important elements."<sup id="cite_ref-9" class="reference"><a href="#cite_note-9">&#91;9&#93;</a></sup></dd></dl>
<div class="thumb tright"><div class="thumbinner" style="width:322px;"><a href="/wiki/File:Eye_tracking_thru_glass.JPG" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/d/de/Eye_tracking_thru_glass.JPG/320px-Eye_tracking_thru_glass.JPG" decoding="async" width="320" height="240" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/d/de/Eye_tracking_thru_glass.JPG/480px-Eye_tracking_thru_glass.JPG 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/d/de/Eye_tracking_thru_glass.JPG/640px-Eye_tracking_thru_glass.JPG 2x" data-file-width="1600" data-file-height="1200" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Eye_tracking_thru_glass.JPG" class="internal" title="Enlarge"></a></div>This study by Hunziker (1970)<sup id="cite_ref-10" class="reference"><a href="#cite_note-10">&#91;10&#93;</a></sup> on <i>eye tracking in problem solving</i> used simple 8&#160;mm film to track eye movement by filming the subject through a glass plate on which the visual problem was displayed.<sup id="cite_ref-11" class="reference"><a href="#cite_note-11">&#91;11&#93;</a></sup><sup id="cite_ref-12" class="reference"><a href="#cite_note-12">&#91;12&#93;</a></sup></div></div></div>
<p>In the 1970s, eye-tracking research expanded rapidly, particularly reading research. A good overview of the research in this period is given by <a href="/wiki/Dr._Keith_Rayner" class="mw-redirect" title="Dr. Keith Rayner">Rayner</a>.<sup id="cite_ref-13" class="reference"><a href="#cite_note-13">&#91;13&#93;</a></sup>
</p><p>In 1980, Just and Carpenter<sup id="cite_ref-14" class="reference"><a href="#cite_note-14">&#91;14&#93;</a></sup> formulated the influential <i>Strong eye-mind hypothesis</i>, that "there is no appreciable lag between what is fixated and what is processed". If this hypothesis is correct, then when a subject looks at a word or object, he or she also thinks about it (process cognitively), and for exactly as long as the recorded fixation. The hypothesis is often taken for granted by researchers using eye-tracking. However, <a href="/wiki/Gaze-contingency_paradigm" title="Gaze-contingency paradigm">gaze-contingent techniques</a> offer an interesting option in order to disentangle overt and covert attentions, to differentiate what is fixated and what is processed.
</p><p>During the 1980s, the eye-mind hypothesis was often questioned in light of covert attention,<sup id="cite_ref-15" class="reference"><a href="#cite_note-15">&#91;15&#93;</a></sup><sup id="cite_ref-16" class="reference"><a href="#cite_note-16">&#91;16&#93;</a></sup> the attention to something that one is not looking at, which people often do. If covert attention is common during eye-tracking recordings, the resulting scan-path and fixation patterns would often show not where our attention has been, but only where the eye has been looking, failing to indicate cognitive processing.
</p><p>The 1980s also saw the birth of using eye-tracking to answer questions related to human-computer interaction.  Specifically, researchers investigated how users search for commands in computer menus.<sup id="cite_ref-17" class="reference"><a href="#cite_note-17">&#91;17&#93;</a></sup>  Additionally, computers allowed researchers to use eye-tracking results in real time, primarily to help disabled users.<sup id="cite_ref-Download_Limit_Exceeded_18-0" class="reference"><a href="#cite_note-Download_Limit_Exceeded-18">&#91;18&#93;</a></sup>
</p><p>More recently, there has been growth in using eye tracking to study how users interact with different computer interfaces. Specific questions researchers ask are related to how easy different interfaces are for users.<sup id="cite_ref-Download_Limit_Exceeded_18-1" class="reference"><a href="#cite_note-Download_Limit_Exceeded-18">&#91;18&#93;</a></sup>  The results of the eye tracking research can lead to changes in design of the interface. Yet another recent area of research focuses on Web development. This can include how users react to drop-down menus or where they focus their attention on a website so the developer knows where to place an advertisement.<sup id="cite_ref-19" class="reference"><a href="#cite_note-19">&#91;19&#93;</a></sup>
</p><p>According to Hoffman,<sup id="cite_ref-20" class="reference"><a href="#cite_note-20">&#91;20&#93;</a></sup> current consensus is that visual attention is always slightly (100 to 250 ms) ahead of the eye. But as soon as attention moves to a new position, the eyes will want to follow.<sup id="cite_ref-21" class="reference"><a href="#cite_note-21">&#91;21&#93;</a></sup>
</p><p>We still cannot infer specific cognitive processes directly from a fixation on a particular object in a scene.<sup id="cite_ref-22" class="reference"><a href="#cite_note-22">&#91;22&#93;</a></sup> For instance, a fixation on a face in a picture may indicate recognition, liking, dislike, puzzlement etc. Therefore, eye tracking is often coupled with other methodologies, such as <a href="/wiki/Protocol_analysis" title="Protocol analysis">introspective verbal protocols</a>.
</p>
<h2><span class="mw-headline" id="Tracker_types">Tracker types</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Eye_tracking&amp;action=edit&amp;section=2" title="Edit section: Tracker types">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Eye-trackers measure rotations of the eye in one of several ways, but principally they fall into three categories: (i) measurement of the movement of an object (normally, a special contact lens) attached to the eye; (ii) optical tracking without direct contact to the eye; and (iii) measurement of electric potentials using electrodes placed around the eyes.
</p>
<h3><span class="mw-headline" id="Eye-attached_tracking">Eye-attached tracking</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Eye_tracking&amp;action=edit&amp;section=3" title="Edit section: Eye-attached tracking">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The first type uses an attachment to the eye, such as a special contact lens with an embedded mirror or magnetic field sensor, and the movement of the attachment is measured with the assumption that it does not slip significantly as the eye rotates. Measurements with tight-fitting contact lenses have provided extremely sensitive recordings of eye movement, and magnetic search coils are the method of choice for researchers studying the dynamics and underlying physiology of eye movement. This method allows the measurement of eye movement in horizontal, vertical and torsion directions.<sup id="cite_ref-23" class="reference"><a href="#cite_note-23">&#91;23&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Optical_tracking">Optical tracking</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Eye_tracking&amp;action=edit&amp;section=4" title="Edit section: Optical tracking">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="thumb tright"><div class="thumbinner" style="width:222px;"><a href="/wiki/File:EYE-SYNC_eye-tracking_analyzer.JPG" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/e/e9/EYE-SYNC_eye-tracking_analyzer.JPG/220px-EYE-SYNC_eye-tracking_analyzer.JPG" decoding="async" width="220" height="165" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/e/e9/EYE-SYNC_eye-tracking_analyzer.JPG/330px-EYE-SYNC_eye-tracking_analyzer.JPG 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/e/e9/EYE-SYNC_eye-tracking_analyzer.JPG/440px-EYE-SYNC_eye-tracking_analyzer.JPG 2x" data-file-width="3264" data-file-height="2448" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:EYE-SYNC_eye-tracking_analyzer.JPG" class="internal" title="Enlarge"></a></div>An eye-tracking <a href="/wiki/Head-mounted_display" title="Head-mounted display">head-mounted display</a>. Each eye has an LED light source (gold-color metal) on the side of the display lens, and a camera under the display lens.</div></div></div>
<p>The second broad category uses some non-contact, optical method for measuring eye motion. Light, typically infrared, is reflected from the eye and sensed by a video camera or some other specially designed optical sensor. The information is then analyzed to extract eye rotation from changes in reflections. Video-based eye trackers typically use the corneal reflection (the first <a href="/wiki/Purkinje_images" title="Purkinje images">Purkinje image</a>) and the center of the pupil as features to track over time. A more sensitive type of eye-tracker, the dual-Purkinje eye tracker,<sup id="cite_ref-24" class="reference"><a href="#cite_note-24">&#91;24&#93;</a></sup> uses reflections from the front of the cornea (first Purkinje image) and the back of the lens (fourth Purkinje image) as features to track. A still more sensitive method of tracking is to image features from inside the eye, such as the retinal blood vessels, and follow these features as the eye rotates. Optical methods, particularly those based on video recording, are widely used for gaze-tracking and are favored for being non-invasive and inexpensive.
</p>
<div role="note" class="hatnote navigation-not-searchable">See also: <a href="/wiki/Video-oculography" title="Video-oculography">Video-oculography</a></div>
<h3><span class="mw-headline" id="Electric_potential_measurement">Electric potential measurement</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Eye_tracking&amp;action=edit&amp;section=5" title="Edit section: Electric potential measurement">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The third category uses electric potentials measured with electrodes placed around the eyes. The eyes are the origin of a steady electric potential field which can also be detected in total darkness and if the eyes are closed. It can be modelled to be generated by a dipole with its positive pole at the cornea and its negative pole at the retina. The electric signal that can be derived using two pairs of contact electrodes placed on the skin around one eye is called <a href="/wiki/EOG" class="mw-redirect" title="EOG">Electrooculogram (EOG)</a>. If the eyes move from the centre position towards the periphery, the retina approaches one electrode while the cornea approaches the opposing one. This change in the orientation of the dipole and consequently the electric potential field results in a change in the measured EOG signal. Inversely, by analysing these changes in eye movement can be tracked. Due to the discretisation given by the common electrode setup, two separate movement components – a horizontal and a vertical – can be identified. A third EOG component is the radial EOG channel,<sup id="cite_ref-25" class="reference"><a href="#cite_note-25">&#91;25&#93;</a></sup> which is the average of the EOG channels referenced to some posterior scalp electrode. This radial EOG channel is sensitive to the saccadic spike potentials stemming from the extra-ocular muscles at the onset of saccades, and allows reliable detection of even miniature saccades.<sup id="cite_ref-26" class="reference"><a href="#cite_note-26">&#91;26&#93;</a></sup>
</p><p>Due to potential drifts and variable relations between the EOG signal amplitudes and the saccade sizes, it is challenging to use EOG for measuring slow eye movement and detecting gaze direction. EOG is, however, a very robust technique for measuring <a href="/wiki/Saccade" title="Saccade">saccadic eye movement</a> associated with gaze shifts and detecting <a href="/wiki/Blink" class="mw-redirect" title="Blink">blinks</a>. 
Contrary to video-based eye-trackers, EOG allows recording of eye movements even with eyes closed, and can thus be used in sleep research. It is a very light-weight approach that, in contrast to current video-based eye-trackers, requires only very low computational power; works under different lighting conditions; and can be implemented as an embedded, self-contained wearable system.<sup id="cite_ref-27" class="reference"><a href="#cite_note-27">&#91;27&#93;</a></sup> It is thus the method of choice for measuring eye movement in mobile daily-life situations and <a href="/wiki/Rapid_eye_movement_sleep" title="Rapid eye movement sleep">REM</a> phases during sleep.  The major disadvantage of EOG is its relatively poor gaze-direction accuracy compared to a video tracker.  That is, it is difficult to determine with good accuracy exactly where a subject is looking, though the time of eye movements can be determined.
</p>
<h2><span class="mw-headline" id="Technologies_and_techniques">Technologies and techniques</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Eye_tracking&amp;action=edit&amp;section=6" title="Edit section: Technologies and techniques">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The most widely used current designs are video-based eye-trackers. A camera focuses on one or both eyes and records eye movement as the viewer looks at some kind of stimulus. Most modern eye-trackers use the center of the pupil and <a href="/wiki/Infrared" title="Infrared">infrared</a> / <a href="/wiki/Near-infrared" class="mw-redirect" title="Near-infrared">near-infrared</a> non-collimated light to create <a href="/wiki/Corneal_reflection" class="mw-redirect" title="Corneal reflection">corneal reflections</a> (CR). The vector between the pupil center and the corneal reflections can be used to compute the point of regard on surface or the gaze direction. A simple calibration procedure of the individual is usually needed before using the eye tracker.<sup id="cite_ref-28" class="reference"><a href="#cite_note-28">&#91;28&#93;</a></sup>
</p><p>Two general types of infrared / near-infrared (also known as active light) eye-tracking techniques are used: bright-pupil and dark-pupil. Their difference is based on the location of the illumination source with respect to the optics. If the illumination is <a href="/wiki/Coaxial" title="Coaxial">coaxial</a> with the optical path, then the eye acts as a <a href="/wiki/Retroreflector" title="Retroreflector">retroreflector</a> as the light reflects off the <a href="/wiki/Retina" title="Retina">retina</a> creating a bright pupil effect similar to <a href="/wiki/Red-eye_effect" title="Red-eye effect">red eye</a>. If the illumination source is offset from the optical path, then the pupil appears dark because the retroreflection from the retina is directed away from the camera.<sup id="cite_ref-gneo_29-0" class="reference"><a href="#cite_note-gneo-29">&#91;29&#93;</a></sup>
</p><p>Bright-pupil tracking creates greater iris/pupil contrast, allowing more robust eye-tracking with all iris pigmentation, and greatly reduces interference caused by eyelashes and other obscuring features.<sup id="cite_ref-30" class="reference"><a href="#cite_note-30">&#91;30&#93;</a></sup> It also allows tracking in lighting conditions ranging from total darkness to very bright.
</p><p>Another, less used, method is known as passive light. It uses visible light to illuminate, something which may cause some distractions to users.<sup id="cite_ref-gneo_29-1" class="reference"><a href="#cite_note-gneo-29">&#91;29&#93;</a></sup> Another challenge with this method is that the contrast of the pupil is less than in the active light methods, therefore, the center of <a href="/wiki/Iris_(anatomy)" title="Iris (anatomy)">iris</a> is used for calculating the vector instead.<sup id="cite_ref-31" class="reference"><a href="#cite_note-31">&#91;31&#93;</a></sup> This calculation needs to detect the boundary of the iris and the white <a href="/wiki/Sclera" title="Sclera">sclera</a> (<a href="/wiki/Corneal_limbus" title="Corneal limbus">limbus</a> tracking). It presents another challenge for vertical eye movements due to obstruction of eyelids.<sup id="cite_ref-32" class="reference"><a href="#cite_note-32">&#91;32&#93;</a></sup>
</p>
<ul class="gallery mw-gallery-packed center">
		<li class="gallerybox" style="width: 217.33333333333px"><div style="width: 217.33333333333px">
			<div class="thumb" style="width: 215.33333333333px;"><div style="margin:0px auto;"><a href="/wiki/File:Bright_pupil_by_infrared_or_near_infrared_illumination.jpg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/1/1c/Bright_pupil_by_infrared_or_near_infrared_illumination.jpg" decoding="async" width="216" height="120" data-file-width="281" data-file-height="157" /></a></div></div>
			<div class="gallerytext">
<p>Infrared / near-infrared: bright pupil.
</p>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 220.66666666667px"><div style="width: 220.66666666667px">
			<div class="thumb" style="width: 218.66666666667px;"><div style="margin:0px auto;"><a href="/wiki/File:Dark_pupil_by_infrared_or_near_infrared_illumination.jpg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/c/cb/Dark_pupil_by_infrared_or_near_infrared_illumination.jpg" decoding="async" width="219" height="120" data-file-width="282" data-file-height="155" /></a></div></div>
			<div class="gallerytext">
<p>Infrared / near-infrared: dark pupil and corneal reflection.
</p>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 184.66666666667px"><div style="width: 184.66666666667px">
			<div class="thumb" style="width: 182.66666666667px;"><div style="margin:0px auto;"><a href="/wiki/File:Visible_light_eye-tracking_algorithm.jpg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/a/ac/Visible_light_eye-tracking_algorithm.jpg/274px-Visible_light_eye-tracking_algorithm.jpg" decoding="async" width="183" height="120" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/a/ac/Visible_light_eye-tracking_algorithm.jpg/410px-Visible_light_eye-tracking_algorithm.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/a/ac/Visible_light_eye-tracking_algorithm.jpg/547px-Visible_light_eye-tracking_algorithm.jpg 2x" data-file-width="606" data-file-height="399" /></a></div></div>
			<div class="gallerytext">
<p>Visible light: center of iris (red), corneal reflection (green), and output vector (blue).
</p>
			</div>
		</div></li>
</ul>
<p>Eye-tracking setups vary greatly: some are head-mounted, some require the head to be stable (for example, with a chin rest), and some function remotely and automatically track the head during motion.  Most use a sampling rate of at least 30&#160;Hz. Although 50/60&#160;Hz is more common, today many video-based eye trackers run at 240, 350 or even 1000/1250&#160;Hz, speeds needed in order to capture fixational eye movements or correctly measure saccade dynamics.
</p><p>Eye movements are typically divided into <a href="/wiki/Fixation_(visual)" title="Fixation (visual)">fixations</a> and saccades – when the eye gaze pauses in a certain position, and when it moves to another position, respectively. The resulting series of fixations and saccades is called a <a href="/w/index.php?title=Scanpath&amp;action=edit&amp;redlink=1" class="new" title="Scanpath (page does not exist)">scanpath</a>. Smooth pursuit describes the eye following a moving object. Fixational eye movements include <a href="/wiki/Microsaccade" title="Microsaccade">microsaccades</a>: small, involuntary saccades that occur during attempted fixation. Most information from the eye is made available during a fixation or smooth pursuit, but not during a saccade.<sup id="cite_ref-33" class="reference"><a href="#cite_note-33">&#91;33&#93;</a></sup>
</p><p>Scanpaths are useful for analyzing cognitive intent, interest, and salience.  Other biological factors (some as simple as gender) may affect the scanpath as well. Eye tracking in <a href="/wiki/Human%E2%80%93computer_interaction" title="Human–computer interaction">human–computer interaction</a> (HCI) typically investigates the scanpath for usability purposes, or as a method of input in <a href="/wiki/Gaze-contingency_paradigm" title="Gaze-contingency paradigm">gaze-contingent displays</a>, also known as <a href="/w/index.php?title=Gaze-based_interfaces&amp;action=edit&amp;redlink=1" class="new" title="Gaze-based interfaces (page does not exist)">gaze-based interfaces</a>.<sup id="cite_ref-34" class="reference"><a href="#cite_note-34">&#91;34&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Data_presentation">Data presentation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Eye_tracking&amp;action=edit&amp;section=7" title="Edit section: Data presentation">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Interpretation of the data that is recorded by the various types of eye-trackers employs a variety of software that animates or visually represents it, so that the visual behavior of one or more users can be graphically resumed. The video is generally manually coded to identify the AOIs(Area Of Interests) or recently using artificial intelligence. Graphical presentation is rarely the basis of research results, since they are limited in terms of what can be analysed - research relying on eye-tracking, for example, usually requires quantitative measures of the eye movement events and their parameters, The following visualisations are the most commonly used:
</p><p><b>Animated representations of a point on the interface</b>
This method is used when the visual behavior is examined individually indicating where the user focused their gaze in each moment, complemented with a small path that indicates the previous saccade movements, as seen in the image.
</p><p><b>Static representations of the saccade path</b>
This is fairly similar to the one described above, with the difference that this is static method. A higher level of expertise than with the animated ones is required to interpret this.
</p><p><b>Heat maps</b>
An alternative static representation, used mainly for the agglomerated analysis of the visual exploration patterns in a group of users. In these representations, the ‘hot’ zones or zones with higher density designate where the users focused their gaze (not their attention) with a higher frequency. Heat maps are the best known visualization technique for eyetracking studies.<sup id="cite_ref-35" class="reference"><a href="#cite_note-35">&#91;35&#93;</a></sup>
</p><p><b>Blind zones maps, or focus maps</b>
This method is a simplified version of the Heat maps where the visually less attended zones by the users are displayed clearly, thus allowing for an easier understanding of the most relevant information, that is to say, we are informed about which zones were not seen by the users.
</p>
<h2><span class="mw-headline" id="Eye-tracking_vs._gaze-tracking">Eye-tracking vs. gaze-tracking</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Eye_tracking&amp;action=edit&amp;section=8" title="Edit section: Eye-tracking vs. gaze-tracking">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Eye-trackers necessarily measure the rotation of the eye with respect to some frame of reference. This is usually tied to the measuring system. Thus, if the measuring system is head-mounted, as with EOG or a video-based system mounted to a helmet, then eye-in-head angles are measured. To deduce the line of sight in world coordinates, the head must be kept in a constant position or its movements must be tracked as well. In these cases, head direction is added to eye-in-head direction to determine gaze direction.
</p><p>If the measuring system is table-mounted, as with scleral search coils or table-mounted camera (“remote”) systems, then gaze angles are measured directly in world coordinates. Typically, in these situations head movements are prohibited. For example, the head position is fixed using a bite bar or a forehead support. Then a head-centered reference frame is identical to a world-centered reference frame. Or colloquially, the eye-in-head position directly determines the gaze direction.
</p><p>Some results are available on human eye movements under natural conditions where head movements are allowed as well.<sup id="cite_ref-36" class="reference"><a href="#cite_note-36">&#91;36&#93;</a></sup><sup id="cite_ref-37" class="reference"><a href="#cite_note-37">&#91;37&#93;</a></sup> The relative position of eye and head, even with constant gaze direction, influences neuronal activity in higher visual areas.<sup id="cite_ref-38" class="reference"><a href="#cite_note-38">&#91;38&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Practice">Practice</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Eye_tracking&amp;action=edit&amp;section=9" title="Edit section: Practice">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>A great deal of research has gone into studies of the mechanisms and dynamics of eye rotation, but the goal of eye- tracking is most often to estimate gaze direction. Users may be interested in what features of an image draw the eye, for example. It is important to realize that the eye-tracker does not provide absolute gaze direction, but rather can measure only changes in gaze direction. In order to know precisely what a subject is looking at, some calibration procedure is required in which the subject looks at a point or series of points, while the eye tracker records the value that corresponds to each gaze position. (Even those techniques that track features of the retina cannot provide exact gaze direction because there is no specific anatomical feature that marks the exact point where the visual axis meets the retina, if indeed there is such a single, stable point.) An accurate and reliable calibration is essential for obtaining valid and repeatable eye movement data, and this can be a significant challenge for non-verbal subjects or those who have unstable gaze.
</p><p>Each method of eye-tracking has advantages and disadvantages, and the choice of an eye-tracking system depends on considerations of cost and application. There are offline methods and online procedures like <a href="/wiki/AttentionTracking" title="AttentionTracking">AttentionTracking</a>. There is a trade-off between cost and sensitivity, with the most sensitive systems costing many tens of thousands of dollars and requiring considerable expertise to operate properly. Advances in computer and video technology have led to the development of relatively low-cost systems that are useful for many applications and fairly easy to use.<sup id="cite_ref-39" class="reference"><a href="#cite_note-39">&#91;39&#93;</a></sup> Interpretation of the results still requires some level of expertise, however, because a misaligned or poorly calibrated system can produce wildly erroneous data.
</p>
<h3><span class="mw-headline" id="Eye-tracking_while_driving_a_car_in_a_difficult_situation">Eye-tracking while driving a car in a difficult situation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Eye_tracking&amp;action=edit&amp;section=10" title="Edit section: Eye-tracking while driving a car in a difficult situation">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="thumb tright"><div class="thumbinner" style="width:536px;"><a href="/wiki/File:Eye_movements_of_drivers.jpg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/en/2/20/Eye_movements_of_drivers.jpg" decoding="async" width="534" height="532" class="thumbimage" data-file-width="534" data-file-height="532" /></a>  <div class="thumbcaption">Frames from narrow road eye tracking described in this section<sup id="cite_ref-40" class="reference"><a href="#cite_note-40">&#91;40&#93;</a></sup></div></div></div>
<p>The eye movement of two groups of drivers have been filmed with a special head camera by a team of the Swiss Federal Institute of Technology: Novice and experienced drivers had their eye-movement recorded while approaching a bend of a narrow road.
The series of images has been condensed from the original film frames<sup id="cite_ref-41" class="reference"><a href="#cite_note-41">&#91;41&#93;</a></sup> to show 2 eye fixations per image for better comprehension.
</p><p>Each of these stills corresponds to approximately 0.5 seconds in realtime.
</p><p>The series of images shows an example of eye fixations #9 to #14 of a typical novice and an experienced driver.
</p><p>Comparison of the top images shows that the experienced driver checks the curve and even has Fixation No. 9 left to look aside while the novice driver needs to check the road and estimate his distance to the parked car.
</p><p>In the middle images, the experienced driver is now fully concentrating on the location where an oncoming car could be seen. The novice driver concentrates his view on the parked car.
</p><p>In the bottom image the novice is busy estimating the distance between the left wall and the parked car, while the experienced driver can use his <a href="/wiki/Peripheral_vision" title="Peripheral vision">peripheral vision</a> for that and still concentrate his view on the dangerous point of the curve: If a car appears there, he has to give way, i. e. stop to the right instead of passing the parked car.<sup id="cite_ref-42" class="reference"><a href="#cite_note-42">&#91;42&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Eye-tracking_of_younger_and_elderly_people_while_walking">Eye-tracking of younger and elderly people while walking</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Eye_tracking&amp;action=edit&amp;section=11" title="Edit section: Eye-tracking of younger and elderly people while walking">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>While walking, elderly subjects depend more on foveal vision than do younger subjects. Their walking speed is decreased by a limited <a href="/wiki/Visual_field" title="Visual field">visual field</a>, probably caused by a deteriorated peripheral vision.
</p><p>Younger subjects make use of both their central and peripheral vision while walking. Their peripheral vision allows faster control over the process of walking.<sup id="cite_ref-43" class="reference"><a href="#cite_note-43">&#91;43&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Applications">Applications</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Eye_tracking&amp;action=edit&amp;section=12" title="Edit section: Applications">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>A wide variety of disciplines use eye-tracking techniques, including <a href="/wiki/Cognitive_science" title="Cognitive science">cognitive science</a>; <a href="/wiki/Psychology" title="Psychology">psychology</a> (notably <a href="/wiki/Psycholinguistics" title="Psycholinguistics">psycholinguistics</a>; the visual world paradigm); <a href="/wiki/Human-computer_interaction" class="mw-redirect" title="Human-computer interaction">human-computer interaction</a> (HCI); <a href="/wiki/Human_factors_and_ergonomics" title="Human factors and ergonomics">human factors and ergonomics</a>; <a href="/wiki/Marketing_research" title="Marketing research">marketing research</a> and medical research (neurological diagnosis). Specific applications include the tracking eye movement in <a href="/wiki/Eye_movement_in_language_reading" class="mw-redirect" title="Eye movement in language reading">language reading</a>, <a href="/wiki/Eye_movement_in_music_reading" title="Eye movement in music reading">music reading</a>, human <a href="/wiki/Activity_recognition" title="Activity recognition">activity recognition</a>, the perception of advertising, and the playing of sports.
</p>
<h3><span class="mw-headline" id="Commercial_applications">Commercial applications</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Eye_tracking&amp;action=edit&amp;section=13" title="Edit section: Commercial applications">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>In recent years, the increased sophistication and accessibility of eye-tracking technologies have generated a great deal of interest in the commercial sector. Applications include <a href="/wiki/Web_usability" title="Web usability">web usability</a>, advertising, sponsorship, package design and automotive engineering. In general, commercial eye-tracking studies function by presenting a target stimulus to a sample of consumers while an eye tracker is used to record the activity of the eye. Examples of target stimuli may include websites; television programs; sporting events; films and commercials; magazines and newspapers; packages; shelf displays; consumer systems (ATMs, checkout systems, kiosks); and software. The resulting data can be statistically analyzed and graphically rendered to provide evidence of specific visual patterns. By examining fixations, <a href="/wiki/Saccades" class="mw-redirect" title="Saccades">saccades</a>, pupil dilation, blinks and a variety of other behaviors, researchers can determine a great deal about the effectiveness of a given medium or product. While some companies complete this type of research internally, there are many private companies that offer eye-tracking services and analysis.
</p><p>One of the most prominent fields of commercial eye-tracking research is web usability.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (March 2009)">citation needed</span></a></i>&#93;</sup> While traditional usability techniques are often quite powerful in providing information on clicking and scrolling patterns, eye-tracking offers the ability to analyze user interaction between the clicks and how much time a user spends between clicks, thereby providing valuable insight into which features are the most eye-catching, which features cause confusion and which are ignored altogether. Specifically, eye-tracking can be used to assess search efficiency, branding, online advertisements, navigation usability, overall design and many other site components. Analyses may target a prototype or competitor site in addition to the main client site.
</p><p>Eye-tracking is commonly used in a variety of different advertising media. Commercials, print ads, online ads and sponsored programs are all conducive to analysis with current eye-tracking technology. For instance in newspapers, eye-tracking studies can be used to find out in what way advertisements should be mixed with the news in order to catch the reader’s eyes.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (November 2018)">citation needed</span></a></i>&#93;</sup> Analyses focus on visibility of a target product or logo in the context of a magazine, newspaper, website, or televised event. One example is an analysis of eye movements over advertisements in the <a href="/wiki/Yellow_pages" title="Yellow pages">Yellow Pages</a>. The study focused on what particular features caused people to notice an ad, whether they viewed ads in a particular order and how viewing times varied. The study revealed that ad size, graphics, color, and copy all influence attention to advertisements. Knowing this allows researchers to assess in great detail how often a sample of consumers fixates on the target logo, product or ad. As a result, an advertiser can quantify the success of a given campaign in terms of actual visual attention.<sup id="cite_ref-44" class="reference"><a href="#cite_note-44">&#91;44&#93;</a></sup> Another example of this is a study that found that in a <a href="/wiki/Search_engine_results_page" title="Search engine results page">search engine results page</a>, authorship snippets received more attention than the paid ads or even the first organic result.<sup id="cite_ref-45" class="reference"><a href="#cite_note-45">&#91;45&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Eye_tracking&amp;action=edit&amp;section=14" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="div-col columns column-width" style="-moz-column-width: 22em; -webkit-column-width: 22em; column-width: 22em;">
<ul><li><a href="/wiki/AttentionTracking" title="AttentionTracking">AttentionTracking</a></li>
<li><a href="/wiki/Eye_movement" title="Eye movement">Eye movement</a></li>
<li><a href="/wiki/Eye_movement_in_language_reading" class="mw-redirect" title="Eye movement in language reading">Eye movement in language reading</a></li>
<li><a href="/wiki/Eye_movement_in_music_reading" title="Eye movement in music reading">Eye movement in music reading</a></li>
<li><a href="/wiki/Eye_Tracking_Device" class="mw-redirect" title="Eye Tracking Device">Eye Tracking Device</a></li>
<li><a href="/wiki/Fovea_centralis" title="Fovea centralis">Fovea</a></li>
<li><a href="/wiki/Foveated_imaging" title="Foveated imaging">Foveated imaging</a></li>
<li><a href="/wiki/Gaze-contingency_paradigm" title="Gaze-contingency paradigm">Gaze-contingency paradigm</a></li>
<li><a href="/wiki/Marketing_research" title="Marketing research">Marketing research</a></li>
<li><a href="/wiki/Mouse-Tracking" class="mw-redirect" title="Mouse-Tracking">Mouse-Tracking</a></li>
<li><a href="/wiki/Peripheral_vision" title="Peripheral vision">Peripheral vision</a></li>
<li><a href="/wiki/Saccade" title="Saccade">Saccade</a></li>
<li><a href="/wiki/Screen_reading" title="Screen reading">Screen reading</a></li>
<li><a href="/wiki/Visage_SDK" title="Visage SDK">visage SDK</a></li></ul>
</div>
<h2><span class="mw-headline" id="Notes">Notes</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Eye_tracking&amp;action=edit&amp;section=15" title="Edit section: Notes">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist columns references-column-width" style="-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;">
<ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text">Reported in Huey 1908/1968.</span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><cite class="citation book">Huey, Edmund. <i>The Psychology and Pedagogy of Reading (Reprint)</i>. MIT Press 1968 (originally published 1908).</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Psychology+and+Pedagogy+of+Reading+%28Reprint%29&amp;rft.pub=MIT+Press+1968+%28originally+published+1908%29&amp;rft.aulast=Huey&amp;rft.aufirst=Edmund&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><style data-mw-deduplicate="TemplateStyles:r886058088">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}</style></span>
</li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text">Buswell (1922, 1937)</span>
</li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text">(1935)</span>
</li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text"><a href="#CITEREFYarbus1967">Yarbus 1967</a></span>
</li>
<li id="cite_note-Yarbus_1967_190-6"><span class="mw-cite-backlink">^ <a href="#cite_ref-Yarbus_1967_190_6-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Yarbus_1967_190_6-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><a href="#CITEREFYarbus1967">Yarbus 1967</a>, p.&#160;190</span>
</li>
<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7">^</a></b></span> <span class="reference-text"><a href="#CITEREFYarbus1967">Yarbus 1967</a>, p.&#160;194</span>
</li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text"><a href="#CITEREFYarbus1967">Yarbus 1967</a>, p.&#160;191</span>
</li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text"><a href="#CITEREFYarbus1967">Yarbus 1967</a>, p.&#160;193</span>
</li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text">Hunziker, H. W. (1970). Visuelle Informationsaufnahme und Intelligenz: Eine Untersuchung über die Augenfixationen beim Problemlösen. Schweizerische Zeitschrift für Psychologie und ihre Anwendungen, 1970, 29, Nr 1/2 (english abstract: <a rel="nofollow" class="external free" href="http://www.learning-systems.ch/multimedia/forsch1e.htm">http://www.learning-systems.ch/multimedia/forsch1e.htm</a> )</span>
</li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external free" href="http://www.learning-systems.ch/multimedia/eye">http://www.learning-systems.ch/multimedia/eye</a> movements problem solving.swf</span>
</li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://www.learning-systems.ch/multimedia/forsch1e.htm">"Visual Perception: Eye Movements in Problem Solving"</a>. <i>www.learning-systems.ch</i><span class="reference-accessdate">. Retrieved <span class="nowrap">9 October</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.learning-systems.ch&amp;rft.atitle=Visual+Perception%3A+Eye+Movements+in+Problem+Solving.&amp;rft_id=http%3A%2F%2Fwww.learning-systems.ch%2Fmultimedia%2Fforsch1e.htm&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text">Rayner (1978)</span>
</li>
<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text">Just and Carpenter (1980)</span>
</li>
<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text">Posner (1980)</span>
</li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text">Wright &amp; Ward (2008)</span>
</li>
<li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17">^</a></b></span> <span class="reference-text"><cite class="citation journal">"Eye Tracking in Human–Computer Interaction and Usability Research: Ready to Deliver the Promises". 1111. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.100.445">10.1.1.100.445</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Eye+Tracking+in+Human%E2%80%93Computer+Interaction+and+Usability+Research%3A+Ready+to+Deliver+the+Promises&amp;rft.date=1111&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.100.445&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-Download_Limit_Exceeded-18"><span class="mw-cite-backlink">^ <a href="#cite_ref-Download_Limit_Exceeded_18-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Download_Limit_Exceeded_18-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">"Download Limit Exceeded". 1111. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.100.445">10.1.1.100.445</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Download+Limit+Exceeded&amp;rft.date=1111&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.100.445&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external autonumber" href="http://www.mmi-interaktiv.de/uploads/media/MMI-Interaktiv0303_SchiesslDudaThoelkeFischer.pdf">[1]</a></span>
</li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text">Hoffman 1998</span>
</li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text"><cite class="citation journal">Deubel, Heiner (1996). <a rel="nofollow" class="external text" href="https://web.archive.org/web/20071017035534/http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B6T0W-3VXNHBP-10&amp;_user=952938&amp;_coverDate=06/30/1996&amp;_rdoc=1&amp;_fmt=&amp;_orig=search&amp;_sort=d&amp;view=c&amp;_acct=C000049220&amp;_version=1&amp;_urlVersion=0&amp;_userid=952938&amp;md5=4f7fbf4f015fde59aa9a39b30154e7f3">"Saccade target selection and object recognition: Evidence for a common attentional mechanism"</a>. <i>Vision Research</i>. <b>36</b> (12): 1827–1837. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1016%2F0042-6989%2895%2900294-4">10.1016/0042-6989(95)00294-4</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/8759451">8759451</a>. Archived from <a rel="nofollow" class="external text" href="http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B6T0W-3VXNHBP-10&amp;_user=952938&amp;_coverDate=06%2F30%2F1996&amp;_rdoc=1&amp;_fmt=&amp;_orig=search&amp;_sort=d&amp;view=c&amp;_acct=C000049220&amp;_version=1&amp;_urlVersion=0&amp;_userid=952938&amp;md5=4f7fbf4f015fde59aa9a39b30154e7f3">the original</a> on 2007-10-17.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Vision+Research&amp;rft.atitle=Saccade+target+selection+and+object+recognition%3A+Evidence+for+a+common+attentional+mechanism&amp;rft.volume=36&amp;rft.issue=12&amp;rft.pages=1827-1837&amp;rft.date=1996&amp;rft_id=info%3Adoi%2F10.1016%2F0042-6989%2895%2900294-4&amp;rft_id=info%3Apmid%2F8759451&amp;rft.aulast=Deubel&amp;rft.aufirst=Heiner&amp;rft_id=http%3A%2F%2Fwww.sciencedirect.com%2Fscience%3F_ob%3DArticleURL%26_udi%3DB6T0W-3VXNHBP-10%26_user%3D952938%26_coverDate%3D06%252F30%252F1996%26_rdoc%3D1%26_fmt%3D%26_orig%3Dsearch%26_sort%3Dd%26view%3Dc%26_acct%3DC000049220%26_version%3D1%26_urlVersion%3D0%26_userid%3D952938%26md5%3D4f7fbf4f015fde59aa9a39b30154e7f3&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22">^</a></b></span> <span class="reference-text">Holsanova 2007</span>
</li>
<li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text">David A. Robinson: A method of measuring eye movement using a scleral search coil in a magnetic field, IEEE Transactions on Bio-Medical Electronics, October 1963, 137–145 (<a rel="nofollow" class="external text" href="http://ebooks.yasse.ir/ebooks/IEEE_Journals/Bio-Medical_Electronics_IRE_T/A_Method_of_Measuring_Eye_Movemnent_Using_a_Scieral_Search_Coil_in_a_Magnetic_Field-274.pdf">PDF</a>)</span>
</li>
<li id="cite_note-24"><span class="mw-cite-backlink"><b><a href="#cite_ref-24">^</a></b></span> <span class="reference-text"><cite class="citation journal">Crane, H.D.; Steele, C.M. (1985). "Generation-V dual-Purkinje-image eyetracker". <i>Applied Optics</i>. <b>24</b> (4): 527–537. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="http://adsabs.harvard.edu/abs/1985ApOpt..24..527C">1985ApOpt..24..527C</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1364%2FAO.24.000527">10.1364/AO.24.000527</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Applied+Optics&amp;rft.atitle=Generation-V+dual-Purkinje-image+eyetracker&amp;rft.volume=24&amp;rft.issue=4&amp;rft.pages=527-537&amp;rft.date=1985&amp;rft_id=info%3Adoi%2F10.1364%2FAO.24.000527&amp;rft_id=info%3Abibcode%2F1985ApOpt..24..527C&amp;rft.aulast=Crane&amp;rft.aufirst=H.D.&amp;rft.au=Steele%2C+C.M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-25">^</a></b></span> <span class="reference-text">Elbert, T., Lutzenberger, W., Rockstroh, B., Birbaumer, N., 1985. Removal of ocular artifacts from the EEG. A biophysical approach to the EOG. Electroencephalogr Clin Neurophysiol 60, 455-463.</span>
</li>
<li id="cite_note-26"><span class="mw-cite-backlink"><b><a href="#cite_ref-26">^</a></b></span> <span class="reference-text"><cite class="citation journal">Keren, A.S.; Yuval-Greenberg, S.; Deouell, L.Y. (2010). "Saccadic spike potentials in gamma-band EEG: Characterization, detection and suppression". <i>NeuroImage</i>. <b>49</b> (3): 2248–2263. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1016%2Fj.neuroimage.2009.10.057">10.1016/j.neuroimage.2009.10.057</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/19874901">19874901</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=NeuroImage&amp;rft.atitle=Saccadic+spike+potentials+in+gamma-band+EEG%3A+Characterization%2C+detection+and+suppression&amp;rft.volume=49&amp;rft.issue=3&amp;rft.pages=2248-2263&amp;rft.date=2010&amp;rft_id=info%3Adoi%2F10.1016%2Fj.neuroimage.2009.10.057&amp;rft_id=info%3Apmid%2F19874901&amp;rft.aulast=Keren&amp;rft.aufirst=A.S.&amp;rft.au=Yuval-Greenberg%2C+S.&amp;rft.au=Deouell%2C+L.Y.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-27"><span class="mw-cite-backlink"><b><a href="#cite_ref-27">^</a></b></span> <span class="reference-text"><cite class="citation journal">Bulling, A.; Roggen, D.; Tröster, G. (2009). "Wearable EOG goggles: Seamless sensing and context-awareness in everyday environments". <i>Journal of Ambient Intelligence and Smart Environments (JAISE)</i>. <b>1</b> (2): 157–171. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.3233%2FAIS-2009-0020">10.3233/AIS-2009-0020</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Ambient+Intelligence+and+Smart+Environments+%28JAISE%29&amp;rft.atitle=Wearable+EOG+goggles%3A+Seamless+sensing+and+context-awareness+in+everyday+environments&amp;rft.volume=1&amp;rft.issue=2&amp;rft.pages=157-171&amp;rft.date=2009&amp;rft_id=info%3Adoi%2F10.3233%2FAIS-2009-0020&amp;rft.aulast=Bulling&amp;rft.aufirst=A.&amp;rft.au=Roggen%2C+D.&amp;rft.au=Tr%C3%B6ster%2C+G.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-28"><span class="mw-cite-backlink"><b><a href="#cite_ref-28">^</a></b></span> <span class="reference-text"><cite class="citation journal">Witzner Hansen, Dan; Qiang Ji (March 2010). <a rel="nofollow" class="external text" href="http://dl.acm.org/citation.cfm?id=1729561">"In the Eye of the Beholder: A Survey of Models for Eyes and Gaze"</a>. <i>IEEE Trans. Pattern Anal. Mach. Intell</i>. <b>32</b> (3): 478–500. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1109%2Ftpami.2009.30">10.1109/tpami.2009.30</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/20075473">20075473</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Trans.+Pattern+Anal.+Mach.+Intell.&amp;rft.atitle=In+the+Eye+of+the+Beholder%3A+A+Survey+of+Models+for+Eyes+and+Gaze&amp;rft.volume=32&amp;rft.issue=3&amp;rft.pages=478-500&amp;rft.date=2010-03&amp;rft_id=info%3Adoi%2F10.1109%2Ftpami.2009.30&amp;rft_id=info%3Apmid%2F20075473&amp;rft.aulast=Witzner+Hansen&amp;rft.aufirst=Dan&amp;rft.au=Qiang+Ji&amp;rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D1729561&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-gneo-29"><span class="mw-cite-backlink">^ <a href="#cite_ref-gneo_29-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-gneo_29-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Gneo, Massimo; Schmid, Maurizio; Conforto, Silvia; D’Alessio, Tommaso (2012). <a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC3543256">"A free geometry model-independent neural eye-gaze tracking system"</a>. <i>Journal of NeuroEngineering and Rehabilitation</i>. <b>9</b> (1): 82. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1186%2F1743-0003-9-82">10.1186/1743-0003-9-82</a>. <a href="/wiki/PubMed_Central" title="PubMed Central">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC3543256">3543256</a></span>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/23158726">23158726</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+NeuroEngineering+and+Rehabilitation&amp;rft.atitle=A+free+geometry+model-independent+neural+eye-gaze+tracking+system&amp;rft.volume=9&amp;rft.issue=1&amp;rft.pages=82&amp;rft.date=2012&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3543256&amp;rft_id=info%3Apmid%2F23158726&amp;rft_id=info%3Adoi%2F10.1186%2F1743-0003-9-82&amp;rft.aulast=Gneo&amp;rft.aufirst=Massimo&amp;rft.au=Schmid%2C+Maurizio&amp;rft.au=Conforto%2C+Silvia&amp;rft.au=D%E2%80%99Alessio%2C+Tommaso&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3543256&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-30"><span class="mw-cite-backlink"><b><a href="#cite_ref-30">^</a></b></span> <span class="reference-text">The Eye: A Survey of Human Vision; Wikimedia Foundation</span>
</li>
<li id="cite_note-31"><span class="mw-cite-backlink"><b><a href="#cite_ref-31">^</a></b></span> <span class="reference-text"><cite class="citation journal">Sigut, J; Sidha, SA (February 2011). "Iris center corneal reflection method for gaze tracking using visible light". <i>IEEE Transactions on Bio-medical Engineering</i>. <b>58</b> (2): 411–9. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1109%2Ftbme.2010.2087330">10.1109/tbme.2010.2087330</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/20952326">20952326</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Bio-medical+Engineering&amp;rft.atitle=Iris+center+corneal+reflection+method+for+gaze+tracking+using+visible+light.&amp;rft.volume=58&amp;rft.issue=2&amp;rft.pages=411-9&amp;rft.date=2011-02&amp;rft_id=info%3Adoi%2F10.1109%2Ftbme.2010.2087330&amp;rft_id=info%3Apmid%2F20952326&amp;rft.aulast=Sigut&amp;rft.aufirst=J&amp;rft.au=Sidha%2C+SA&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-32"><span class="mw-cite-backlink"><b><a href="#cite_ref-32">^</a></b></span> <span class="reference-text"><cite class="citation journal">Hua, H; Krishnaswamy, P; Rolland, JP (15 May 2006). "Video-based eyetracking methods and algorithms in head-mounted displays". <i>Optics Express</i>. <b>14</b> (10): 4328–50. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="http://adsabs.harvard.edu/abs/2006OExpr..14.4328H">2006OExpr..14.4328H</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1364%2Foe.14.004328">10.1364/oe.14.004328</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/19516585">19516585</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Optics+Express&amp;rft.atitle=Video-based+eyetracking+methods+and+algorithms+in+head-mounted+displays.&amp;rft.volume=14&amp;rft.issue=10&amp;rft.pages=4328-50&amp;rft.date=2006-05-15&amp;rft_id=info%3Apmid%2F19516585&amp;rft_id=info%3Adoi%2F10.1364%2Foe.14.004328&amp;rft_id=info%3Abibcode%2F2006OExpr..14.4328H&amp;rft.aulast=Hua&amp;rft.aufirst=H&amp;rft.au=Krishnaswamy%2C+P&amp;rft.au=Rolland%2C+JP&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-33"><span class="mw-cite-backlink"><b><a href="#cite_ref-33">^</a></b></span> <span class="reference-text"><cite class="citation book">Purves, D; st al. (2001). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/books/NBK11156/"><i>Neuroscience, 2d ed</i></a>. Sunderland (MA): Sinauer Assocs. p.&#160;What Eye Movements Accomplish.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Neuroscience%2C+2d+ed.&amp;rft.place=Sunderland+%28MA%29&amp;rft.pages=What+Eye+Movements+Accomplish&amp;rft.pub=Sinauer+Assocs.&amp;rft.date=2001&amp;rft.aulast=Purves&amp;rft.aufirst=D&amp;rft.au=st+al.&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fbooks%2FNBK11156%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-34"><span class="mw-cite-backlink"><b><a href="#cite_ref-34">^</a></b></span> <span class="reference-text">Majaranta, P., Aoki, H., Donegan, M., Hansen, D.W., Hansen, J.P., Hyrskykari, A., Räihä, K.J., <i>Gaze Interaction and Applications of Eye Tracking: Advances in Assistive Technologies</i>, IGI Global, 2011</span>
</li>
<li id="cite_note-35"><span class="mw-cite-backlink"><b><a href="#cite_ref-35">^</a></b></span> <span class="reference-text">Nielsen, Jakob. Pernice, Kara. (2010). "<a rel="nofollow" class="external autonumber" href="https://books.google.com/books?id=EeQhHqjgQosC">[2]</a> Eyetracking Web Usability." New Rideres Publishing. p. 11. <link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/><a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/0-321-49836-4" title="Special:BookSources/0-321-49836-4">0-321-49836-4</a>. Google Book Search. Retrieved on October 28, 2013.</span>
</li>
<li id="cite_note-36"><span class="mw-cite-backlink"><b><a href="#cite_ref-36">^</a></b></span> <span class="reference-text"><cite class="citation journal">Einhäuser, W; Moeller, GU; Schumann, F; Conradt, J; Vockeroth, J; Bartl, K; Schneider, E; König, P (2009). "Eye-head coordination during free exploration in human and cat". <i>Annals of the New York Academy of Sciences</i>. <b>1164</b> (1): 353–366. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="http://adsabs.harvard.edu/abs/2009NYASA1164..353E">2009NYASA1164..353E</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1111%2Fj.1749-6632.2008.03709.x">10.1111/j.1749-6632.2008.03709.x</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/19645927">19645927</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Annals+of+the+New+York+Academy+of+Sciences&amp;rft.atitle=Eye-head+coordination+during+free+exploration+in+human+and+cat&amp;rft.volume=1164&amp;rft.issue=1&amp;rft.pages=353-366&amp;rft.date=2009&amp;rft_id=info%3Apmid%2F19645927&amp;rft_id=info%3Adoi%2F10.1111%2Fj.1749-6632.2008.03709.x&amp;rft_id=info%3Abibcode%2F2009NYASA1164..353E&amp;rft.aulast=Einh%C3%A4user&amp;rft.aufirst=W&amp;rft.au=Moeller%2C+GU&amp;rft.au=Schumann%2C+F&amp;rft.au=Conradt%2C+J&amp;rft.au=Vockeroth%2C+J&amp;rft.au=Bartl%2C+K&amp;rft.au=Schneider%2C+E&amp;rft.au=K%C3%B6nig%2C+P&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-37"><span class="mw-cite-backlink"><b><a href="#cite_ref-37">^</a></b></span> <span class="reference-text">Einhäuser W, Schumann F, Bardins S, Bartl K, Böning G, Schneider E and König P (2007). Human eye-head co-ordination in natural exploration. Network: Computation in Neural Systems 18:267-297. Electronical publication: <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1080%2F09548980701671094">10.1080/09548980701671094</a></span>
</li>
<li id="cite_note-38"><span class="mw-cite-backlink"><b><a href="#cite_ref-38">^</a></b></span> <span class="reference-text"><cite class="citation journal">Andersen, R. A.; Bracewell, R. M.; Barash, S.; Gnadt, J. W.; Fogassi, L. (1990). <a rel="nofollow" class="external text" href="http://www.jneurosci.org/content/jneuro/10/4/1176.full.pdf">"Eye position effects on visual, memory, and saccade-related activity in areas LIP and 7a of macaque"</a> <span class="cs1-format">(PDF)</span>. <i>Journal of Neuroscience</i>. <b>10</b> (4): 1176–1196. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1523%2FJNEUROSCI.10-04-01176.1990">10.1523/JNEUROSCI.10-04-01176.1990</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Neuroscience&amp;rft.atitle=Eye+position+effects+on+visual%2C+memory%2C+and+saccade-related+activity+in+areas+LIP+and+7a+of+macaque&amp;rft.volume=10&amp;rft.issue=4&amp;rft.pages=1176-1196&amp;rft.date=1990&amp;rft_id=info%3Adoi%2F10.1523%2FJNEUROSCI.10-04-01176.1990&amp;rft.aulast=Andersen&amp;rft.aufirst=R.+A.&amp;rft.au=Bracewell%2C+R.+M.&amp;rft.au=Barash%2C+S.&amp;rft.au=Gnadt%2C+J.+W.&amp;rft.au=Fogassi%2C+L.&amp;rft_id=http%3A%2F%2Fwww.jneurosci.org%2Fcontent%2Fjneuro%2F10%2F4%2F1176.full.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-39"><span class="mw-cite-backlink"><b><a href="#cite_ref-39">^</a></b></span> <span class="reference-text"><cite class="citation journal">Ferhat, Onur; Vilariño, Fernando (2016). <a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC4808529">"Low Cost Eye Tracking: The Current Panorama"</a>. <i>Computational Intelligence and Neuroscience</i>. <b>2016</b>: 1–14. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1155%2F2016%2F8680541">10.1155/2016/8680541</a>. <a href="/wiki/PubMed_Central" title="PubMed Central">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC4808529">4808529</a></span>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/27034653">27034653</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Computational+Intelligence+and+Neuroscience&amp;rft.atitle=Low+Cost+Eye+Tracking%3A+The+Current+Panorama&amp;rft.volume=2016&amp;rft.pages=1-14&amp;rft.date=2016&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC4808529&amp;rft_id=info%3Apmid%2F27034653&amp;rft_id=info%3Adoi%2F10.1155%2F2016%2F8680541&amp;rft.aulast=Ferhat&amp;rft.aufirst=Onur&amp;rft.au=Vilari%C3%B1o%2C+Fernando&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC4808529&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-40"><span class="mw-cite-backlink"><b><a href="#cite_ref-40">^</a></b></span> <span class="reference-text">Hans-Werner Hunziker, (2006) Im Auge des Lesers: foveale und periphere Wahrnehmung - vom Buchstabieren zur Lesefreude [In the eye of the reader: foveal and peripheral perception - from letter recognition to the joy of reading] Transmedia Stäubli Verlag Zürich 2006 <link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/><a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-3-7266-0068-6" title="Special:BookSources/978-3-7266-0068-6">978-3-7266-0068-6</a> Based on data from:Cohen, A. S. (1983). Informationsaufnahme beim Befahren von Kurven, Psychologie für die Praxis 2/83, Bulletin der Schweizerischen Stiftung für Angewandte Psychologie</span>
</li>
<li id="cite_note-41"><span class="mw-cite-backlink"><b><a href="#cite_ref-41">^</a></b></span> <span class="reference-text">Cohen, A. S. (1983). Informationsaufnahme beim Befahren von Kurven, Psychologie für die Praxis 2/83, Bulletin der Schweizerischen Stiftung für Angewandte Psychologie</span>
</li>
<li id="cite_note-42"><span class="mw-cite-backlink"><b><a href="#cite_ref-42">^</a></b></span> <span class="reference-text">Pictures from: Hans-Werner Hunziker, (2006) Im Auge des Lesers: foveale und periphere Wahrnehmung – vom Buchstabieren zur Lesefreude  [In the eye of the reader: foveal and peripheral perception – from letter recognition to the joy of reading] Transmedia Stäubli Verlag Zürich 2006 <link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/><a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-3-7266-0068-6" title="Special:BookSources/978-3-7266-0068-6">978-3-7266-0068-6</a></span>
</li>
<li id="cite_note-43"><span class="mw-cite-backlink"><b><a href="#cite_ref-43">^</a></b></span> <span class="reference-text"><cite class="citation journal">Itoh, Nana; Fukuda, Tadahiko (2002). "Comparative Study of Eye Movements in Extent of Central and Peripheral Vision and Use by Young and Elderly Walkers". <i>Perceptual and Motor Skills</i>. <b>94</b> (3_suppl): 1283–1291. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.2466%2Fpms.2002.94.3c.1283">10.2466/pms.2002.94.3c.1283</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/12186250">12186250</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Perceptual+and+Motor+Skills&amp;rft.atitle=Comparative+Study+of+Eye+Movements+in+Extent+of+Central+and+Peripheral+Vision+and+Use+by+Young+and+Elderly+Walkers&amp;rft.volume=94&amp;rft.issue=3_suppl&amp;rft.pages=1283-1291&amp;rft.date=2002&amp;rft_id=info%3Adoi%2F10.2466%2Fpms.2002.94.3c.1283&amp;rft_id=info%3Apmid%2F12186250&amp;rft.aulast=Itoh&amp;rft.aufirst=Nana&amp;rft.au=Fukuda%2C+Tadahiko&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-44"><span class="mw-cite-backlink"><b><a href="#cite_ref-44">^</a></b></span> <span class="reference-text"><cite id="CITEREFLohseWu2001" class="citation journal">Lohse, Gerald; Wu, D. J. (1 February 2001). "Eye Movement Patterns on Chinese Yellow Pages Advertising". <i>Electronic Markets</i>. <b>11</b> (2): 87–96. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1080%2F101967801300197007">10.1080/101967801300197007</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Electronic+Markets&amp;rft.atitle=Eye+Movement+Patterns+on+Chinese+Yellow+Pages+Advertising&amp;rft.volume=11&amp;rft.issue=2&amp;rft.pages=87-96&amp;rft.date=2001-02-01&amp;rft_id=info%3Adoi%2F10.1080%2F101967801300197007&amp;rft.aulast=Lohse&amp;rft.aufirst=Gerald&amp;rft.au=Wu%2C+D.+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>
</li>
<li id="cite_note-45"><span class="mw-cite-backlink"><b><a href="#cite_ref-45">^</a></b></span> <span class="reference-text">"Eye Tracking Study: The Importance of Using Google Authorship in Search Results"<a rel="nofollow" class="external autonumber" href="http://www.searchenginejournal.com/eye-tracking-study-importance-using-google-authorship-search-results/71207/">[3]</a></span>
</li>
</ol></div>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Eye_tracking&amp;action=edit&amp;section=16" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<table role="presentation" class="mbox-small plainlinks sistersitebox" style="background-color:#f9f9f9;border:1px solid #aaa;color:#000">
<tbody><tr>
<td class="mbox-image"><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/30px-Commons-logo.svg.png" decoding="async" width="30" height="40" class="noviewer" srcset="//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/45px-Commons-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/59px-Commons-logo.svg.png 2x" data-file-width="1024" data-file-height="1376" /></td>
<td class="mbox-text plainlist">Wikimedia Commons has media related to <i><b><a href="https://commons.wikimedia.org/wiki/Category:Eye_tracking" class="extiw" title="commons:Category:Eye tracking">Eye tracking</a></b></i>.</td></tr></tbody></table>
<table role="presentation" class="metadata mbox-small" style="background-color:#f9f9f9;border:1px solid #aaa;color:#000">
<tbody><tr>
<td class="mbox-image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/d/d8/Scholia_logo.png/40px-Scholia_logo.png" decoding="async" width="40" height="40" class="noviewer" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/d/d8/Scholia_logo.png/60px-Scholia_logo.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/d/d8/Scholia_logo.png/80px-Scholia_logo.png 2x" data-file-width="1200" data-file-height="1200" /></td>
<td class="mbox-text plainlist">Scholia has a <i>topic</i> profile for <a class="external text" href="//tools.wmflabs.org/scholia/topic/Q970687"><i><b>Eye tracking</b></i></a>.</td></tr></tbody></table>
<ul><li><cite class="citation journal">Adler, FH; Fliegelman (1934). "Influence of fixation on the visual acuity". <i>Arch. Ophthalmology</i>. <b>12</b> (4): 475–483. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1001%2Farchopht.1934.00830170013002">10.1001/archopht.1934.00830170013002</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Arch.+Ophthalmology&amp;rft.atitle=Influence+of+fixation+on+the+visual+acuity&amp;rft.volume=12&amp;rft.issue=4&amp;rft.pages=475-483&amp;rft.date=1934&amp;rft_id=info%3Adoi%2F10.1001%2Farchopht.1934.00830170013002&amp;rft.aulast=Adler&amp;rft.aufirst=FH&amp;rft.au=Fliegelman&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></li>
<li>Buswell, G.T. (1922). Fundamental reading habits: A study of their development. Chicago, IL: University of Chicago Press.</li>
<li>Buswell G.T. (1935). How People Look at Pictures. Chicago: Univ. Chicago Press 137–55. Hillsdale, NJ: Erlbaum</li>
<li>Buswell, G.T. (1937). How adults read. Chicago, IL: University of Chicago Press.</li>
<li>Carpenter, Roger H.S.; Movements of the Eyes (2nd ed.). Pion Ltd, London, 1988. <link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/><a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/0-85086-109-8" title="Special:BookSources/0-85086-109-8">0-85086-109-8</a>.</li>
<li><cite class="citation journal">Cornsweet, TN; Crane, HD (1973). <a rel="nofollow" class="external text" href="http://www.escholarship.org/uc/item/8hg953zz">"Accurate two-dimensional eye tracker using first and fourth Purkinje images"</a>. <i>J Opt Soc Am</i>. <b>63</b> (8): 921–8. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1364%2Fjosa.63.000921">10.1364/josa.63.000921</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/4722578">4722578</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=J+Opt+Soc+Am&amp;rft.atitle=Accurate+two-dimensional+eye+tracker+using+first+and+fourth+Purkinje+images&amp;rft.volume=63&amp;rft.issue=8&amp;rft.pages=921-8&amp;rft.date=1973&amp;rft_id=info%3Adoi%2F10.1364%2Fjosa.63.000921&amp;rft_id=info%3Apmid%2F4722578&amp;rft.aulast=Cornsweet&amp;rft.aufirst=TN&amp;rft.au=Crane%2C+HD&amp;rft_id=http%3A%2F%2Fwww.escholarship.org%2Fuc%2Fitem%2F8hg953zz&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></li>
<li><cite class="citation journal"><a href="/wiki/Tom_N._Cornsweet" class="mw-redirect" title="Tom N. Cornsweet">Cornsweet, TN</a> (1958). <a rel="nofollow" class="external text" href="http://www.escholarship.org/uc/item/9kg0b3nf">"New technique for the measurement of small eye movements"</a>. <i>JOSA</i>. <b>48</b> (11): 808–811. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1364%2Fjosa.48.000808">10.1364/josa.48.000808</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=JOSA&amp;rft.atitle=New+technique+for+the+measurement+of+small+eye+movements&amp;rft.volume=48&amp;rft.issue=11&amp;rft.pages=808-811&amp;rft.date=1958&amp;rft_id=info%3Adoi%2F10.1364%2Fjosa.48.000808&amp;rft.aulast=Cornsweet&amp;rft.aufirst=TN&amp;rft_id=http%3A%2F%2Fwww.escholarship.org%2Fuc%2Fitem%2F9kg0b3nf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></li>
<li><cite class="citation journal">Deubel, H.; Schneider, W.X. (1996). "Saccade target selection and object recognition: Evidence for a common attentional mechanism". <i>Vision Research</i>. <b>36</b> (12): 1827–1837. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1016%2F0042-6989%2895%2900294-4">10.1016/0042-6989(95)00294-4</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/8759451">8759451</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Vision+Research&amp;rft.atitle=Saccade+target+selection+and+object+recognition%3A+Evidence+for+a+common+attentional+mechanism&amp;rft.volume=36&amp;rft.issue=12&amp;rft.pages=1827-1837&amp;rft.date=1996&amp;rft_id=info%3Adoi%2F10.1016%2F0042-6989%2895%2900294-4&amp;rft_id=info%3Apmid%2F8759451&amp;rft.aulast=Deubel&amp;rft.aufirst=H.&amp;rft.au=Schneider%2C+W.X.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></li>
<li>Duchowski, A. T., "A Breadth-First Survey of Eye Tracking Applications", Behavior Research Methods, Instruments, &amp; Computers (BRMIC), 34(4), November 2002, pp.&#160;455–470.</li>
<li><cite class="citation journal">Eizenman, M; Hallett, PE; Frecker, RC (1985). "Power spectra for ocular drift and tremor". <i>Vision Res</i>. <b>25</b> (11): 1635–40. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1016%2F0042-6989%2885%2990134-8">10.1016/0042-6989(85)90134-8</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/3832587">3832587</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Vision+Res.&amp;rft.atitle=Power+spectra+for+ocular+drift+and+tremor&amp;rft.volume=25&amp;rft.issue=11&amp;rft.pages=1635-40&amp;rft.date=1985&amp;rft_id=info%3Adoi%2F10.1016%2F0042-6989%2885%2990134-8&amp;rft_id=info%3Apmid%2F3832587&amp;rft.aulast=Eizenman&amp;rft.aufirst=M&amp;rft.au=Hallett%2C+PE&amp;rft.au=Frecker%2C+RC&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></li>
<li>Ferguson RD (1998). Servo tracking system utilizing phase-sensitive detection of reflectance variations. US Patent # 5,767,941</li>
<li><cite class="citation journal">Hammer, DX; Ferguson, RD; Magill, JC; White, MA; Elsner, AE; Webb, RH (2003). "Compact scanning laser ophthalmoscope with high-speed retinal tracker". <i>Appl Opt</i>. <b>42</b> (22): 4621–32. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="http://adsabs.harvard.edu/abs/2003ApOpt..42.4621H">2003ApOpt..42.4621H</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1364%2Fao.42.004621">10.1364/ao.42.004621</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/12916631">12916631</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Appl+Opt&amp;rft.atitle=Compact+scanning+laser+ophthalmoscope+with+high-speed+retinal+tracker&amp;rft.volume=42&amp;rft.issue=22&amp;rft.pages=4621-32&amp;rft.date=2003&amp;rft_id=info%3Apmid%2F12916631&amp;rft_id=info%3Adoi%2F10.1364%2Fao.42.004621&amp;rft_id=info%3Abibcode%2F2003ApOpt..42.4621H&amp;rft.aulast=Hammer&amp;rft.aufirst=DX&amp;rft.au=Ferguson%2C+RD&amp;rft.au=Magill%2C+JC&amp;rft.au=White%2C+MA&amp;rft.au=Elsner%2C+AE&amp;rft.au=Webb%2C+RH&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></li>
<li>Hoffman, J. E. (1998). Visual attention and eye movements. In H. Pashler (ed.), Attention (pp.&#160;119–154). Hove, UK: Psychology Press.</li>
<li>Holsanova, J. (forthcoming) Picture viewing and picture descriptions, Benjamins.<sup class="noprint Inline-Template" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:CITESHORT" class="mw-redirect" title="Wikipedia:CITESHORT"><span title="More information is required to link this short citation to its long citation.">incomplete short citation</span></a></i>&#93;</sup></li>
<li>Huey, E.B. (1968). The psychology and pedagogy of reading. Cambridge, MA: MIT Press. (Originally published 1908)</li>
<li>Jacob, R. J. K. &amp; Karn, K. S. (2003). <a rel="nofollow" class="external text" href="https://www.sciencedirect.com/science/article/pii/B9780444510204500311">Eye Tracking in Human-Computer Interaction and Usability Research: Ready to Deliver the Promises</a>. In R. Radach, J. Hyona, &amp; H. Deubel (eds.), The mind's eye: cognitive and applied aspects of eye movement research (pp.&#160;573–605). Boston: North-Holland/Elsevier.</li>
<li>Just MA, Carpenter PA (1980) <a rel="nofollow" class="external text" href="http://studentsuccess.aua.am/files/2013/10/A-Theory-of-Reading-From-Eye-Fixation-to-Comprehension-Just-and-Carpenter.pdf">A theory of reading: from eye fixation to comprehension</a>. Psychol Rev 87:329–354</li>
<li>Liechty, J, Pieters, R, &amp; Wedel, M. (2003). The Representation of Local and Global Exploration Modes in Eye Movements through Bayesian Hidden Markov Models. Psychometrika, 68 (4), 519–542.</li>
<li>Mulligan, JB, (1997). Recovery of Motion Parameters from Distortions in Scanned Images. Proceedings of the NASA Image Registration Workshop (IRW97), NASA Goddard Space Flight Center, MD</li>
<li><cite class="citation journal">Ott, D; Daunicht, WJ (1992). "Eye movement measurement with the scanning laser ophthalmoscope". <i>Clin. Vision Sci</i>. <b>7</b>: 551–556.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Clin.+Vision+Sci.&amp;rft.atitle=Eye+movement+measurement+with+the+scanning+laser+ophthalmoscope&amp;rft.volume=7&amp;rft.pages=551-556&amp;rft.date=1992&amp;rft.aulast=Ott&amp;rft.aufirst=D&amp;rft.au=Daunicht%2C+WJ&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></li>
<li>Pirri, F., Pizzoli, M., Rudi, A, (2011). A general method for the point of regard estimation in 3D space. Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, 921-928. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FCVPR.2011.5995634">10.1109/CVPR.2011.5995634</a></li>
<li><cite class="citation journal">Posner, M. I. (1980). "Orienting of attention". <i>Quarterly Journal of Experimental Psychology</i>. <b>32</b> (1): 3–25. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1080%2F00335558008248231">10.1080/00335558008248231</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/7367577">7367577</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Quarterly+Journal+of+Experimental+Psychology&amp;rft.atitle=Orienting+of+attention&amp;rft.volume=32&amp;rft.issue=1&amp;rft.pages=3-25&amp;rft.date=1980&amp;rft_id=info%3Adoi%2F10.1080%2F00335558008248231&amp;rft_id=info%3Apmid%2F7367577&amp;rft.aulast=Posner&amp;rft.aufirst=M.+I.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></li>
<li><cite class="citation journal">Rayner, K (1978). "Eye movements in reading and information processing". <i>Psychological Bulletin</i>. <b>85</b> (3): 618–660. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.294.4262">10.1.1.294.4262</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1037%2F0033-2909.85.3.618">10.1037/0033-2909.85.3.618</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/353867">353867</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Psychological+Bulletin&amp;rft.atitle=Eye+movements+in+reading+and+information+processing&amp;rft.volume=85&amp;rft.issue=3&amp;rft.pages=618-660&amp;rft.date=1978&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.294.4262&amp;rft_id=info%3Apmid%2F353867&amp;rft_id=info%3Adoi%2F10.1037%2F0033-2909.85.3.618&amp;rft.aulast=Rayner&amp;rft.aufirst=K&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></li>
<li><cite class="citation journal">Rayner, K (1998). "Eye movements in reading and information processing: 20 years of research". <i>Psychological Bulletin</i>. <b>124</b> (3): 372–422. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.211.3546">10.1.1.211.3546</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1037%2F0033-2909.124.3.372">10.1037/0033-2909.124.3.372</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/9849112">9849112</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Psychological+Bulletin&amp;rft.atitle=Eye+movements+in+reading+and+information+processing%3A+20+years+of+research&amp;rft.volume=124&amp;rft.issue=3&amp;rft.pages=372-422&amp;rft.date=1998&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.211.3546&amp;rft_id=info%3Apmid%2F9849112&amp;rft_id=info%3Adoi%2F10.1037%2F0033-2909.124.3.372&amp;rft.aulast=Rayner&amp;rft.aufirst=K&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></li>
<li><cite class="citation journal">Riggs, LA; Armington, JC; Ratliff, F (1954). "Motions of the retinal image during fixation". <i>JOSA</i>. <b>44</b> (4): 315–321. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1364%2Fjosa.44.000315">10.1364/josa.44.000315</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=JOSA&amp;rft.atitle=Motions+of+the+retinal+image+during+fixation&amp;rft.volume=44&amp;rft.issue=4&amp;rft.pages=315-321&amp;rft.date=1954&amp;rft_id=info%3Adoi%2F10.1364%2Fjosa.44.000315&amp;rft.aulast=Riggs&amp;rft.aufirst=LA&amp;rft.au=Armington%2C+JC&amp;rft.au=Ratliff%2C+F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></li>
<li><cite class="citation journal">Riggs, L. A.; Niehl, E. W. (1960). "Eye movements recorded during convergence and divergence". <i>J Opt Soc Am</i>. <b>50</b> (9): 913–920. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1364%2Fjosa.50.000913">10.1364/josa.50.000913</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=J+Opt+Soc+Am&amp;rft.atitle=Eye+movements+recorded+during+convergence+and+divergence&amp;rft.volume=50&amp;rft.issue=9&amp;rft.pages=913-920&amp;rft.date=1960&amp;rft_id=info%3Adoi%2F10.1364%2Fjosa.50.000913&amp;rft.aulast=Riggs&amp;rft.aufirst=L.+A.&amp;rft.au=Niehl%2C+E.+W.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></li>
<li>Riju Srimal, Jorn Diedrichsen, Edward B. Ryklin, and Clayton E. Curtis. "<a rel="nofollow" class="external text" href="https://www.physiology.org/doi/full/10.1152/jn.01024.2007">Obligatory adaptation of saccade gains</a>. J Neurophysiol. 2008 Mar;99(3):1554-8</li>
<li>Robinson, D. A. A method of measuring eye movement using a scleral search coil in a magnetic field. <i>IEEE Trans. Biomed. Eng.</i> vol. BME-l0, pp.&#160;137–145, 1963</li>
<li>Wright, R.D., &amp; Ward, L.M. (2008). Orienting of Attention. New York. Oxford University Press.</li>
<li><cite id="CITEREFYarbus1967" class="citation">Yarbus, A. L. (1967), <i>Eye Movements and Vision</i>, New York: Plenum</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Eye+Movements+and+Vision&amp;rft.place=New+York&amp;rft.pub=Plenum&amp;rft.date=1967&amp;rft.aulast=Yarbus&amp;rft.aufirst=A.+L.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/>. (Originally published in Russian 1962)</li></ul>
<h3><span class="mw-headline" id="Commercial_eye_tracking">Commercial eye tracking</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Eye_tracking&amp;action=edit&amp;section=17" title="Edit section: Commercial eye tracking">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<ul><li><cite class="citation journal">Bojko, A (2006). <a rel="nofollow" class="external text" href="http://www.upassoc.org/upa_publications/jus/2006_may/bojko_eye_tracking.html">"Using Eye Tracking to Compare Web Page Designs: A Case Study"</a>. <i>Journal of Usability Studies</i>. <b>1</b> (3).</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Usability+Studies&amp;rft.atitle=Using+Eye+Tracking+to+Compare+Web+Page+Designs%3A+A+Case+Study&amp;rft.volume=1&amp;rft.issue=3&amp;rft.date=2006&amp;rft.aulast=Bojko&amp;rft.aufirst=A&amp;rft_id=http%3A%2F%2Fwww.upassoc.org%2Fupa_publications%2Fjus%2F2006_may%2Fbojko_eye_tracking.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></li>
<li><cite class="citation journal">Bojko, A.; Stephenson, A. (2005). "It's All in the Eye of the User: How eye tracking can help answer usability questions". <i>User Experience</i>. <b>4</b> (1).</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=User+Experience&amp;rft.atitle=It%27s+All+in+the+Eye+of+the+User%3A+How+eye+tracking+can+help+answer+usability+questions&amp;rft.volume=4&amp;rft.issue=1&amp;rft.date=2005&amp;rft.aulast=Bojko&amp;rft.aufirst=A.&amp;rft.au=Stephenson%2C+A.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/><sup class="noprint Inline-Template" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:CITESHORT" class="mw-redirect" title="Wikipedia:CITESHORT"><span title="More information is required to link this short citation to its long citation.">incomplete short citation</span></a></i>&#93;</sup></li>
<li>Chandon, Pierre, J. Wesley Hutchinson, and Scott H. Young (2001), Measuring Value of Point-of-Purchase Marketing with Commercial Eye-Tracking Data. <a rel="nofollow" class="external autonumber" href="https://web.archive.org/web/20060907093047/http://ged.insead.edu/fichiersti/inseadwp2001/2001-19.pdf">[4]</a></li>
<li><cite class="citation journal">Duchowski, A. T. (2002). "A Breadth-First Survey of Eye Tracking Applications". <i>Behavior Research Methods, Instruments, &amp; Computers (BRMIC)</i>. <b>34</b> (4): 455–470. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.3758%2Fbf03195475">10.3758/bf03195475</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Behavior+Research+Methods%2C+Instruments%2C+%26+Computers+%28BRMIC%29&amp;rft.atitle=A+Breadth-First+Survey+of+Eye+Tracking+Applications&amp;rft.volume=34&amp;rft.issue=4&amp;rft.pages=455-470&amp;rft.date=2002&amp;rft_id=info%3Adoi%2F10.3758%2Fbf03195475&amp;rft.aulast=Duchowski&amp;rft.aufirst=A.+T.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></li>
<li>National Highway Traffic Safety Administration. (n.d.) Retrieved July 9, 2006, from <a rel="nofollow" class="external autonumber" href="https://web.archive.org/web/20061011024317/http://www-nrd.nhtsa.dot.gov/departments/nrd-13/newDriverDistraction.html">[5]</a></li>
<li><cite class="citation journal">Pieters, R.; Wedel, M.; Zhang, J. (2007). "Optimal Feature Advertising Under Competitive Clutter". <i>Management Science</i>. <b>51</b> (11): 1815–1828. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1287%2Fmnsc.1070.0732">10.1287/mnsc.1070.0732</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Management+Science&amp;rft.atitle=Optimal+Feature+Advertising+Under+Competitive+Clutter&amp;rft.volume=51&amp;rft.issue=11&amp;rft.pages=1815-1828&amp;rft.date=2007&amp;rft_id=info%3Adoi%2F10.1287%2Fmnsc.1070.0732&amp;rft.aulast=Pieters&amp;rft.aufirst=R.&amp;rft.au=Wedel%2C+M.&amp;rft.au=Zhang%2C+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></li>
<li><cite class="citation journal">Pieters, R.; Wedel, M. (2007). "Goal Control of Visual Attention to Advertising: The Yarbus Implication". <i>Journal of Consumer Research</i>. <b>34</b> (2): 224–233. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.524.9550">10.1.1.524.9550</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1086%2F519150">10.1086/519150</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Consumer+Research&amp;rft.atitle=Goal+Control+of+Visual+Attention+to+Advertising%3A+The+Yarbus+Implication&amp;rft.volume=34&amp;rft.issue=2&amp;rft.pages=224-233&amp;rft.date=2007&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.524.9550&amp;rft_id=info%3Adoi%2F10.1086%2F519150&amp;rft.aulast=Pieters&amp;rft.aufirst=R.&amp;rft.au=Wedel%2C+M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></li>
<li><cite class="citation journal">Pieters, R.; Wedel, M. (2004). "Attention Capture and Transfer by elements of Advertisements". <i>Journal of Marketing</i>. <b>68</b> (2): 36–50. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.115.3006">10.1.1.115.3006</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1509%2Fjmkg.68.2.36.27794">10.1509/jmkg.68.2.36.27794</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Marketing&amp;rft.atitle=Attention+Capture+and+Transfer+by+elements+of+Advertisements&amp;rft.volume=68&amp;rft.issue=2&amp;rft.pages=36-50&amp;rft.date=2004&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.115.3006&amp;rft_id=info%3Adoi%2F10.1509%2Fjmkg.68.2.36.27794&amp;rft.aulast=Pieters&amp;rft.aufirst=R.&amp;rft.au=Wedel%2C+M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></li>
<li>Thomas RECORDING GmbH, high-speed Eye Tracking Systems for neuro-scientific purposes <a rel="nofollow" class="external autonumber" href="https://web.archive.org/web/20110717051418/http://www.thomasrecording.com/en/cms/front_content.php?idcatart=63&amp;lang=1&amp;client=1">[6]</a><sup class="noprint Inline-Template" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:CITESHORT" class="mw-redirect" title="Wikipedia:CITESHORT"><span title="More information is required to link this short citation to its long citation.">incomplete short citation</span></a></i>&#93;</sup></li>
<li><cite class="citation journal">Weatherhead, James (2005). <a rel="nofollow" class="external text" href="http://itnow.oxfordjournals.org/cgi/reprint/47/6/32">"Eye on the Future"</a>. <i>British Computer Society, ITNOW Future of Computing</i>. <b>47</b> (6): 32–33. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1093%2Fitnow%2Fbwi127">10.1093/itnow/bwi127</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=British+Computer+Society%2C+ITNOW+Future+of+Computing&amp;rft.atitle=Eye+on+the+Future&amp;rft.volume=47&amp;rft.issue=6&amp;rft.pages=32-33&amp;rft.date=2005&amp;rft_id=info%3Adoi%2F10.1093%2Fitnow%2Fbwi127&amp;rft.aulast=Weatherhead&amp;rft.aufirst=James&amp;rft_id=http%3A%2F%2Fitnow.oxfordjournals.org%2Fcgi%2Freprint%2F47%2F6%2F32&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></li>
<li><cite class="citation journal">Wedel, M.; Pieters, R. (2000). "Eye fixations on advertisements and memory for brands: a model and findings". <i>Marketing Science</i>. <b>19</b> (4): 297–312. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1287%2Fmksc.19.4.297.11794">10.1287/mksc.19.4.297.11794</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Marketing+Science&amp;rft.atitle=Eye+fixations+on+advertisements+and+memory+for+brands%3A+a+model+and+findings&amp;rft.volume=19&amp;rft.issue=4&amp;rft.pages=297-312&amp;rft.date=2000&amp;rft_id=info%3Adoi%2F10.1287%2Fmksc.19.4.297.11794&amp;rft.aulast=Wedel&amp;rft.aufirst=M.&amp;rft.au=Pieters%2C+R.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEye+tracking" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></li>
<li>Wittenstein, Jerran. (2006). EyeTracking sees gold in its technology. [Electronic Version]. San Diego Source, The Daily Transcript, April, 3rd, 2006. <a rel="nofollow" class="external autonumber" href="http://www.sddt.com/news/article.cfm?SourceCode=20060403czh">[7]</a></li></ul>
<div role="navigation" class="navbox" aria-labelledby="Self-driving_cars_and_enabling_technologies" style="padding:3px"><table class="nowraplinks hlist collapsible collapsed navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Autonomous_cars_and_enabling_technologies" title="Template:Autonomous cars and enabling technologies"><abbr title="View this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Autonomous_cars_and_enabling_technologies" title="Template talk:Autonomous cars and enabling technologies"><abbr title="Discuss this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">t</abbr></a></li><li class="nv-edit"><a class="external text" href="//en.wikipedia.org/w/index.php?title=Template:Autonomous_cars_and_enabling_technologies&amp;action=edit"><abbr title="Edit this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">e</abbr></a></li></ul></div><div id="Self-driving_cars_and_enabling_technologies" style="font-size:114%;margin:0 4em"><a href="/wiki/Self-driving_car" title="Self-driving car">Self-driving cars</a> and enabling technologies</div></th></tr><tr><th scope="row" class="navbox-group" style="width:1%">Overview and context</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/History_of_self-driving_cars" title="History of self-driving cars">History of self-driving cars</a></li>
<li><a href="/wiki/Intelligent_transportation_system" title="Intelligent transportation system">Intelligent transportation system</a></li>
<li><a href="/wiki/Context-aware_pervasive_systems" title="Context-aware pervasive systems">Context-aware pervasive systems</a></li>
<li><a href="/wiki/Mobile_computing" title="Mobile computing">Mobile computing</a></li>
<li><a href="/wiki/Smart,_connected_products" class="mw-redirect" title="Smart, connected products">Smart, connected products</a></li>
<li><a href="/wiki/Ubiquitous_computing" title="Ubiquitous computing">Ubiquitous computing</a></li>
<li><a href="/wiki/Ambient_intelligence" title="Ambient intelligence">Ambient intelligence</a></li>
<li><a href="/wiki/Internet_of_things" title="Internet of things">Internet of things</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Self-driving_cars#Classification" class="mw-redirect" title="Self-driving cars">SAE Levels</a></th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:9em">Human driver monitors the driving environment<br />(Levels 0,1,2)</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Lane_departure_warning_system" title="Lane departure warning system">Lane departure warning system</a></li>
<li><a href="/wiki/Automatic_parking" title="Automatic parking">Automatic parking</a></li>
<li><a href="/wiki/Collision_avoidance_system" title="Collision avoidance system">Collision avoidance system</a></li>
<li><a href="/wiki/Autonomous_cruise_control_system" class="mw-redirect" title="Autonomous cruise control system">Adaptive cruise control</a></li>
<li><a href="/wiki/Advanced_driver-assistance_systems" title="Advanced driver-assistance systems">Advanced driver-assistance systems</a></li>
<li><a href="/wiki/Driver_drowsiness_detection" title="Driver drowsiness detection">Driver drowsiness detection</a></li>
<li><a href="/wiki/Intelligent_speed_adaptation" title="Intelligent speed adaptation">Intelligent speed adaptation</a></li>
<li><a href="/wiki/Blind_spot_monitor" title="Blind spot monitor">Blind spot monitor</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:9em">System monitors the driving environment<br />(Levels 3,4,5)</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Vehicle-to-vehicle" class="mw-redirect" title="Vehicle-to-vehicle">Vehicle-to-vehicle</a> (V2V)</li>
<li><a href="/wiki/Connected_car" title="Connected car">Connected car</a></li>
<li><a href="/wiki/Automotive_navigation_system" title="Automotive navigation system">Automotive navigation system</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Vehicles</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:9em">Cars</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/VaMP" title="VaMP">VaMP</a> (1994)</li>
<li><a href="/wiki/Spirit_of_Berlin" title="Spirit of Berlin">Spirit of Berlin</a> (2007)</li>
<li><a href="/wiki/General_Motors_EN-V" title="General Motors EN-V">General Motors EN-V</a> (2010)</li>
<li><a href="/wiki/MadeInGermany" title="MadeInGermany">MadeInGermany</a> (2011)</li>
<li><a href="/wiki/Waymo" title="Waymo">Waymo</a>, formerly Google Car (2012)</li>
<li><a href="/wiki/Tesla_Model_S#Autopilot" title="Tesla Model S">Tesla Model S <i>with</i> Autopilot</a> (2015)</li>
<li><a href="/wiki/LUTZ_Pathfinder" title="LUTZ Pathfinder">LUTZ Pathfinder</a> (2015)</li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:9em">Buses and commercial vehicles</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Automated_guideway_transit" title="Automated guideway transit">Automated guideway transit</a></li>
<li><a href="/wiki/ParkShuttle" title="ParkShuttle">ParkShuttle</a></li>
<li><a href="/wiki/Navia_(vehicle)" title="Navia (vehicle)">Navia shuttle</a></li>
<li><a href="/wiki/NuTonomy" title="NuTonomy">NuTonomy</a> taxi</li>
<li><a href="/wiki/Freightliner_Inspiration" title="Freightliner Inspiration">Freightliner Inspiration</a></li>
<li><a href="/wiki/Driverless_tractor" title="Driverless tractor">Driverless tractor</a></li>
<li><a href="/wiki/Mobility_as_a_service_(transport)" class="mw-redirect" title="Mobility as a service (transport)">Mobility as a service</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Regulation</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Self-driving_cars#Legislation" class="mw-redirect" title="Self-driving cars">Legislation</a></li>
<li><a href="/wiki/IEEE_802.11p" title="IEEE 802.11p">IEEE 802.11p</a></li>
<li><a href="/wiki/Assured_clear_distance_ahead" title="Assured clear distance ahead"> Safe speed automotive common law</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Enabling technologies</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Radar" title="Radar">Radar</a></li>
<li><a href="/wiki/Laser" title="Laser">Laser</a></li>
<li><a href="/wiki/LIDAR" class="mw-redirect" title="LIDAR">LIDAR</a></li>
<li><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural network</a></li>
<li><a href="/wiki/Computer_stereo_vision" title="Computer stereo vision">Computer stereo vision</a></li>
<li><a href="/wiki/Computer_vision#Recognition" title="Computer vision">Image recognition</a></li>
<li><a href="/wiki/Dedicated_short-range_communications" title="Dedicated short-range communications">Dedicated short-range communications</a></li>
<li><a href="/wiki/Real-time_Control_System" title="Real-time Control System">Real-time Control System</a></li>
<li><a class="mw-selflink selflink">Eye tracking</a></li>
<li><a href="/wiki/Radio-frequency_identification" title="Radio-frequency identification">Radio-frequency identification</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Organizations, Projects &amp; People</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:9em">Organizations, projects and events</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/DAVI" title="DAVI">DAVI</a></li>
<li><a href="/wiki/European_Land-Robot_Trial" title="European Land-Robot Trial">European Land-Robot Trial</a></li>
<li><a href="/wiki/Navlab" title="Navlab">Navlab</a></li>
<li><a href="/wiki/DARPA_Grand_Challenge" title="DARPA Grand Challenge">DARPA Grand Challenge</a></li>
<li><a href="/wiki/VisLab_Intercontinental_Autonomous_Challenge" title="VisLab Intercontinental Autonomous Challenge">VisLab Intercontinental Autonomous Challenge</a></li>
<li><a href="/wiki/Eureka_Prometheus_Project" title="Eureka Prometheus Project">Eureka Prometheus Project</a></li>
<li><a href="/wiki/IEEE_Intelligent_Transportation_Systems_Society" title="IEEE Intelligent Transportation Systems Society">IEEE Intelligent Transportation Systems Society</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:9em">People</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Harold_Goddijn" title="Harold Goddijn">Harold Goddijn</a></li>
<li><a href="/wiki/Alberto_Broggi" title="Alberto Broggi">Alberto Broggi</a></li>
<li><a href="/wiki/Anthony_Levandowski" title="Anthony Levandowski">Anthony Levandowski</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr></tbody></table></div>
<!-- 
NewPP limit report
Parsed by mw1261
Cached time: 20190330190554
Cache expiry: 2592000
Dynamic content: false
CPU time usage: 0.712 seconds
Real time usage: 0.901 seconds
Preprocessor visited node count: 3671/1000000
Preprocessor generated node count: 0/1500000
Post‐expand include size: 114465/2097152 bytes
Template argument size: 3892/2097152 bytes
Highest expansion depth: 13/40
Expensive parser function count: 6/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 118397/5000000 bytes
Number of Wikibase entities loaded: 5/400
Lua time usage: 0.391/10.000 seconds
Lua memory usage: 7.04 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  761.598      1 -total
 45.09%  343.382      1 Template:Reflist
 34.74%  264.615     35 Template:Cite_journal
  9.92%   75.515      2 Template:Cite_book
  7.90%   60.137      1 Template:About
  7.83%   59.606      2 Template:Citation_needed
  7.49%   57.044      3 Template:Fix
  6.87%   52.359      4 Template:ISBN
  4.98%   37.903      1 Template:Commons_category
  4.14%   31.505      5 Template:Category_handler
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:1543423-0!canonical and timestamp 20190330190553 and revision id 890163428
 -->
</div><noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>
		
		<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Eye_tracking&amp;oldid=890163428">https://en.wikipedia.org/w/index.php?title=Eye_tracking&amp;oldid=890163428</a>"</div>
		
		<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Attention" title="Category:Attention">Attention</a></li><li><a href="/wiki/Category:Cognitive_science" title="Category:Cognitive science">Cognitive science</a></li><li><a href="/wiki/Category:Eye" title="Category:Eye">Eye</a></li><li><a href="/wiki/Category:History_of_human%E2%80%93computer_interaction" title="Category:History of human–computer interaction">History of human–computer interaction</a></li><li><a href="/wiki/Category:Market_research" title="Category:Market research">Market research</a></li><li><a href="/wiki/Category:Multimodal_interaction" title="Category:Multimodal interaction">Multimodal interaction</a></li><li><a href="/wiki/Category:Promotion_and_marketing_communications" title="Category:Promotion and marketing communications">Promotion and marketing communications</a></li><li><a href="/wiki/Category:Usability" title="Category:Usability">Usability</a></li><li><a href="/wiki/Category:Vision" title="Category:Vision">Vision</a></li><li><a href="/wiki/Category:Web_design" title="Category:Web design">Web design</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_March_2009" title="Category:Articles with unsourced statements from March 2009">Articles with unsourced statements from March 2009</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_November_2018" title="Category:Articles with unsourced statements from November 2018">Articles with unsourced statements from November 2018</a></li><li><a href="/wiki/Category:Commons_category_link_is_on_Wikidata" title="Category:Commons category link is on Wikidata">Commons category link is on Wikidata</a></li><li><a href="/wiki/Category:Articles_needing_more_detailed_references" title="Category:Articles needing more detailed references">Articles needing more detailed references</a></li><li><a href="/wiki/Category:Articles_containing_video_clips" title="Category:Articles containing video clips">Articles containing video clips</a></li></ul></div></div>
		
		<div class="visualClear"></div>
		
	</div>
</div>

		<div id="mw-navigation">
			<h2>Navigation menu</h2>
			<div id="mw-head">
									<div id="p-personal" role="navigation" aria-labelledby="p-personal-label">
						<h3 id="p-personal-label">Personal tools</h3>
						<ul>
							<li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Eye+tracking" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Eye+tracking" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o">Log in</a></li>						</ul>
					</div>
									<div id="left-navigation">
										<div id="p-namespaces" role="navigation" class="vectorTabs" aria-labelledby="p-namespaces-label">
						<h3 id="p-namespaces-label">Namespaces</h3>
						<ul>
							<li id="ca-nstab-main" class="selected"><span><a href="/wiki/Eye_tracking" title="View the content page [c]" accesskey="c">Article</a></span></li><li id="ca-talk"><span><a href="/wiki/Talk:Eye_tracking" rel="discussion" title="Discussion about the content page [t]" accesskey="t">Talk</a></span></li>						</ul>
					</div>
										<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
												<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-variants-label" />
						<h3 id="p-variants-label">
							<span>Variants</span>
						</h3>
						<ul class="menu">
													</ul>
					</div>
									</div>
				<div id="right-navigation">
										<div id="p-views" role="navigation" class="vectorTabs" aria-labelledby="p-views-label">
						<h3 id="p-views-label">Views</h3>
						<ul>
							<li id="ca-view" class="collapsible selected"><span><a href="/wiki/Eye_tracking">Read</a></span></li><li id="ca-edit" class="collapsible"><span><a href="/w/index.php?title=Eye_tracking&amp;action=edit" title="Edit this page [e]" accesskey="e">Edit</a></span></li><li id="ca-history" class="collapsible"><span><a href="/w/index.php?title=Eye_tracking&amp;action=history" title="Past revisions of this page [h]" accesskey="h">View history</a></span></li>						</ul>
					</div>
										<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
						<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-cactions-label" />
						<h3 id="p-cactions-label"><span>More</span></h3>
						<ul class="menu">
													</ul>
					</div>
										<div id="p-search" role="search">
						<h3>
							<label for="searchInput">Search</label>
						</h3>
						<form action="/w/index.php" id="searchform">
							<div id="simpleSearch">
								<input type="search" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" accesskey="f" id="searchInput"/><input type="hidden" value="Special:Search" name="title"/><input type="submit" name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton"/><input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton"/>							</div>
						</form>
					</div>
									</div>
			</div>
			<div id="mw-panel">
				<div id="p-logo" role="banner"><a class="mw-wiki-logo" href="/wiki/Main_Page" title="Visit the main page"></a></div>
						<div class="portal" role="navigation" id="p-navigation" aria-labelledby="p-navigation-label">
			<h3 id="p-navigation-label">Navigation</h3>
			<div class="body">
								<ul>
					<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-interaction" aria-labelledby="p-interaction-label">
			<h3 id="p-interaction-label">Interaction</h3>
			<div class="body">
								<ul>
					<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-tb" aria-labelledby="p-tb-label">
			<h3 id="p-tb-label">Tools</h3>
			<div class="body">
								<ul>
					<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Eye_tracking" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Eye_tracking" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Eye_tracking&amp;oldid=890163428" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Eye_tracking&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q970687" title="Link to connected data repository item [g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Eye_tracking&amp;id=890163428" title="Information on how to cite this page">Cite this page</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-coll-print_export" aria-labelledby="p-coll-print_export-label">
			<h3 id="p-coll-print_export-label">Print/export</h3>
			<div class="body">
								<ul>
					<li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Eye+tracking">Create a book</a></li><li id="coll-download-as-rdf2latex"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=Eye+tracking&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a href="/w/index.php?title=Eye_tracking&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-wikibase-otherprojects" aria-labelledby="p-wikibase-otherprojects-label">
			<h3 id="p-wikibase-otherprojects-label">In other projects</h3>
			<div class="body">
								<ul>
					<li class="wb-otherproject-link wb-otherproject-commons"><a href="https://commons.wikimedia.org/wiki/Category:Eye_tracking" hreflang="en">Wikimedia Commons</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-lang" aria-labelledby="p-lang-label">
			<h3 id="p-lang-label">Languages</h3>
			<div class="body">
								<ul>
					<li class="interlanguage-link interwiki-ar"><a href="https://ar.wikipedia.org/wiki/%D8%AA%D8%AA%D8%A8%D8%B9_%D8%AD%D8%B1%D9%83%D8%A9_%D8%A7%D9%84%D8%B9%D9%8A%D9%86" title="تتبع حركة العين – Arabic" lang="ar" hreflang="ar" class="interlanguage-link-target">العربية</a></li><li class="interlanguage-link interwiki-ca"><a href="https://ca.wikipedia.org/wiki/Seguiment_d%27ulls" title="Seguiment d&#039;ulls – Catalan" lang="ca" hreflang="ca" class="interlanguage-link-target">Català</a></li><li class="interlanguage-link interwiki-cs"><a href="https://cs.wikipedia.org/wiki/Sledov%C3%A1n%C3%AD_pohybu_o%C4%8D%C3%AD" title="Sledování pohybu očí – Czech" lang="cs" hreflang="cs" class="interlanguage-link-target">Čeština</a></li><li class="interlanguage-link interwiki-de"><a href="https://de.wikipedia.org/wiki/Eye-Tracking" title="Eye-Tracking – German" lang="de" hreflang="de" class="interlanguage-link-target">Deutsch</a></li><li class="interlanguage-link interwiki-et"><a href="https://et.wikipedia.org/wiki/Pilguj%C3%A4lgimine" title="Pilgujälgimine – Estonian" lang="et" hreflang="et" class="interlanguage-link-target">Eesti</a></li><li class="interlanguage-link interwiki-es"><a href="https://es.wikipedia.org/wiki/Seguimiento_de_ojos" title="Seguimiento de ojos – Spanish" lang="es" hreflang="es" class="interlanguage-link-target">Español</a></li><li class="interlanguage-link interwiki-fa"><a href="https://fa.wikipedia.org/wiki/%D8%B1%D8%AF%DB%8C%D8%A7%D8%A8%DB%8C_%DA%86%D8%B4%D9%85" title="ردیابی چشم – Persian" lang="fa" hreflang="fa" class="interlanguage-link-target">فارسی</a></li><li class="interlanguage-link interwiki-fr"><a href="https://fr.wikipedia.org/wiki/Oculom%C3%A9trie" title="Oculométrie – French" lang="fr" hreflang="fr" class="interlanguage-link-target">Français</a></li><li class="interlanguage-link interwiki-it"><a href="https://it.wikipedia.org/wiki/Oculometria" title="Oculometria – Italian" lang="it" hreflang="it" class="interlanguage-link-target">Italiano</a></li><li class="interlanguage-link interwiki-lt"><a href="https://lt.wikipedia.org/wiki/Okulografija" title="Okulografija – Lithuanian" lang="lt" hreflang="lt" class="interlanguage-link-target">Lietuvių</a></li><li class="interlanguage-link interwiki-ja"><a href="https://ja.wikipedia.org/wiki/%E3%82%A2%E3%82%A4%E3%83%88%E3%83%A9%E3%83%83%E3%82%AD%E3%83%B3%E3%82%B0" title="アイトラッキング – Japanese" lang="ja" hreflang="ja" class="interlanguage-link-target">日本語</a></li><li class="interlanguage-link interwiki-pl"><a href="https://pl.wikipedia.org/wiki/Okulografia" title="Okulografia – Polish" lang="pl" hreflang="pl" class="interlanguage-link-target">Polski</a></li><li class="interlanguage-link interwiki-ru"><a href="https://ru.wikipedia.org/wiki/%D0%9E%D0%BA%D1%83%D0%BB%D0%BE%D0%B3%D1%80%D0%B0%D1%84%D0%B8%D1%8F" title="Окулография – Russian" lang="ru" hreflang="ru" class="interlanguage-link-target">Русский</a></li><li class="interlanguage-link interwiki-sv"><a href="https://sv.wikipedia.org/wiki/Blicksp%C3%A5rning" title="Blickspårning – Swedish" lang="sv" hreflang="sv" class="interlanguage-link-target">Svenska</a></li><li class="interlanguage-link interwiki-uk"><a href="https://uk.wikipedia.org/wiki/%D0%90%D0%B9-%D1%82%D1%80%D0%B5%D0%BA%D1%96%D0%BD%D0%B3" title="Ай-трекінг – Ukrainian" lang="uk" hreflang="uk" class="interlanguage-link-target">Українська</a></li><li class="interlanguage-link interwiki-zh"><a href="https://zh.wikipedia.org/wiki/%E7%9C%BC%E5%8A%A8%E8%BF%BD%E8%B8%AA" title="眼动追踪 – Chinese" lang="zh" hreflang="zh" class="interlanguage-link-target">中文</a></li>				</ul>
				<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q970687#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>			</div>
		</div>
				</div>
		</div>
				<div id="footer" role="contentinfo">
						<ul id="footer-info">
								<li id="footer-info-lastmod"> This page was last edited on 30 March 2019, at 15:36<span class="anonymous-show">&#160;(UTC)</span>.</li>
								<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="//creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
							</ul>
						<ul id="footer-places">
								<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
								<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
								<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
								<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
								<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
								<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
								<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=Eye_tracking&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
							</ul>
										<ul id="footer-icons" class="noprint">
										<li id="footer-copyrightico">
						<a href="https://wikimediafoundation.org/"><img src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation"/></a>					</li>
										<li id="footer-poweredbyico">
						<a href="//www.mediawiki.org/"><img src="/static/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"/></a>					</li>
									</ul>
						<div style="clear: both;"></div>
		</div>
		

<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.712","walltime":"0.901","ppvisitednodes":{"value":3671,"limit":1000000},"ppgeneratednodes":{"value":0,"limit":1500000},"postexpandincludesize":{"value":114465,"limit":2097152},"templateargumentsize":{"value":3892,"limit":2097152},"expansiondepth":{"value":13,"limit":40},"expensivefunctioncount":{"value":6,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":118397,"limit":5000000},"entityaccesscount":{"value":5,"limit":400},"timingprofile":["100.00%  761.598      1 -total"," 45.09%  343.382      1 Template:Reflist"," 34.74%  264.615     35 Template:Cite_journal","  9.92%   75.515      2 Template:Cite_book","  7.90%   60.137      1 Template:About","  7.83%   59.606      2 Template:Citation_needed","  7.49%   57.044      3 Template:Fix","  6.87%   52.359      4 Template:ISBN","  4.98%   37.903      1 Template:Commons_category","  4.14%   31.505      5 Template:Category_handler"]},"scribunto":{"limitreport-timeusage":{"value":"0.391","limit":"10.000"},"limitreport-memusage":{"value":7380410,"limit":52428800}},"cachereport":{"origin":"mw1261","timestamp":"20190330190554","ttl":2592000,"transientcontent":false}}});mw.config.set({"wgBackendResponseTime":95,"wgHostname":"mw1330"});});</script>
</body>
</html>
